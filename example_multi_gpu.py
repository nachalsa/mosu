#!/usr/bin/env python3
"""
ë©€í‹° GPU í•™ìŠµ ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸
í•œêµ­ ìˆ˜í™” ì¸ì‹ ëª¨ë¸ì„ ë©€í‹° GPUë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import torch
import sys
import os
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from advanced_config import AdvancedTrainingConfig
from advanced_trainer import AdvancedSignLanguageTrainer

def create_multi_gpu_config():
    """ë©€í‹° GPU í•™ìŠµì„ ìœ„í•œ ìµœì í™”ëœ ì„¤ì • ìƒì„±"""
    
    # GPU ìˆ˜ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
    gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 1
    base_batch_size = 16  # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
    optimized_batch_size = base_batch_size * max(1, gpu_count)
    
    print(f"ğŸ” ê°ì§€ëœ GPU ìˆ˜: {gpu_count}")
    print(f"ğŸ“Š ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°: {optimized_batch_size}")
    
    config = AdvancedTrainingConfig(
        # ì‹¤í—˜ ì„¤ì •
        experiment_name="multi_gpu_sign_language",
        
        # ë©€í‹° GPU ì„¤ì •
        multi_gpu=True,              # ë©€í‹° GPU ëª¨ë“œ í™œì„±í™”
        use_data_parallel=True,      # DataParallel ì‚¬ìš©
        auto_adjust_batch_size=True, # ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        
        # ê¸°ë³¸ í•™ìŠµ ì„¤ì •
        device="auto",               # ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€
        
        # ë‹¤ë‹¨ê³„ í•™ìŠµ ì„¤ì •
        multi_stage=AdvancedTrainingConfig.MultiStageConfig(
            stages=[
                # Stage 1: ê¸°ë³¸ í•™ìŠµ (ë©€í‹° GPU ìµœì í™”)
                AdvancedTrainingConfig.TrainingStageConfig(
                    name="baseline_multi_gpu",
                    epochs=10,
                    batch_size=optimized_batch_size,
                    learning_rate=0.001,
                    enable_augmentation=False,
                    patience=5,
                    min_delta=0.001
                ),
                # Stage 2: ë°ì´í„° ì¦ê°• í•™ìŠµ
                AdvancedTrainingConfig.TrainingStageConfig(
                    name="augmentation_multi_gpu", 
                    epochs=15,
                    batch_size=optimized_batch_size,
                    learning_rate=0.0005,
                    enable_augmentation=True,
                    patience=7,
                    min_delta=0.0005
                ),
                # Stage 3: ì„¸ë°€í•œ ì¡°ì •
                AdvancedTrainingConfig.TrainingStageConfig(
                    name="fine_tuning_multi_gpu",
                    epochs=20,
                    batch_size=optimized_batch_size // 2,  # ì„¸ë°€í•œ ì¡°ì •ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
                    learning_rate=0.0001,
                    enable_augmentation=True,
                    patience=10,
                    min_delta=0.0001,
                    weight_decay=0.01
                )
            ],
            max_stages=3,
            min_improvement_threshold=0.001,
            patience_between_stages=3
        ),
        
        # ëª¨ë¸ ì„¤ì • (ë©€í‹° GPUì— ìµœì í™”)
        model=AdvancedTrainingConfig.ModelConfig(
            hidden_dim=512,      # ë©€í‹° GPUì—ì„œ ë” í° ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥
            num_layers=8,        # ê¹Šì€ ë„¤íŠ¸ì›Œí¬
            num_heads=8,
            dropout=0.1,
            max_seq_length=60
        ),
        
        # ë°ì´í„° ì„¤ì •
        data=AdvancedTrainingConfig.DataConfig(
            data_dir="data",
            train_ratio=0.7,
            val_ratio=0.2,
            test_ratio=0.1,
            min_sequence_length=5,
            max_sequence_length=60
        ),
        
        # ìµœì í™” ì„¤ì •
        optimization=AdvancedTrainingConfig.OptimizationConfig(
            optimizer_type="adamw",
            weight_decay=0.001,
            gradient_clip_norm=1.0,
            warmup_steps=500,
            scheduler_type="cosine"
        ),
        
        # ì²´í¬í¬ì¸íŠ¸ ë° ë¡œê¹…
        save_top_k=3,
        save_every_n_epochs=5,
        log_every_n_steps=50,
        
        # ì¬í˜„ì„±
        deterministic_training=True,
        random_seed=42
    )
    
    return config

def run_multi_gpu_training():
    """ë©€í‹° GPU í•™ìŠµ ì‹¤í–‰"""
    
    print("ğŸš€ ë©€í‹° GPU í•œêµ­ ìˆ˜í™” ì¸ì‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 80)
    
    # ë©€í‹° GPU ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
    if not torch.cuda.is_available():
        print("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    elif torch.cuda.device_count() == 1:
        print("â„¹ï¸ GPUê°€ 1ê°œë§Œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹±ê¸€ GPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    else:
        print(f"âœ… {torch.cuda.device_count()}ê°œì˜ GPUê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for i in range(torch.cuda.device_count()):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
    
    # ì„¤ì • ìƒì„±
    config = create_multi_gpu_config()
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    print("\nğŸ“‹ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì¤‘...")
    trainer = AdvancedSignLanguageTrainer(config)
    
    # í•™ìŠµ ì‹¤í–‰
    print("\nğŸ¯ í•™ìŠµ ì‹œì‘...")
    try:
        results = trainer.train()
        
        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print("=" * 80)
        
        # ê²°ê³¼ ìš”ì•½
        if results:
            best_accuracy = max(stage['best_val_accuracy'] for stage in results)
            print(f"ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {best_accuracy:.4f}")
            
            total_epochs = sum(stage['epochs_completed'] for stage in results)
            print(f"ğŸ“Š ì´ í•™ìŠµ ì—í¬í¬: {total_epochs}")
            
            print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {trainer.checkpoint_dir}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def show_multi_gpu_tips():
    """ë©€í‹° GPU ì‚¬ìš© íŒ"""
    print("\n" + "=" * 80)
    print("ğŸ’¡ ë©€í‹° GPU í•™ìŠµ íŒ")
    print("=" * 80)
    
    tips = [
        "1. ë°°ì¹˜ í¬ê¸°ëŠ” GPU ìˆ˜ì— ë¹„ë¡€í•´ì„œ ì¦ê°€ì‹œí‚¤ì„¸ìš” (GPU 2ê°œ = ë°°ì¹˜ í¬ê¸° 2ë°°)",
        "2. í•™ìŠµë¥ ë„ ë°°ì¹˜ í¬ê¸°ì— ë§ì¶° ì¡°ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤",
        "3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ --auto-adjust-batch-size ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”",
        "4. ëª¨ë“  GPUê°€ ë™ì¼í•œ ë©”ëª¨ë¦¬ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”",
        "5. DataParallel ì‚¬ìš© ì‹œ ì²« ë²ˆì§¸ GPUì— ë” ë§ì€ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤",
        "6. í° ëª¨ë¸ì¼ìˆ˜ë¡ ë©€í‹° GPU íš¨ê³¼ê°€ ë” í½ë‹ˆë‹¤"
    ]
    
    for tip in tips:
        print(f"  {tip}")

if __name__ == "__main__":
    print("ğŸ§ª ë©€í‹° GPU í•™ìŠµ ì˜ˆì œ")
    
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì²˜ë¦¬
    import argparse
    parser = argparse.ArgumentParser(description="ë©€í‹° GPU í•œêµ­ ìˆ˜í™” ì¸ì‹ ëª¨ë¸ í•™ìŠµ")
    parser.add_argument("--tips", action="store_true", help="ë©€í‹° GPU ì‚¬ìš© íŒ í‘œì‹œ")
    parser.add_argument("--test-only", action="store_true", help="í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰ (í•™ìŠµ ì•ˆí•¨)")
    
    args = parser.parse_args()
    
    if args.tips:
        show_multi_gpu_tips()
    
    if args.test_only:
        print("ğŸ§ª ë©€í‹° GPU í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        # ì—¬ê¸°ì„œ test_multi_gpu.pyì˜ ê¸°ëŠ¥ì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŒ
    else:
        run_multi_gpu_training()
        show_multi_gpu_tips()
