#!/usr/bin/env python3
"""
YOLO11L + RTMW XPU í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸°
Large ëª¨ë¸ì„ ì‚¬ìš©í•œ ê³ ì •í™•ë„ ì‚¬ëŒ ê²€ì¶œ + XPU í¬ì¦ˆ ì¶”ì •
"""

import os
import torch
import cv2
import numpy as np
import time
from typing import List, Tuple, Optional
from collections import deque
from mmpose.apis import init_model, inference_topdown

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    print("âš ï¸ ultralytics ë¯¸ì„¤ì¹˜ - pip install ultralytics")
    YOLO_AVAILABLE = False

def check_xpu_availability():
    """XPU ê°€ìš©ì„± í™•ì¸"""
    try:
        if torch.xpu.is_available():
            device_count = torch.xpu.device_count()
            print(f"âœ… Intel XPU ì‚¬ìš© ê°€ëŠ¥: {device_count}ê°œ ë””ë°”ì´ìŠ¤")
            return True
        else:
            print("âš ï¸ Intel XPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            return False
    except Exception as e:
        print(f"âš ï¸ XPU í™•ì¸ ì‹¤íŒ¨: {e} - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

class YOLO11LXPUHybridInferencer:
    """YOLO11L + RTMW XPU í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸° - ê³ ì •í™•ë„ ë²„ì „"""
    
    def __init__(self, 
                 rtmw_config: str, 
                 rtmw_checkpoint: str,
                 detection_device: str = "auto",
                 pose_device: str = "auto",
                 optimize_for_accuracy: bool = True):
        """
        Args:
            rtmw_config: RTMW ì„¤ì • íŒŒì¼ ê²½ë¡œ
            rtmw_checkpoint: RTMW ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            detection_device: ê²€ì¶œ ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda', 'xpu')
            pose_device: í¬ì¦ˆ ì¶”ì • ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda', 'xpu')
            optimize_for_accuracy: ì •í™•ë„ ìµœì í™” ì—¬ë¶€
        """
        if not YOLO_AVAILABLE:
            raise ImportError("ultralyticsê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install ultralytics")
            
        self.rtmw_config = rtmw_config
        self.rtmw_checkpoint = rtmw_checkpoint
        self.yolo_model_name = "yolo11l.pt"  # Large ëª¨ë¸ ì‚¬ìš©
        self.optimize_for_accuracy = optimize_for_accuracy
        
        # XPU ê°€ìš©ì„± í™•ì¸
        self.xpu_available = check_xpu_availability()
        
        # ë””ë°”ì´ìŠ¤ ê²°ì •
        self.detection_device = self._determine_device(detection_device, "ê²€ì¶œ")
        self.pose_device = self._determine_device(pose_device, "í¬ì¦ˆì¶”ì •")
        
        print(f"ğŸš€ YOLO11L + RTMW í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸° ì´ˆê¸°í™”:")
        print(f"   - YOLO ëª¨ë¸: YOLO11L (Large - ê³ ì •í™•ë„)")
        print(f"   - ê²€ì¶œ ë””ë°”ì´ìŠ¤: {self.detection_device}")
        print(f"   - í¬ì¦ˆ ì¶”ì • ë””ë°”ì´ìŠ¤: {self.pose_device}")
        print(f"   - ì •í™•ë„ ìµœì í™”: {'ON' if optimize_for_accuracy else 'OFF'}")
        
        # PyTorch ë³´ì•ˆ ì„¤ì •
        self.original_load = torch.load
        torch.load = lambda *args, **kwargs: self.original_load(*args, **kwargs, weights_only=False) if 'weights_only' not in kwargs else self.original_load(*args, **kwargs)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._init_detection_model()
        self._init_pose_model()
        
        # ì„±ëŠ¥ í†µê³„
        self.inference_times = {
            'detection': [],
            'pose': [],
            'total': []
        }
        
        # ìµœì í™” ì„¤ì •
        self._setup_optimization()
        
    def _determine_device(self, device: str, task_name: str) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê²°ì •"""
        if device == "auto":
            if self.xpu_available:
                return "xpu"
            elif torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        else:
            if device == "xpu" and not self.xpu_available:
                print(f"âš ï¸ {task_name} XPU ë¯¸ì‚¬ìš© ê°€ëŠ¥ - CPUë¡œ í´ë°±")
                return "cpu"
            elif device == "cuda" and not torch.cuda.is_available():
                print(f"âš ï¸ {task_name} CUDA ë¯¸ì‚¬ìš© ê°€ëŠ¥ - CPUë¡œ í´ë°±")
                return "cpu"
            return device
    
    def _setup_optimization(self):
        """ìµœì í™” ì„¤ì • - ì •í™•ë„ ìš°ì„ """
        if self.optimize_for_accuracy:
            print("ğŸ¯ ì •í™•ë„ ìµœì í™” ì„¤ì • ì ìš© ì¤‘...")
            
            # YOLO11L ì •í™•ë„ ìš°ì„  íŒŒë¼ë¯¸í„°
            self.yolo_conf_thresh = 0.4     # ë‚®ì€ ì‹ ë¢°ë„ (ë” ë§ì€ ê²€ì¶œ)
            self.yolo_iou_thresh = 0.6      # ì ë‹¹í•œ IoU ì„ê³„ê°’
            self.yolo_max_det = 50          # ë” ë§ì€ ê²€ì¶œ í—ˆìš©
            self.yolo_classes = [0]         # ì‚¬ëŒ í´ë˜ìŠ¤ë§Œ
            
            # ì´ë¯¸ì§€ í¬ê¸° ìµœì í™” (ë” í° ì…ë ¥ í¬ê¸°)
            self.detection_img_size = 832   # Large ëª¨ë¸ì— ì í•©í•œ í° ì…ë ¥ í¬ê¸°
            self.pose_batch_size = 1        # í¬ì¦ˆ ì¶”ì • ë°°ì¹˜ í¬ê¸°
            
            # ë©€í‹°ìŠ¤ì¼€ì¼ í…ŒìŠ¤íŠ¸ (ì„ íƒì )
            self.use_multiscale = False     # ì‹œê°„ì´ ë” ê±¸ë¦¬ë¯€ë¡œ ê¸°ë³¸ false
            
            print("âœ… ì •í™•ë„ ìµœì í™” ì„¤ì • ì™„ë£Œ")
        else:
            # ê· í˜• ì„¤ì •
            self.yolo_conf_thresh = 0.5
            self.yolo_iou_thresh = 0.7
            self.yolo_max_det = 100
            self.yolo_classes = None
            self.detection_img_size = 640
            self.use_multiscale = False
    
    def _init_detection_model(self):
        """YOLO11L ê²€ì¶œ ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"ğŸ”§ YOLO11L ê²€ì¶œ ëª¨ë¸ ë¡œë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {self.detection_device})")
        start_time = time.time()
        
        try:
            # YOLO11L ëª¨ë¸ ë¡œë“œ
            model_path = os.path.join("../models", self.yolo_model_name)
            if not os.path.exists(model_path):
                print(f"ğŸ“¥ YOLO11L ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {self.yolo_model_name}")
                
            self.detection_model = YOLO(model_path)
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            if self.detection_device != "cpu":
                try:
                    self.detection_model.to(self.detection_device)
                    print(f"âœ… YOLO11L {self.detection_device.upper()} ëª¨ë“œ í™œì„±í™”")
                except Exception as e:
                    print(f"âš ï¸ YOLO11L {self.detection_device.upper()} ì‹¤íŒ¨, CPUë¡œ í´ë°±: {e}")
                    self.detection_device = "cpu"
                    self.detection_model.to('cpu')
            
            init_time = time.time() - start_time
            print(f"âœ… YOLO11L ê²€ì¶œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {init_time:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"âŒ YOLO11L ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.detection_model = None
            self.use_simple_detection = True
            print(f"ğŸ”„ ê°„ë‹¨í•œ ê²€ì¶œê¸°ë¡œ í´ë°±")
    
    def _init_pose_model(self):
        """RTMW í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"ğŸ”§ RTMW í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {self.pose_device})")
        start_time = time.time()
        
        try:
            self.pose_model = init_model(
                config=self.rtmw_config,
                checkpoint=self.rtmw_checkpoint,
                device=self.pose_device
            )
            
            init_time = time.time() - start_time
            print(f"âœ… RTMW í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {init_time:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"âŒ {self.pose_device} í¬ì¦ˆ ëª¨ë¸ ì‹¤íŒ¨: {e}")
            print(f"ğŸ”„ CPUë¡œ í´ë°±...")
            self.pose_device = 'cpu'
            
            self.pose_model = init_model(
                config=self.rtmw_config,
                checkpoint=self.rtmw_checkpoint,
                device='cpu'
            )
            
            init_time = time.time() - start_time
            print(f"âœ… CPU í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {init_time:.2f}ì´ˆ")
    
    def detect_persons_high_accuracy(self, image: np.ndarray) -> List[List[float]]:
        """ê³ ì •í™•ë„ ì‚¬ëŒ ê²€ì¶œ"""
        if self.detection_model is None:
            return self._simple_person_detection(image)
        
        try:
            start_time = time.time()
            
            # YOLO11L ì¶”ë¡  (ê³ ì •í™•ë„ íŒŒë¼ë¯¸í„°)
            results = self.detection_model(
                image,
                conf=self.yolo_conf_thresh,
                iou=self.yolo_iou_thresh,
                max_det=self.yolo_max_det,
                classes=self.yolo_classes,
                verbose=False,
                imgsz=self.detection_img_size,
                augment=self.use_multiscale  # TTA (Test Time Augmentation)
            )
            
            detection_time = time.time() - start_time
            self.inference_times['detection'].append(detection_time)
            
            # ì‚¬ëŒ(class 0) ê²€ì¶œ ê²°ê³¼ ì¶”ì¶œ
            person_boxes = []
            
            for result in results:
                boxes = result.boxes
                if boxes is not None and len(boxes) > 0:
                    person_coords = boxes.xyxy
                    person_confs = boxes.conf
                    
                    # ì‹ ë¢°ë„ ì¬í•„í„°ë§
                    conf_mask = person_confs >= self.yolo_conf_thresh
                    if conf_mask.any():
                        filtered_boxes = person_coords[conf_mask]
                        filtered_confs = person_confs[conf_mask]
                        
                        # numpy ë³€í™˜
                        if isinstance(filtered_boxes, torch.Tensor):
                            filtered_boxes = filtered_boxes.cpu().numpy()
                            filtered_confs = filtered_confs.cpu().numpy()
                        
                        # ì‹ ë¢°ë„ìˆœ ì •ë ¬
                        sorted_indices = np.argsort(filtered_confs)[::-1]
                        sorted_boxes = filtered_boxes[sorted_indices]
                        
                        person_boxes.extend(sorted_boxes.tolist())
            
            # print(f"ğŸ” YOLO11L ê²€ì¶œ ê²°ê³¼: {len(person_boxes)}ëª…, {detection_time:.3f}ì´ˆ")
            
            return person_boxes if person_boxes else self._simple_person_detection(image)
            
        except Exception as e:
            print(f"âŒ YOLO11L ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return self._simple_person_detection(image)
    
    def _simple_person_detection(self, image: np.ndarray) -> List[List[float]]:
        """ê°„ë‹¨í•œ ì‚¬ëŒ ê²€ì¶œ (í´ë°±)"""
        h, w = image.shape[:2]
        margin_w = int(w * 0.1)
        margin_h = int(h * 0.1)
        bbox = [margin_w, margin_h, w - margin_w, h - margin_h]
        return [bbox]
    
    def estimate_pose(self, image: np.ndarray, bbox: List[float]) -> Tuple[np.ndarray, np.ndarray]:
        """RTMWë¥¼ ì‚¬ìš©í•œ í¬ì¦ˆ ì¶”ì • (XPU) - ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„"""
        try:
            start_time = time.time()
            
            # MMPose ì¶”ë¡ 
            results = inference_topdown(
                model=self.pose_model,
                img=image,
                bboxes=[bbox],
                bbox_format='xyxy'
            )
            
            pose_time = time.time() - start_time
            self.inference_times['pose'].append(pose_time)
            
            if results and len(results) > 0:
                keypoints = results[0].pred_instances.keypoints[0]
                scores = results[0].pred_instances.keypoint_scores[0]
                
                if isinstance(keypoints, torch.Tensor):
                    keypoints = keypoints.cpu().numpy()
                if isinstance(scores, torch.Tensor):
                    scores = scores.cpu().numpy()
                    
                return keypoints, scores
            else:
                return np.zeros((133, 2)), np.zeros(133)
                
        except Exception as e:
            print(f"âŒ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return np.zeros((133, 2)), np.zeros(133)
    
    def estimate_pose_on_crop(self, crop_image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """í¬ë¡­ëœ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ í¬ì¦ˆ ì¶”ì • - í¬ë¡­ ì´ë¯¸ì§€ ì¢Œí‘œê³„"""
        try:
            start_time = time.time()
            
            # í¬ë¡­ ì´ë¯¸ì§€ ì „ì²´ë¥¼ ë°”ìš´ë”©ë°•ìŠ¤ë¡œ ì‚¬ìš©
            h, w = crop_image.shape[:2]
            full_bbox = [0, 0, w, h]
            
            # MMPose ì¶”ë¡ 
            results = inference_topdown(
                model=self.pose_model,
                img=crop_image,
                bboxes=[full_bbox],
                bbox_format='xyxy'
            )
            
            pose_time = time.time() - start_time
            self.inference_times['pose'].append(pose_time)
            
            if results and len(results) > 0:
                keypoints = results[0].pred_instances.keypoints[0]
                scores = results[0].pred_instances.keypoint_scores[0]
                
                if isinstance(keypoints, torch.Tensor):
                    keypoints = keypoints.cpu().numpy()
                if isinstance(scores, torch.Tensor):
                    scores = scores.cpu().numpy()
                    
                return keypoints, scores
            else:
                return np.zeros((133, 2)), np.zeros(133)
                
        except Exception as e:
            print(f"âŒ í¬ë¡­ ì´ë¯¸ì§€ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return np.zeros((133, 2)), np.zeros(133)
    
    def estimate_pose_batch(self, crop_images: List[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """ë°°ì¹˜ í¬ì¦ˆ ì¶”ì • (GPU ë³‘ë ¬ ì²˜ë¦¬)"""
        if not crop_images:
            return np.array([]), np.array([])
        
        batch_size = len(crop_images)
        batch_keypoints = []
        batch_scores = []
        
        try:
            # ë°°ì¹˜ í…ì„œ ì¤€ë¹„
            batch_tensor = torch.stack([
                self._preprocess_crop_for_batch(img) for img in crop_images
            ]).to(self.pose_device)
            
            # ë°°ì¹˜ ì¶”ë¡ 
            with torch.no_grad():
                batch_results = self.pose_model(batch_tensor)
            
            # í›„ì²˜ë¦¬
            for i in range(batch_size):
                keypoints, scores = self._postprocess_batch_result(batch_results, i, crop_images[i].shape)
                batch_keypoints.append(keypoints)
                batch_scores.append(scores)
            
            return np.array(batch_keypoints), np.array(batch_scores)
        
        except Exception as e:
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨, ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±: {e}")
            # í´ë°±: ê°œë³„ ì²˜ë¦¬
            for crop_image in crop_images:
                keypoints, scores = self.estimate_pose_on_crop(crop_image)
                batch_keypoints.append(keypoints)
                batch_scores.append(scores)
        
        return np.array(batch_keypoints), np.array(batch_scores)

    def _preprocess_crop_for_batch(self, crop_image: np.ndarray) -> torch.Tensor:
        """ë°°ì¹˜ ì²˜ë¦¬ìš© ì „ì²˜ë¦¬"""
        # ê¸°ì¡´ ì „ì²˜ë¦¬ ë¡œì§ì„ í…ì„œë¡œ ë³€í™˜
        # ... êµ¬ì²´ì ì¸ êµ¬í˜„ì€ RTMW ëª¨ë¸ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ë‹¬ë¼ì§
        pass

    def _postprocess_batch_result(self, batch_results, batch_idx: int, original_shape) -> Tuple[np.ndarray, np.ndarray]:
        """ë°°ì¹˜ ê²°ê³¼ í›„ì²˜ë¦¬"""
        # ë°°ì¹˜ ê²°ê³¼ì—ì„œ ê°œë³„ ê²°ê³¼ ì¶”ì¶œ ë° í›„ì²˜ë¦¬
        # ... êµ¬ì²´ì ì¸ êµ¬í˜„ì€ RTMW ëª¨ë¸ ì¶œë ¥ í˜•íƒœì— ë”°ë¼ ë‹¬ë¼ì§
        pass
    
    def process_frame(self, image: np.ndarray, conf_thresh: float = None) -> Tuple[np.ndarray, List[Tuple[np.ndarray, np.ndarray, List[float]]]]:
        """í”„ë ˆì„ ì²˜ë¦¬ (ê³ ì •í™•ë„ ê²€ì¶œ + í¬ì¦ˆ ì¶”ì •)"""
        start_time = time.time()
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •
        if conf_thresh is None:
            conf_thresh = self.yolo_conf_thresh
        
        # 1. ê³ ì •í™•ë„ ì‚¬ëŒ ê²€ì¶œ
        person_boxes = self.detect_persons_high_accuracy(image)
        
        # 2. ê° ì‚¬ëŒì— ëŒ€í•´ í¬ì¦ˆ ì¶”ì •
        results = []
        for bbox in person_boxes:
            keypoints, scores = self.estimate_pose(image, bbox)
            results.append((keypoints, scores, bbox))
        
        total_time = time.time() - start_time
        self.inference_times['total'].append(total_time)
        
        # 3. ì‹œê°í™”
        vis_image = self.visualize_results(image, results)
        
        return vis_image, results
    
    def visualize_results(self, image: np.ndarray, results: List[Tuple[np.ndarray, np.ndarray, List[float]]]) -> np.ndarray:
        """ê²°ê³¼ ì‹œê°í™” - í–¥ìƒëœ ë²„ì „"""
        vis_image = image.copy()
        
        for i, (keypoints, scores, bbox) in enumerate(results):
            # ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ)
            x1, y1, x2, y2 = map(int, bbox)
            color = (0, 255, 0) if i == 0 else (255, 0, 255)  # ì²« ë²ˆì§¸ëŠ” ì´ˆë¡, ë‚˜ë¨¸ì§€ëŠ” ìí™
            cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, 2)
            
            # ì‚¬ëŒ ë²ˆí˜¸ í‘œì‹œ
            cv2.putText(vis_image, f"Person {i+1}", (x1, y1-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            
            # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° (ì‹ ë¢°ë„ë³„ ìƒ‰ìƒ)
            for j, (kpt, score) in enumerate(zip(keypoints, scores)):
                if score > 0.3:
                    x, y = int(kpt[0]), int(kpt[1])
                    if 0 <= x < vis_image.shape[1] and 0 <= y < vis_image.shape[0]:
                        # ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ
                        if score > 0.8:
                            kpt_color = (0, 255, 0)    # ë†’ì€ ì‹ ë¢°ë„: ì´ˆë¡
                        elif score > 0.6:
                            kpt_color = (0, 255, 255)  # ì¤‘ê°„ ì‹ ë¢°ë„: ë…¸ë‘
                        else:
                            kpt_color = (0, 0, 255)    # ë‚®ì€ ì‹ ë¢°ë„: ë¹¨ê°•
                        
                        cv2.circle(vis_image, (x, y), 3, kpt_color, -1)
        
        # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
        if self.inference_times['total']:
            fps = 1.0 / self.inference_times['total'][-1]
            cv2.putText(vis_image, f"FPS: {fps:.1f}", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # ëª¨ë¸ ì •ë³´
        cv2.putText(vis_image, "YOLO11L + RTMW (High Accuracy)", 
                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(vis_image, f"Detection: {self.detection_device.upper()}", 
                   (10, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(vis_image, f"Pose: {self.pose_device.upper()}", 
                   (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return vis_image
    
    def benchmark_performance(self, image: np.ndarray, num_runs: int = 15) -> dict:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (Large ëª¨ë¸ì´ë¯€ë¡œ ì ì€ ì‹¤í–‰)"""
        print(f"ğŸƒ YOLO11L ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ({num_runs}íšŒ)...")
        
        # ì›Œë°ì—… (Large ëª¨ë¸ì€ ë” ì˜¤ë˜ ê±¸ë¦¼)
        for _ in range(3):
            self.process_frame(image)
        
        # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
        self.inference_times = {'detection': [], 'pose': [], 'total': []}
        
        for i in range(num_runs):
            self.process_frame(image)
            if (i + 1) % 5 == 0:
                print(f"   ì§„í–‰ë¥ : {i+1}/{num_runs}")
        
        # í†µê³„ ê³„ì‚°
        stats = {}
        for key, times in self.inference_times.items():
            if times:
                stats[key] = {
                    'mean': np.mean(times),
                    'std': np.std(times),
                    'min': np.min(times),
                    'max': np.max(times),
                    'fps': 1.0 / np.mean(times) if key == 'total' else None
                }
        
        return stats
    
    def test_single_image(self, image_path: str):
        """ë‹¨ì¼ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== YOLO11L + RTMW ê³ ì •í™•ë„ í…ŒìŠ¤íŠ¸: {os.path.basename(image_path)} ===")
        
        if not os.path.exists(image_path):
            print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path}")
            return
            
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            return
        
        print(f"ğŸ“· ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
        
        # ì²˜ë¦¬
        vis_image, results = self.process_frame(image)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ… ê²€ì¶œëœ ì‚¬ëŒ ìˆ˜: {len(results)}")
        
        for i, (keypoints, scores, bbox) in enumerate(results):
            valid_kpts = np.sum(scores > 0.3)
            high_conf_kpts = np.sum(scores > 0.8)
            bbox_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
            print(f"   ì‚¬ëŒ {i+1}: {valid_kpts}/133 í‚¤í¬ì¸íŠ¸ (ì‹ ë¢°ë„ > 0.3), {high_conf_kpts} ê³ ì‹ ë¢°ë„")
            print(f"            ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸°: {bbox_area:.0f}pxÂ²")
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        stats = self.benchmark_performance(image, num_runs=15)
        
        print(f"\nğŸ“Š YOLO11L ì„±ëŠ¥ í†µê³„:")
        for stage, stat in stats.items():
            if stat:
                print(f"   {stage}:")
                print(f"     - í‰ê· : {stat['mean']*1000:.1f}ms")
                print(f"     - ìµœì†Œ/ìµœëŒ€: {stat['min']*1000:.1f}/{stat['max']*1000:.1f}ms")
                if stat['fps']:
                    print(f"     - FPS: {stat['fps']:.1f}")
        
        # ê²°ê³¼ ì €ì¥
        output_path = f"yolo11l_result_{os.path.basename(image_path)}"
        cv2.imwrite(output_path, vis_image)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        
        return vis_image, results, stats
    
    def test_webcam(self, camera_id: int = 0, window_size: Tuple[int, int] = (1280, 720)):
        """ì‹¤ì‹œê°„ ì›¹ìº  í…ŒìŠ¤íŠ¸ - YOLO11L ê³ ì •í™•ë„"""
        print(f"\n=== YOLO11L ì‹¤ì‹œê°„ ì›¹ìº  í…ŒìŠ¤íŠ¸ (ì¹´ë©”ë¼ ID: {camera_id}) ===")
        print("ğŸ“¹ ì›¹ìº  ì—°ê²° ì¤‘...")
        
        cap = cv2.VideoCapture(camera_id)
        if not cap.isOpened():
            print(f"âŒ ì›¹ìº  ì—´ê¸° ì‹¤íŒ¨ (ì¹´ë©”ë¼ ID: {camera_id})")
            return
        
        # ì›¹ìº  ì„¤ì •
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, window_size[0])
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, window_size[1])
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        # JPEG í¬ë§·ìœ¼ë¡œ ì„¤ì •
        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))
        
        # ì‹¤ì œ ì›¹ìº  í•´ìƒë„ í™•ì¸
        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = cap.get(cv2.CAP_PROP_FPS)
        fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))
        
        print(f"âœ… ì›¹ìº  ì—°ê²° ì„±ê³µ: {actual_width}x{actual_height}, {actual_fps:.1f}fps")
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ í¬ë§·: JPEG (MJPG) - {fourcc}")
        print(f"ğŸ® ì¡°ì‘ë²•:")
        print(f"   - ESC: ì¢…ë£Œ")
        print(f"   - S: í˜„ì¬ í”„ë ˆì„ ìŠ¤í¬ë¦°ìƒ·")
        print(f"   - SPACE: ì¼ì‹œì •ì§€/ì¬ìƒ")
        print(f"   - A: ì •í™•ë„ ëª¨ë“œ í† ê¸€ (ë†’ìŒ/í‘œì¤€)")
        print(f"   - +/-: ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì •")
        
        # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
        frame_count = 0
        fps_history = deque(maxlen=30)
        paused = False
        screenshot_count = 0
        accuracy_mode = True  # ì •í™•ë„ ëª¨ë“œ
        current_conf_thresh = self.yolo_conf_thresh
        
        try:
            while True:
                if not paused:
                    ret, frame = cap.read()
                    if not ret:
                        print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                        break
                    
                    # í”„ë ˆì„ ì²˜ë¦¬
                    start_time = time.time()
                    vis_frame, results = self.process_frame(frame, conf_thresh=current_conf_thresh)
                    process_time = time.time() - start_time
                    
                    # FPS ê³„ì‚°
                    fps = 1.0 / process_time if process_time > 0 else 0
                    fps_history.append(fps)
                    avg_fps = np.mean(fps_history) if fps_history else 0
                    
                    # ì¶”ê°€ ì •ë³´ í‘œì‹œ
                    info_y = 140
                    cv2.putText(vis_frame, f"Avg FPS: {avg_fps:.1f}", 
                               (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    cv2.putText(vis_frame, f"XPU: {self.detection_device}/{self.pose_device}", 
                               (10, info_y + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    cv2.putText(vis_frame, f"Mode: {'High Accuracy' if accuracy_mode else 'Standard'}", 
                               (10, info_y + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    cv2.putText(vis_frame, f"Conf: {current_conf_thresh:.2f}", 
                               (10, info_y + 75), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    
                    # ê²€ì¶œëœ ì‚¬ëŒ ì •ë³´
                    if results:
                        person_info = f"Persons: {len(results)}"
                        for i, (_, scores, _, ) in enumerate(results):
                            valid_kpts = np.sum(scores > 0.3)
                            high_conf_kpts = np.sum(scores > 0.8)
                            person_info += f" | P{i+1}: {valid_kpts}/133 ({high_conf_kpts} high)"
                        cv2.putText(vis_frame, person_info, 
                                   (10, info_y + 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
                    
                    frame_count += 1
                else:
                    # ì¼ì‹œì •ì§€ ìƒíƒœì—ì„œëŠ” ë§ˆì§€ë§‰ í”„ë ˆì„ ê³„ì† í‘œì‹œ
                    pass
                
                # í™”ë©´ í‘œì‹œ
                cv2.imshow('YOLO11L + RTMW Real-time (High Accuracy XPU)', vis_frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                
                if key == 27:  # ESC
                    break
                elif key == ord('s') or key == ord('S'):  # ìŠ¤í¬ë¦°ìƒ·
                    screenshot_name = f"yolo11l_webcam_screenshot_{screenshot_count:04d}.jpg"
                    cv2.imwrite(screenshot_name, vis_frame)
                    print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_name}")
                    screenshot_count += 1
                elif key == ord(' '):  # ì¼ì‹œì •ì§€/ì¬ìƒ
                    paused = not paused
                    print(f"â¸ï¸ {'ì¼ì‹œì •ì§€' if paused else 'ì¬ìƒ'}")
                elif key == ord('a') or key == ord('A'):  # ì •í™•ë„ ëª¨ë“œ í† ê¸€
                    accuracy_mode = not accuracy_mode
                    if accuracy_mode:
                        current_conf_thresh = 0.4
                        self.detection_img_size = 832
                        print("ğŸ¯ ê³ ì •í™•ë„ ëª¨ë“œ í™œì„±í™”")
                    else:
                        current_conf_thresh = 0.6
                        self.detection_img_size = 640
                        print("âš¡ í‘œì¤€ ì†ë„ ëª¨ë“œ í™œì„±í™”")
                elif key == ord('+') or key == ord('='):  # ì‹ ë¢°ë„ ì¦ê°€
                    current_conf_thresh = min(0.9, current_conf_thresh + 0.05)
                    print(f"ğŸ“ˆ ì‹ ë¢°ë„ ì„ê³„ê°’: {current_conf_thresh:.2f}")
                elif key == ord('-'):  # ì‹ ë¢°ë„ ê°ì†Œ
                    current_conf_thresh = max(0.1, current_conf_thresh - 0.05)
                    print(f"ğŸ“‰ ì‹ ë¢°ë„ ì„ê³„ê°’: {current_conf_thresh:.2f}")
                
                # ì„±ëŠ¥ í†µê³„ ì£¼ê¸°ì  ì¶œë ¥
                if frame_count % 60 == 0 and frame_count > 0:
                    print(f"ğŸ“Š í”„ë ˆì„ {frame_count}: í‰ê·  {avg_fps:.1f}fps, "
                          f"{len(results)}ëª… ê²€ì¶œ (ì‹ ë¢°ë„: {current_conf_thresh:.2f})")
                    
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ìê°€ ì›¹ìº  í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ìµœì¢… í†µê³„
            if fps_history:
                final_avg_fps = np.mean(fps_history)
                print(f"\nğŸ“Š YOLO11L ì›¹ìº  í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
                print(f"   - ì²˜ë¦¬ëœ í”„ë ˆì„: {frame_count}")
                print(f"   - í‰ê·  FPS: {final_avg_fps:.1f}")
                print(f"   - ì‚¬ìš©ëœ ë””ë°”ì´ìŠ¤: ê²€ì¶œ({self.detection_device}), í¬ì¦ˆ({self.pose_device})")
                print(f"   - ìŠ¤í¬ë¦°ìƒ·: {screenshot_count}ê°œ ì €ì¥")
                print(f"   - ìµœì¢… ì‹ ë¢°ë„ ì„ê³„ê°’: {current_conf_thresh:.2f}")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    rtmw_config = "../configs/wholebody_2d_keypoint/rtmpose/cocktail14/rtmw-x_8xb320-270e_cocktail14-384x288.py"
    rtmw_checkpoint = "../models/rtmw-x_simcc-cocktail14_pt-ucoco_270e-384x288-f840f204_20231122.pth"
    
    try:
        print("ğŸš€ YOLO11L + RTMW XPU ê³ ì •í™•ë„ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸° í…ŒìŠ¤íŠ¸")
        print("=" * 70)
        
        # YOLO11L í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸° ìƒì„±
        inferencer = YOLO11LXPUHybridInferencer(
            rtmw_config=rtmw_config,
            rtmw_checkpoint=rtmw_checkpoint,
            detection_device="auto",
            pose_device="auto",
            optimize_for_accuracy=True
        )
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
        test_image = "winter01.jpg"
        inferencer.test_single_image(test_image)
        
        # ì‹¤ì‹œê°„ ì›¹ìº  í…ŒìŠ¤íŠ¸
        print(f"\nğŸ¥ YOLO11L ì‹¤ì‹œê°„ ì›¹ìº  í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
        choice = input().strip().lower()
        
        if choice == 'y':
            inferencer.test_webcam()
        
        print(f"\nğŸ† YOLO11L í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ íŠ¹ì§•:")
        print(f"   ğŸ¯ ìµœê³  ì •í™•ë„: Large ëª¨ë¸ë¡œ ë” ì •í™•í•œ ì‚¬ëŒ ê²€ì¶œ")
        print(f"   ğŸ” ì •ë°€ ê²€ì¶œ: ë‚®ì€ ì‹ ë¢°ë„ ì„ê³„ê°’ìœ¼ë¡œ ë†“ì¹˜ê¸° ì‰¬ìš´ ì‚¬ëŒë„ ê²€ì¶œ")
        print(f"   ğŸ“ í° ì…ë ¥ í¬ê¸°: 832pxë¡œ ë” ì„¸ë°€í•œ ê²€ì¶œ")
        print(f"   ğŸ’» XPU ê°€ì†: ê²€ì¶œê³¼ í¬ì¦ˆ ì¶”ì • ëª¨ë‘ XPU í™œìš©")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
