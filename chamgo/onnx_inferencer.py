#!/usr/bin/env python3
"""
YOLO11L + RTMW-L ONNX í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸°
Large ëª¨ë¸ì„ ì‚¬ìš©í•œ ê³ ì •í™•ë„ ì‚¬ëŒ ê²€ì¶œ + ONNX í¬ì¦ˆ ì¶”ì •
"""

import os
import torch
import cv2
import numpy as np
import time
import urllib.request
import hashlib
import zipfile
import tempfile
from pathlib import Path
from typing import List, Tuple, Optional
from collections import deque
import onnxruntime as ort

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    print("âš ï¸ ultralytics ë¯¸ì„¤ì¹˜ - pip install ultralytics")
    YOLO_AVAILABLE = False

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ìƒìˆ˜
MODEL_URLS = {
    "rtmw-dw-x-l_simcc-cocktail14_270e-384x288.onnx": {
        "type": "zip",
        "url": "https://download.openmmlab.com/mmpose/v1/projects/rtmw/onnx_sdk/rtmw-dw-x-l_simcc-cocktail14_270e-384x288_20231122.zip",
        "extracted_name": "end2end.onnx",
        "md5": None  # zip íŒŒì¼ì˜ MD5ëŠ” ë³„ë„ë¡œ í™•ì¸í•˜ì§€ ì•ŠìŒ
    },
    "rtmw-l_simcc-cocktail14_pt-ucoco_270e-384x288.onnx": {
        "type": "direct",
        "url": "https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/onnx_sdk/rtmw-l_simcc-cocktail14_pt-ucoco_270e-384x288.onnx",
        "md5": "6708b5b97b65d476b982a6e8b2fc56e1"
    }
}

def download_file_with_progress(url: str, filepath: str, expected_md5: str = None):
    """ì§„í–‰ë¥  í‘œì‹œí•˜ë©´ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {os.path.basename(filepath)}")
    print(f"   URL: {url}")
    
    def show_progress(block_num, block_size, total_size):
        downloaded = block_num * block_size
        if total_size > 0:
            percent = min(downloaded * 100.0 / total_size, 100.0)
            downloaded_mb = downloaded / (1024 * 1024)
            total_mb = total_size / (1024 * 1024)
            print(f"\r   ì§„í–‰ë¥ : {percent:.1f}% ({downloaded_mb:.1f}/{total_mb:.1f} MB)", end='')
    
    try:
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        urllib.request.urlretrieve(url, filepath, show_progress)
        print()  # ìƒˆ ì¤„
        
        # MD5 ì²´í¬ì„¬ í™•ì¸
        if expected_md5:
            print("   MD5 ì²´í¬ì„¬ í™•ì¸ ì¤‘...")
            with open(filepath, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
            
            if file_hash != expected_md5:
                os.remove(filepath)
                raise ValueError(f"MD5 ì²´í¬ì„¬ ë¶ˆì¼ì¹˜: ì˜ˆìƒ({expected_md5}) != ì‹¤ì œ({file_hash})")
            print("   âœ… MD5 ì²´í¬ì„¬ í™•ì¸ ì™„ë£Œ")
        
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {os.path.basename(filepath)}")
        return True
        
    except Exception as e:
        print(f"\nâŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        if os.path.exists(filepath):
            os.remove(filepath)
        return False

def download_and_extract_zip(url: str, target_filepath: str, extracted_name: str = "end2end.onnx"):
    """ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íŠ¹ì • íŒŒì¼ ì••ì¶• í•´ì œ"""
    print(f"ğŸ“¦ ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ ì¤‘...")
    print(f"   URL: {url}")
    print(f"   ëŒ€ìƒ íŒŒì¼: {extracted_name}")
    print(f"   ì €ì¥ ê²½ë¡œ: {target_filepath}")
    
    try:
        # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(target_filepath), exist_ok=True)
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        with tempfile.TemporaryDirectory() as temp_dir:
            zip_path = os.path.join(temp_dir, "model.zip")
            
            # ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            def show_progress(block_num, block_size, total_size):
                if total_size > 0:
                    downloaded = block_num * block_size
                    percent = min(downloaded * 100.0 / total_size, 100.0)
                    downloaded_mb = downloaded / (1024 * 1024)
                    total_mb = total_size / (1024 * 1024)
                    print(f"\r   ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ : {percent:.1f}% ({downloaded_mb:.1f}/{total_mb:.1f} MB)", end='', flush=True)
            
            print("   ğŸŒ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            urllib.request.urlretrieve(url, zip_path, show_progress)
            print("\n   âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            
            # ZIP íŒŒì¼ ì••ì¶• í•´ì œ
            print("   ğŸ“‚ ì••ì¶• í•´ì œ ì¤‘...")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                # ZIP íŒŒì¼ ë‚´ìš© í™•ì¸
                file_list = zip_ref.namelist()
                print(f"   ğŸ“‹ ZIP íŒŒì¼ ë‚´ìš©: {len(file_list)}ê°œ íŒŒì¼")
                
                # ëŒ€ìƒ íŒŒì¼ ì°¾ê¸° - ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ ì‹œë„
                target_file_in_zip = None
                possible_names = [extracted_name, f"*/{extracted_name}", f"**/{extracted_name}"]
                
                for file_path in file_list:
                    # ì •í™•í•œ ì´ë¦„ ë§¤ì¹˜
                    if os.path.basename(file_path) == extracted_name:
                        target_file_in_zip = file_path
                        break
                    # .onnx í™•ì¥ìë¡œ ëë‚˜ëŠ” íŒŒì¼ ì°¾ê¸°
                    elif file_path.endswith('.onnx') and 'end2end' in file_path:
                        target_file_in_zip = file_path
                        break
                
                if target_file_in_zip is None:
                    print(f"   âŒ ZIP íŒŒì¼ì—ì„œ ONNX ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"   ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤:")
                    for file_path in file_list[:20]:  # ì²˜ìŒ 20ê°œë§Œ ì¶œë ¥
                        print(f"      - {file_path}")
                    if len(file_list) > 20:
                        print(f"      ... ì´ {len(file_list)}ê°œ íŒŒì¼")
                    return False
                
                print(f"   âœ… ëŒ€ìƒ íŒŒì¼ ë°œê²¬: {target_file_in_zip}")
                
                # íŒŒì¼ ì••ì¶• í•´ì œ
                with zip_ref.open(target_file_in_zip) as source, open(target_filepath, 'wb') as target:
                    file_size = zip_ref.getinfo(target_file_in_zip).file_size
                    extracted_size = 0
                    chunk_size = 8192  # 8KB ì²­í¬
                    
                    while True:
                        chunk = source.read(chunk_size)
                        if not chunk:
                            break
                        target.write(chunk)
                        extracted_size += len(chunk)
                        
                        if file_size > 0:
                            percent = min(extracted_size * 100.0 / file_size, 100.0)
                            print(f"\r   ì••ì¶• í•´ì œ ì§„í–‰ë¥ : {percent:.1f}%", end='', flush=True)
                
                print("\n   âœ… ì••ì¶• í•´ì œ ì™„ë£Œ")
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                if os.path.exists(target_filepath):
                    file_size_mb = os.path.getsize(target_filepath) / (1024 * 1024)
                    print(f"   ğŸ“„ ìµœì¢… íŒŒì¼ í¬ê¸°: {file_size_mb:.1f} MB")
                    if file_size_mb > 10:  # 10MB ì´ìƒì´ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                        return True
                    else:
                        print(f"   âš ï¸ íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ ({file_size_mb:.1f}MB < 10MB)")
                        return False
                else:
                    print(f"   âŒ ì••ì¶• í•´ì œëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
    
    except Exception as e:
        print(f"\nâŒ ZIP ë‹¤ìš´ë¡œë“œ/ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
        if os.path.exists(target_filepath):
            try:
                os.remove(target_filepath)
            except:
                pass
        return False

def ensure_model_exists(model_path: str, model_name: str = None) -> bool:
    """ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ìë™ ë‹¤ìš´ë¡œë“œ (ìºì‹± ê°œì„ )"""
    # íŒŒì¼ ì¡´ì¬ ë° í¬ê¸° ê²€ì¦
    if os.path.exists(model_path):
        try:
            file_size = os.path.getsize(model_path)
            if file_size > 50 * 1024 * 1024:  # 50MB ì´ìƒì´ì–´ì•¼ ìœ íš¨í•œ ONNX ëª¨ë¸
                print(f"âœ… ëª¨ë¸ íŒŒì¼ ì¡´ì¬: {os.path.basename(model_path)} ({file_size/1024/1024:.1f}MB)")
                return True
            else:
                print(f"âš ï¸ ë¶ˆì™„ì „í•œ ëª¨ë¸ íŒŒì¼ ê°ì§€ ({file_size/1024/1024:.1f}MB < 50MB), ì¬ë‹¤ìš´ë¡œë“œ í•„ìš”")
                os.remove(model_path)  # ë¶ˆì™„ì „í•œ íŒŒì¼ ì‚­ì œ
        except OSError as e:
            print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}, ì¬ë‹¤ìš´ë¡œë“œ ì‹œë„")
    
    if model_name is None:
        model_name = os.path.basename(model_path)
    
    # ì•Œë ¤ì§„ ëª¨ë¸ì¸ì§€ í™•ì¸
    if model_name in MODEL_URLS:
        model_info = MODEL_URLS[model_name]
        print(f"ğŸ” ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_name}")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        model_dir = os.path.dirname(model_path)
        if model_dir:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
            os.makedirs(model_dir, exist_ok=True)
        else:
            # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ëª¨ë¸ ì €ì¥
            model_path = os.path.basename(model_path)
            print(f"âš ï¸ ëª¨ë¸ ê²½ë¡œ ìˆ˜ì •: {model_path}")
            
        # ë‹¤ìš´ë¡œë“œ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
        if model_info.get("type") == "zip":
            # ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ
            success = download_and_extract_zip(
                model_info["url"],
                model_path,
                model_info["extracted_name"]
            )
            # ë‹¤ìš´ë¡œë“œ ì„±ê³µ í›„ íŒŒì¼ í¬ê¸° ì¬ê²€ì¦
            if success and os.path.exists(model_path):
                final_size = os.path.getsize(model_path)
                print(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (ìµœì¢… í¬ê¸°: {final_size/1024/1024:.1f}MB)")
                return True
            return False
        else:
            # ì§ì ‘ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            success = download_file_with_progress(
                model_info["url"], 
                model_path, 
                model_info.get("md5")
            )
            # ë‹¤ìš´ë¡œë“œ ì„±ê³µ í›„ íŒŒì¼ í¬ê¸° ì¬ê²€ì¦
            if success and os.path.exists(model_path):
                final_size = os.path.getsize(model_path)
                print(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (ìµœì¢… í¬ê¸°: {final_size/1024/1024:.1f}MB)")
                return True
            return False
    else:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
        print(f"   ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥í•˜ì„¸ìš”: {model_path}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥
        print(f"   ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
        for available_model in MODEL_URLS.keys():
            print(f"      - {available_model}")
        return False

def get_available_providers():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ONNX ì‹¤í–‰ ì œê³µì í™•ì¸"""
    available = ort.get_available_providers()
    print(f"ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ONNX Providers: {', '.join(available)}")
    return available

def select_best_provider():
    """ìµœì ì˜ ONNX ì‹¤í–‰ ì œê³µì ì„ íƒ"""
    available = get_available_providers()
    
    # ìš°ì„ ìˆœìœ„: OpenVINO > CUDA > DirectML > CPU
    priority_order = [
        'OpenVINOExecutionProvider',    # Intel GPU/CPU ìµœì í™”
        'CUDAExecutionProvider',        # NVIDIA GPU
        'DmlExecutionProvider',         # DirectML (Windows GPU)
        'CPUExecutionProvider'          # CPU í´ë°±
    ]
    
    for provider in priority_order:
        if provider in available:
            print(f"âœ… ì„ íƒëœ ONNX Provider: {provider}")
            return provider
    
    return 'CPUExecutionProvider'

class YOLO11LRTMWONNXInferencer:
    """YOLO11L + RTMW ONNX í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸° (ZIP ë‹¤ìš´ë¡œë“œ ì§€ì›)"""
    
    def __init__(self, 
                 rtmw_onnx_path: str,
                 detection_device: str = "auto",
                 pose_device: str = "auto",
                 optimize_for_accuracy: bool = True):
        """
        Args:
            rtmw_onnx_path: RTMW ONNX ëª¨ë¸ ê²½ë¡œ (ZIP íŒŒì¼ ìë™ ë‹¤ìš´ë¡œë“œ/ì••ì¶•í•´ì œ ì§€ì›)
            detection_device: ê²€ì¶œ ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda', 'xpu')
            pose_device: í¬ì¦ˆ ì¶”ì • ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda', 'openvino')
            optimize_for_accuracy: ì •í™•ë„ ìµœì í™” ì—¬ë¶€
        """
        if not YOLO_AVAILABLE:
            raise ImportError("ultralyticsê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install ultralytics")
            
        self.rtmw_onnx_path = rtmw_onnx_path
        self.yolo_model_name = "yolo11l.pt"  # Large ëª¨ë¸ ì‚¬ìš©
        self.optimize_for_accuracy = optimize_for_accuracy
        
        # ë””ë°”ì´ìŠ¤ ê²°ì •
        self.detection_device = self._determine_detection_device(detection_device)
        self.pose_provider = self._determine_pose_provider(pose_device)
        
        # ëª¨ë¸ ì´ë¦„ í™•ì¸ (ê²½ë¡œì—ì„œ ì¶”ì¶œ)
        model_filename = os.path.basename(rtmw_onnx_path)
        model_type = "RTMW-DW-X-L" if "dw-x-l" in model_filename else "RTMW"
        
        print(f"ğŸš€ YOLO11L + {model_type} ONNX í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸° ì´ˆê¸°í™”:")
        print(f"   - YOLO ëª¨ë¸: YOLO11L (Large - ê³ ì •í™•ë„)")
        print(f"   - RTMW ëª¨ë¸: {model_type} 384x288 ONNX (ZIP ë‹¤ìš´ë¡œë“œ ì§€ì›)")
        print(f"   - RTMW ëª¨ë¸: RTMW-L 384x288 ONNX")
        print(f"   - ê²€ì¶œ ë””ë°”ì´ìŠ¤: {self.detection_device}")
        print(f"   - í¬ì¦ˆ Provider: {self.pose_provider}")
        print(f"   - ì •í™•ë„ ìµœì í™”: {'ON' if optimize_for_accuracy else 'OFF'}")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._init_detection_model()
        self._init_pose_model()
        
        # ì„±ëŠ¥ í†µê³„
        self.inference_times = {
            'detection': [],
            'pose': [],
            'total': []
        }
        
        # ìµœì í™” ì„¤ì •
        self._setup_optimization()
        
    def _determine_detection_device(self, device: str) -> str:
        """ê²€ì¶œ ë””ë°”ì´ìŠ¤ ìë™ ê²°ì •"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def _determine_pose_provider(self, device: str) -> str:
        """í¬ì¦ˆ ì¶”ì • Provider ìë™ ê²°ì •"""
        if device == "auto":
            return select_best_provider()
        elif device == "openvino":
            return 'OpenVINOExecutionProvider'
        elif device == "cuda":
            return 'CUDAExecutionProvider'
        elif device == "directml":
            return 'DmlExecutionProvider'
        else:
            return 'CPUExecutionProvider'
    
    def _setup_optimization(self):
        """ìµœì í™” ì„¤ì • - ì •í™•ë„ ìš°ì„ """
        if self.optimize_for_accuracy:
            print("ğŸ¯ ì •í™•ë„ ìµœì í™” ì„¤ì • ì ìš© ì¤‘...")
            
            # YOLO11L ì •í™•ë„ ìš°ì„  íŒŒë¼ë¯¸í„°
            self.yolo_conf_thresh = 0.4     # ë‚®ì€ ì‹ ë¢°ë„ (ë” ë§ì€ ê²€ì¶œ)
            self.yolo_iou_thresh = 0.6      # ì ë‹¹í•œ IoU ì„ê³„ê°’
            self.yolo_max_det = 50          # ë” ë§ì€ ê²€ì¶œ í—ˆìš©
            self.yolo_classes = [0]         # ì‚¬ëŒ í´ë˜ìŠ¤ë§Œ
            
            # ì´ë¯¸ì§€ í¬ê¸° ìµœì í™”
            self.detection_img_size = 832   # Large ëª¨ë¸ì— ì í•©í•œ í° ì…ë ¥ í¬ê¸°
            self.pose_input_size = (288, 384)  # RTMW-L 384x288 ì…ë ¥ í¬ê¸°
            
            print("âœ… ì •í™•ë„ ìµœì í™” ì„¤ì • ì™„ë£Œ")
        else:
            # ê· í˜• ì„¤ì •
            self.yolo_conf_thresh = 0.5
            self.yolo_iou_thresh = 0.7
            self.yolo_max_det = 100
            self.yolo_classes = None
            self.detection_img_size = 640
            self.pose_input_size = (288, 384)
    
    def _init_detection_model(self):
        """YOLO11L ê²€ì¶œ ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"ğŸ”§ YOLO11L ê²€ì¶œ ëª¨ë¸ ë¡œë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {self.detection_device})")
        start_time = time.time()
        
        try:
            # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
            models_dir = os.path.join(os.path.dirname(__file__), "..", "models")
            os.makedirs(models_dir, exist_ok=True)
            
            # YOLO11L ëª¨ë¸ ê²½ë¡œ
            model_path = os.path.join(models_dir, self.yolo_model_name)
            
            # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(model_path):
                print(f"ğŸ“¥ YOLO11L ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ì¤‘: {self.yolo_model_name}")
                print("   (Ultralyticsì—ì„œ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤)")
            
            # YOLO ëª¨ë¸ ë¡œë“œ (ìë™ ë‹¤ìš´ë¡œë“œ í¬í•¨)
            self.detection_model = YOLO(self.yolo_model_name)
            
            # ëª¨ë¸ì„ ì§€ì •ëœ ìœ„ì¹˜ì— ë³µì‚¬ (ë‹¤ìŒì— ë” ë¹ ë¥¸ ë¡œë”©ì„ ìœ„í•´)
            if not os.path.exists(model_path) and hasattr(self.detection_model, 'ckpt_path'):
                try:
                    import shutil
                    if os.path.exists(self.detection_model.ckpt_path):
                        shutil.copy2(self.detection_model.ckpt_path, model_path)
                        print(f"ğŸ’¾ ëª¨ë¸ ë³µì‚¬ë¨: {model_path}")
                except Exception as e:
                    print(f"âš ï¸ ëª¨ë¸ ë³µì‚¬ ì‹¤íŒ¨ (ì •ìƒ ì‘ë™): {e}")
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            if self.detection_device != "cpu":
                try:
                    self.detection_model.to(self.detection_device)
                    print(f"âœ… YOLO11L {self.detection_device.upper()} ëª¨ë“œ í™œì„±í™”")
                except Exception as e:
                    print(f"âš ï¸ YOLO11L {self.detection_device.upper()} ì‹¤íŒ¨, CPUë¡œ í´ë°±: {e}")
                    self.detection_device = "cpu"
                    self.detection_model.to('cpu')
            
            init_time = time.time() - start_time
            print(f"âœ… YOLO11L ê²€ì¶œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {init_time:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"âŒ YOLO11L ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.detection_model = None
            print(f"ğŸ”„ ê°„ë‹¨í•œ ê²€ì¶œê¸°ë¡œ í´ë°±")
    
    def _init_pose_model(self):
        """RTMW-L ONNX í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"ğŸ”§ RTMW-L ONNX í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì¤‘... (Provider: {self.pose_provider})")
        start_time = time.time()
        
        try:
            # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ìë™ ë‹¤ìš´ë¡œë“œ
            model_name = os.path.basename(self.rtmw_onnx_path)
            if not ensure_model_exists(self.rtmw_onnx_path, model_name):
                # ê¸°ë³¸ ê²½ë¡œì—ì„œë„ ì‹œë„
                default_path = os.path.join(os.path.dirname(__file__), "..", "models", model_name)
                if not ensure_model_exists(default_path, model_name):
                    raise FileNotFoundError(f"RTMW-L ONNX ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.rtmw_onnx_path}")
                else:
                    self.rtmw_onnx_path = default_path
            
            # ONNX ì„¸ì…˜ ì˜µì…˜ ì„¤ì •
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # Provider ì„¤ì •
            providers = [self.pose_provider]
            if self.pose_provider != 'CPUExecutionProvider':
                providers.append('CPUExecutionProvider')  # í´ë°±ìš©
            
            # ONNX ì„¸ì…˜ ìƒì„±
            self.pose_session = ort.InferenceSession(
                self.rtmw_onnx_path,
                sess_options=sess_options,
                providers=providers
            )
            
            # ì…ë ¥/ì¶œë ¥ ì •ë³´ í™•ì¸
            self.input_name = self.pose_session.get_inputs()[0].name
            self.input_shape = self.pose_session.get_inputs()[0].shape
            self.output_names = [output.name for output in self.pose_session.get_outputs()]
            
            print(f"ğŸ“‹ ONNX ëª¨ë¸ ì •ë³´:")
            print(f"   - ì…ë ¥: {self.input_name} {self.input_shape}")
            print(f"   - ì¶œë ¥: {len(self.output_names)}ê°œ")
            print(f"   - Provider: {self.pose_session.get_providers()[0]}")
            
            init_time = time.time() - start_time
            print(f"âœ… RTMW-L ONNX í¬ì¦ˆ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {init_time:.2f}ì´ˆ")
            
        except Exception as e:
            print(f"âŒ RTMW-L ONNX ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # CPU í´ë°± ì‹œë„
            print("ğŸ”„ CPU Providerë¡œ í´ë°± ì‹œë„...")
            try:
                self.pose_session = ort.InferenceSession(
                    self.rtmw_onnx_path,
                    providers=['CPUExecutionProvider']
                )
                self.input_name = self.pose_session.get_inputs()[0].name
                self.input_shape = self.pose_session.get_inputs()[0].shape
                self.output_names = [output.name for output in self.pose_session.get_outputs()]
                self.pose_provider = 'CPUExecutionProvider'
                print(f"âœ… CPU í´ë°± ì„±ê³µ")
            except Exception as e2:
                print(f"âŒ CPU í´ë°±ë„ ì‹¤íŒ¨: {e2}")
                raise e2
    
    def detect_persons_high_accuracy(self, image: np.ndarray) -> List[List[float]]:
        """ê³ ì •í™•ë„ ì‚¬ëŒ ê²€ì¶œ"""
        if self.detection_model is None:
            return self._simple_person_detection(image)
        
        try:
            start_time = time.time()
            
            # YOLO11L ì¶”ë¡  (ê³ ì •í™•ë„ íŒŒë¼ë¯¸í„°)
            results = self.detection_model(
                image,
                conf=self.yolo_conf_thresh,
                iou=self.yolo_iou_thresh,
                max_det=self.yolo_max_det,
                classes=self.yolo_classes,
                verbose=False,
                imgsz=self.detection_img_size
            )
            
            detection_time = time.time() - start_time
            self.inference_times['detection'].append(detection_time)
            
            # ì‚¬ëŒ(class 0) ê²€ì¶œ ê²°ê³¼ ì¶”ì¶œ
            person_boxes = []
            
            for result in results:
                boxes = result.boxes
                if boxes is not None and len(boxes) > 0:
                    person_coords = boxes.xyxy
                    person_confs = boxes.conf
                    
                    # ì‹ ë¢°ë„ ì¬í•„í„°ë§
                    conf_mask = person_confs >= self.yolo_conf_thresh
                    if conf_mask.any():
                        filtered_boxes = person_coords[conf_mask]
                        filtered_confs = person_confs[conf_mask]
                        
                        # numpy ë³€í™˜
                        if isinstance(filtered_boxes, torch.Tensor):
                            filtered_boxes = filtered_boxes.cpu().numpy()
                            filtered_confs = filtered_confs.cpu().numpy()
                        
                        # ì‹ ë¢°ë„ìˆœ ì •ë ¬
                        sorted_indices = np.argsort(filtered_confs)[::-1]
                        sorted_boxes = filtered_boxes[sorted_indices]
                        
                        person_boxes.extend(sorted_boxes.tolist())
            
            return person_boxes if person_boxes else self._simple_person_detection(image)
            
        except Exception as e:
            print(f"âŒ YOLO11L ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return self._simple_person_detection(image)
    
    def _simple_person_detection(self, image: np.ndarray) -> List[List[float]]:
        """ê°„ë‹¨í•œ ì‚¬ëŒ ê²€ì¶œ (í´ë°±)"""
        h, w = image.shape[:2]
        margin_w = int(w * 0.1)
        margin_h = int(h * 0.1)
        bbox = [margin_w, margin_h, w - margin_w, h - margin_h]
        return [bbox]
    
    def _preprocess_image_for_pose(self, crop_image: np.ndarray) -> Optional[np.ndarray]:
        """í¬ì¦ˆ ì¶”ì •ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # ì…ë ¥ ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬
            if crop_image is None or crop_image.size == 0:
                print(f"âš ï¸ ë¹ˆ í¬ë¡­ ì´ë¯¸ì§€ê°€ ì „ë‹¬ë¨")
                return None
            
            h, w = crop_image.shape[:2]
            if h == 0 or w == 0:
                print(f"âš ï¸ í¬ê¸°ê°€ 0ì¸ í¬ë¡­ ì´ë¯¸ì§€: {h}x{w}")
                return None
            
            # RTMW-L 384x288 í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            try:
                resized = cv2.resize(crop_image, self.pose_input_size)  # (288, 384)
            except cv2.error as e:
                print(f"âš ï¸ ë¦¬ì‚¬ì´ì¦ˆ ì‹¤íŒ¨: {e}, ì´ë¯¸ì§€ í¬ê¸°: {crop_image.shape}")
                return None
            
            # BGR to RGB
            rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
            
            # ì •ê·œí™” (0-1 ë²”ìœ„)
            normalized = rgb_image.astype(np.float32) / 255.0
            
            # í‘œì¤€í™” (ImageNet í‰ê· /í‘œì¤€í¸ì°¨)
            mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
            std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
            standardized = (normalized - mean) / std
            
            # ì°¨ì› ë³€ê²½: HWC -> CHW
            transposed = standardized.transpose(2, 0, 1)
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: CHW -> BCHW
            batched = np.expand_dims(transposed, axis=0).astype(np.float32)
            
            return batched
        
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _postprocess_pose_output(self, outputs: List[np.ndarray], 
                               original_crop_shape: Tuple[int, int]) -> Tuple[np.ndarray, np.ndarray]:
        """í¬ì¦ˆ ì¶”ì • ì¶œë ¥ í›„ì²˜ë¦¬"""
        try:
            # RTMW ì¶œë ¥ í˜•íƒœì— ë”°ë¼ ì¡°ì • í•„ìš”
            # ì¼ë°˜ì ìœ¼ë¡œ keypointsì™€ heatmap/simcc ì¶œë ¥ì´ ìˆìŒ
            
            if len(outputs) >= 2:
                # SimCC ë°©ì‹ì˜ ê²½ìš°
                pred_x = outputs[0]  # x ì¢Œí‘œ ì˜ˆì¸¡
                pred_y = outputs[1]  # y ì¢Œí‘œ ì˜ˆì¸¡
                
                # ìµœëŒ€ê°’ ìœ„ì¹˜ ì°¾ê¸°
                keypoints_x = np.argmax(pred_x[0], axis=1)
                keypoints_y = np.argmax(pred_y[0], axis=1)
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                scores_x = np.max(pred_x[0], axis=1)
                scores_y = np.max(pred_y[0], axis=1)
                scores = np.minimum(scores_x, scores_y)
                
                # ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ (ëª¨ë¸ ì…ë ¥ í¬ê¸°ì—ì„œ í¬ë¡­ ì´ë¯¸ì§€ í¬ê¸°ë¡œ)
                scale_x = original_crop_shape[1] / self.pose_input_size[0]  # width
                scale_y = original_crop_shape[0] / self.pose_input_size[1]  # height
                
                keypoints_x = keypoints_x * scale_x
                keypoints_y = keypoints_y * scale_y
                
                # í‚¤í¬ì¸íŠ¸ ë°°ì—´ ìƒì„±
                keypoints = np.stack([keypoints_x, keypoints_y], axis=1)
                
                return keypoints, scores
            else:
                # ë‹¤ë¥¸ ì¶œë ¥ í˜•íƒœì˜ ê²½ìš°
                print(f"âš ï¸ ì˜ˆìƒí•˜ì§€ ëª»í•œ ì¶œë ¥ í˜•íƒœ: {len(outputs)}ê°œ ì¶œë ¥")
                return np.zeros((133, 2)), np.zeros(133)
                
        except Exception as e:
            print(f"âŒ í¬ì¦ˆ ì¶œë ¥ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return np.zeros((133, 2)), np.zeros(133)
    
    def estimate_pose_on_crop(self, crop_image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """í¬ë¡­ëœ ì´ë¯¸ì§€ì—ì„œ ONNX í¬ì¦ˆ ì¶”ì •"""
        try:
            start_time = time.time()
            
            # ì „ì²˜ë¦¬
            input_tensor = self._preprocess_image_for_pose(crop_image)
            
            # ONNX ì¶”ë¡ 
            outputs = self.pose_session.run(
                self.output_names,
                {self.input_name: input_tensor}
            )
            
            # í›„ì²˜ë¦¬
            keypoints, scores = self._postprocess_pose_output(outputs, crop_image.shape[:2])
            
            pose_time = time.time() - start_time
            self.inference_times['pose'].append(pose_time)
            
            return keypoints, scores
            
        except Exception as e:
            print(f"âŒ ONNX í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
            return np.zeros((133, 2)), np.zeros(133)
    
    def estimate_pose_batch(self, crop_images: List[np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:
        """ë°°ì¹˜ í¬ì¦ˆ ì¶”ì • (ONNX)"""
        if not crop_images:
            return np.array([]), np.array([])
        
        batch_keypoints = []
        batch_scores = []
        
        try:
            # ë°°ì¹˜ ì „ì²˜ë¦¬
            batch_inputs = []
            valid_indices = []  # ìœ íš¨í•œ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ì¶”ì 
            original_shapes = []  # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì¶”ì 
            
            for i, crop_image in enumerate(crop_images):
                # ì´ë¯¸ ì „ì²˜ë¦¬ëœ í…ì„œì¸ì§€ í™•ì¸
                if len(crop_image.shape) == 4 and crop_image.shape[1] == 3:
                    # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°°ì¹˜ í…ì„œ (B, C, H, W)
                    batch_inputs.append(crop_image[0])  # ë°°ì¹˜ ì°¨ì› ì œê±°
                    valid_indices.append(i)
                    original_shapes.append((384, 288))  # RTMW ì…ë ¥ í¬ê¸°
                elif len(crop_image.shape) == 3 and crop_image.shape[0] == 3:
                    # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë‹¨ì¼ í…ì„œ (C, H, W)
                    batch_inputs.append(crop_image)
                    valid_indices.append(i)
                    original_shapes.append((384, 288))  # RTMW ì…ë ¥ í¬ê¸°
                else:
                    # ì›ë³¸ ì´ë¯¸ì§€ì¸ ê²½ìš° ì „ì²˜ë¦¬ í•„ìš”
                    input_tensor = self._preprocess_image_for_pose(crop_image)
                    if input_tensor is not None:
                        batch_inputs.append(input_tensor[0])  # ë°°ì¹˜ ì°¨ì› ì œê±°
                        valid_indices.append(i)
                        original_shapes.append(crop_image.shape[:2])
                    else:
                        print(f"âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ì—ì„œ ì´ë¯¸ì§€ {i} ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            
            if not batch_inputs:
                print(f"âš ï¸ ìœ íš¨í•œ ì…ë ¥ ì´ë¯¸ì§€ê°€ ì—†ìŒ")
                return np.array([]), np.array([])
            
            batch_tensor = np.stack(batch_inputs, axis=0)
            
            # ë°°ì¹˜ ì¶”ë¡ 
            outputs = self.pose_session.run(
                self.output_names,
                {self.input_name: batch_tensor}
            )
            
            # ê° ìœ íš¨í•œ ì´ë¯¸ì§€ì— ëŒ€í•´ í›„ì²˜ë¦¬
            batch_idx = 0
            for i, crop_image in enumerate(crop_images):
                if i in valid_indices:
                    # ë°°ì¹˜ ì¶œë ¥ì—ì„œ í•´ë‹¹ ê²°ê³¼ ì¶”ì¶œ
                    image_outputs = [output[batch_idx:batch_idx+1] for output in outputs]
                    keypoints, scores = self._postprocess_pose_output(image_outputs, original_shapes[batch_idx])
                    batch_keypoints.append(keypoints)
                    batch_scores.append(scores)
                    batch_idx += 1
                else:
                    # ì‹¤íŒ¨í•œ ì´ë¯¸ì§€ëŠ” ê¸°ë³¸ê°’
                    batch_keypoints.append(np.zeros((133, 2)))
                    batch_scores.append(np.zeros(133))
            
            return np.array(batch_keypoints), np.array(batch_scores)
        
        except Exception as e:
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨, ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±: {e}")
            # í´ë°±: ê°œë³„ ì²˜ë¦¬
            for crop_image in crop_images:
                keypoints, scores = self.estimate_pose_on_crop(crop_image)
                batch_keypoints.append(keypoints)
                batch_scores.append(scores)
        
            return np.array(batch_keypoints), np.array(batch_scores)
    
    def process_frame(self, image: np.ndarray, conf_thresh: float = None) -> Tuple[np.ndarray, List[Tuple[np.ndarray, np.ndarray, List[float]]]]:
        """í”„ë ˆì„ ì²˜ë¦¬ (ê³ ì •í™•ë„ ê²€ì¶œ + ONNX í¬ì¦ˆ ì¶”ì •)"""
        start_time = time.time()
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •
        if conf_thresh is None:
            conf_thresh = self.yolo_conf_thresh
        
        # 1. ê³ ì •í™•ë„ ì‚¬ëŒ ê²€ì¶œ
        person_boxes = self.detect_persons_high_accuracy(image)
        
        # 2. ê° ì‚¬ëŒì— ëŒ€í•´ í¬ì¦ˆ ì¶”ì •
        results = []
        for bbox in person_boxes:
            # ë°”ìš´ë”©ë°•ìŠ¤ì—ì„œ í¬ë¡­
            x1, y1, x2, y2 = map(int, bbox)
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)
            
            if x2 > x1 and y2 > y1:
                crop_image = image[y1:y2, x1:x2]
                keypoints, scores = self.estimate_pose_on_crop(crop_image)
                
                # í‚¤í¬ì¸íŠ¸ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                keypoints[:, 0] += x1
                keypoints[:, 1] += y1
                
                results.append((keypoints, scores, bbox))
        
        total_time = time.time() - start_time
        self.inference_times['total'].append(total_time)
        
        # 3. ì‹œê°í™”
        vis_image = self.visualize_results(image, results)
        
        return vis_image, results
    
    def visualize_results(self, image: np.ndarray, results: List[Tuple[np.ndarray, np.ndarray, List[float]]]) -> np.ndarray:
        """ê²°ê³¼ ì‹œê°í™”"""
        vis_image = image.copy()
        
        for i, (keypoints, scores, bbox) in enumerate(results):
            # ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            x1, y1, x2, y2 = map(int, bbox)
            color = (0, 255, 0) if i == 0 else (255, 0, 255)
            cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, 2)
            
            # ì‚¬ëŒ ë²ˆí˜¸ í‘œì‹œ
            cv2.putText(vis_image, f"Person {i+1}", (x1, y1-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            
            # í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° (ì‹ ë¢°ë„ë³„ ìƒ‰ìƒ)
            for j, (kpt, score) in enumerate(zip(keypoints, scores)):
                if score > 0.3:
                    x, y = int(kpt[0]), int(kpt[1])
                    if 0 <= x < vis_image.shape[1] and 0 <= y < vis_image.shape[0]:
                        # ì‹ ë¢°ë„ì— ë”°ë¥¸ ìƒ‰ìƒ
                        if score > 0.8:
                            kpt_color = (0, 255, 0)    # ë†’ì€ ì‹ ë¢°ë„: ì´ˆë¡
                        elif score > 0.6:
                            kpt_color = (0, 255, 255)  # ì¤‘ê°„ ì‹ ë¢°ë„: ë…¸ë‘
                        else:
                            kpt_color = (0, 0, 255)    # ë‚®ì€ ì‹ ë¢°ë„: ë¹¨ê°•
                        
                        cv2.circle(vis_image, (x, y), 3, kpt_color, -1)
        
        # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
        if self.inference_times['total']:
            fps = 1.0 / self.inference_times['total'][-1]
            cv2.putText(vis_image, f"FPS: {fps:.1f}", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # ëª¨ë¸ ì •ë³´
        cv2.putText(vis_image, "YOLO11L + RTMW-L ONNX", 
                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(vis_image, f"Detection: {self.detection_device.upper()}", 
                   (10, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(vis_image, f"Pose: {self.pose_provider.split('ExecutionProvider')[0]}", 
                   (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return vis_image
    
    def benchmark_performance(self, image: np.ndarray, num_runs: int = 15) -> dict:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print(f"ğŸƒ YOLO11L + RTMW-L ONNX ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ({num_runs}íšŒ)...")
        
        # ì›Œë°ì—…
        for _ in range(3):
            self.process_frame(image)
        
        # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
        self.inference_times = {'detection': [], 'pose': [], 'total': []}
        
        for i in range(num_runs):
            self.process_frame(image)
            if (i + 1) % 5 == 0:
                print(f"   ì§„í–‰ë¥ : {i+1}/{num_runs}")
        
        # í†µê³„ ê³„ì‚°
        stats = {}
        for key, times in self.inference_times.items():
            if times:
                stats[key] = {
                    'mean': np.mean(times),
                    'std': np.std(times),
                    'min': np.min(times),
                    'max': np.max(times),
                    'fps': 1.0 / np.mean(times) if key == 'total' else None
                }
        
        return stats
    
    def test_single_image(self, image_path: str):
        """ë‹¨ì¼ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== YOLO11L + RTMW-L ONNX í…ŒìŠ¤íŠ¸: {os.path.basename(image_path)} ===")
        
        if not os.path.exists(image_path):
            print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path}")
            return
            
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(image_path)
        if image is None:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            return
        
        print(f"ğŸ“· ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
        
        # ì²˜ë¦¬
        vis_image, results = self.process_frame(image)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ… ê²€ì¶œëœ ì‚¬ëŒ ìˆ˜: {len(results)}")
        
        for i, (keypoints, scores, bbox) in enumerate(results):
            valid_kpts = np.sum(scores > 0.3)
            high_conf_kpts = np.sum(scores > 0.8)
            bbox_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
            print(f"   ì‚¬ëŒ {i+1}: {valid_kpts}/133 í‚¤í¬ì¸íŠ¸ (ì‹ ë¢°ë„ > 0.3), {high_conf_kpts} ê³ ì‹ ë¢°ë„")
            print(f"            ë°”ìš´ë”©ë°•ìŠ¤ í¬ê¸°: {bbox_area:.0f}pxÂ²")
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        stats = self.benchmark_performance(image, num_runs=15)
        
        print(f"\nğŸ“Š ì„±ëŠ¥ í†µê³„:")
        for stage, stat in stats.items():
            if stat:
                print(f"   {stage}:")
                print(f"     - í‰ê· : {stat['mean']*1000:.1f}ms")
                print(f"     - ìµœì†Œ/ìµœëŒ€: {stat['min']*1000:.1f}/{stat['max']*1000:.1f}ms")
                if stat['fps']:
                    print(f"     - FPS: {stat['fps']:.1f}")
        
        # ê²°ê³¼ ì €ì¥
        output_path = f"yolo11l_rtmw_onnx_result_{os.path.basename(image_path)}"
        cv2.imwrite(output_path, vis_image)
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        
        return vis_image, results, stats

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì • - ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ í™•ì¸
    models_dir = os.path.join(os.path.dirname(__file__), "..", "models")
    # ìƒˆ ê³ ì„±ëŠ¥ ëª¨ë¸ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
    rtmw_model_name = "rtmw-dw-x-l_simcc-cocktail14_270e-384x288.onnx"
    
    # ê°€ëŠ¥í•œ ëª¨ë¸ ê²½ë¡œë“¤
    possible_paths = [
        os.path.join(models_dir, rtmw_model_name),
        os.path.join("../models", rtmw_model_name),
        os.path.join("models", rtmw_model_name),
        rtmw_model_name  # í˜„ì¬ ë””ë ‰í† ë¦¬
    ]
    
    rtmw_onnx_path = None
    for path in possible_paths:
        if os.path.exists(path):
            rtmw_onnx_path = path
            break
    
    # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš© (ìë™ ë‹¤ìš´ë¡œë“œë¨)
    if rtmw_onnx_path is None:
        rtmw_onnx_path = possible_paths[0]  # ì²« ë²ˆì§¸ ê²½ë¡œ ì‚¬ìš©
    
    try:
        print("ğŸš€ YOLO11L + RTMW-DW-X-L ONNX í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸° í…ŒìŠ¤íŠ¸")
        print("=" * 70)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ONNX Provider í™•ì¸
        get_available_providers()
        
        # YOLO11L + RTMW-DW-X-L ONNX í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡ ê¸° ìƒì„±
        inferencer = YOLO11LRTMWONNXInferencer(
            rtmw_onnx_path=rtmw_onnx_path,
            detection_device="auto",
            pose_device="auto",
            optimize_for_accuracy=True
        )
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ - ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œ í™•ì¸
        test_image_names = ["winter01.jpg", "test.jpg", "demo.jpg", "sample.jpg"]
        test_image = None
        
        for img_name in test_image_names:
            possible_img_paths = [
                img_name,  # í˜„ì¬ ë””ë ‰í† ë¦¬
                os.path.join("../demo/resources", img_name),
                os.path.join("demo/resources", img_name),
                os.path.join("resources", img_name)
            ]
            
            for img_path in possible_img_paths:
                if os.path.exists(img_path):
                    test_image = img_path
                    break
            
            if test_image:
                break
        
        if test_image:
            inferencer.test_single_image(test_image)
        else:
            print("âš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹ìº  í…ŒìŠ¤íŠ¸ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
            print("   í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•˜ë ¤ë©´ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥í•˜ì„¸ìš”:")
            for img_name in test_image_names:
                print(f"   - {img_name}")
        
        # ì‹¤ì‹œê°„ ì›¹ìº  í…ŒìŠ¤íŠ¸
        print(f"\nğŸ¥ YOLO11L + RTMW-L ONNX ì‹¤ì‹œê°„ ì›¹ìº  í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
        try:
            choice = input().strip().lower()
        except (KeyboardInterrupt, EOFError):
            choice = 'n'
        
        if choice == 'y':
            inferencer.test_webcam()
        
        print(f"\nğŸ† YOLO11L + RTMW ONNX ì‹œìŠ¤í…œ íŠ¹ì§•:")
        print(f"   ğŸ¯ ìµœê³  ì •í™•ë„: Large ëª¨ë¸ë¡œ ë” ì •í™•í•œ ì‚¬ëŒ ê²€ì¶œ")
        print(f"   ğŸ” ì •ë°€ ê²€ì¶œ: ë‚®ì€ ì‹ ë¢°ë„ ì„ê³„ê°’ìœ¼ë¡œ ë†“ì¹˜ê¸° ì‰¬ìš´ ì‚¬ëŒë„ ê²€ì¶œ")
        print(f"   ğŸ“ ìµœì  ì…ë ¥: YOLO 832px, RTMW 384x288")
        print(f"   âš¡ ONNX ê°€ì†: OpenVINO/CUDA/DirectML í™œìš©")
        print(f"   ğŸ’¨ ë¹ ë¥¸ ì¶”ë¡ : ONNX Runtime ìµœì í™”")
        print(f"   ğŸ“¦ ZIP ì§€ì›: ìë™ ZIP ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ")
        print(f"   ğŸ“¥ ìë™ ë‹¤ìš´ë¡œë“œ: í•„ìš”í•œ ëª¨ë¸ ìë™ ì„¤ì¹˜")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

    def test_webcam(self, camera_id: int = 0, window_size: Tuple[int, int] = (1280, 720)):
        """ì‹¤ì‹œê°„ ì›¹ìº  í…ŒìŠ¤íŠ¸ - YOLO11L + RTMW-L ONNX"""
        print(f"\n=== YOLO11L + RTMW-L ONNX ì‹¤ì‹œê°„ ì›¹ìº  í…ŒìŠ¤íŠ¸ (ì¹´ë©”ë¼ ID: {camera_id}) ===")
        print("ğŸ“¹ ì›¹ìº  ì—°ê²° ì¤‘...")
        
        cap = cv2.VideoCapture(camera_id)
        if not cap.isOpened():
            print(f"âŒ ì›¹ìº  ì—´ê¸° ì‹¤íŒ¨ (ì¹´ë©”ë¼ ID: {camera_id})")
            return
        
        # ì›¹ìº  ì„¤ì •
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, window_size[0])
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, window_size[1])
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))
        
        # ì‹¤ì œ ì›¹ìº  í•´ìƒë„ í™•ì¸
        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = cap.get(cv2.CAP_PROP_FPS)
        
        print(f"âœ… ì›¹ìº  ì—°ê²° ì„±ê³µ: {actual_width}x{actual_height}, {actual_fps:.1f}fps")
        print(f"ğŸ® ì¡°ì‘ë²•:")
        print(f"   - ESC: ì¢…ë£Œ")
        print(f"   - S: í˜„ì¬ í”„ë ˆì„ ìŠ¤í¬ë¦°ìƒ·")
        print(f"   - SPACE: ì¼ì‹œì •ì§€/ì¬ìƒ")
        print(f"   - A: ì •í™•ë„ ëª¨ë“œ í† ê¸€ (ë†’ìŒ/í‘œì¤€)")
        print(f"   - +/-: ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì •")
        
        # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
        frame_count = 0
        fps_history = deque(maxlen=30)
        paused = False
        screenshot_count = 0
        accuracy_mode = True
        current_conf_thresh = self.yolo_conf_thresh
        
        try:
            while True:
                if not paused:
                    ret, frame = cap.read()
                    if not ret:
                        print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                        break
                    
                    # í”„ë ˆì„ ì²˜ë¦¬
                    start_time = time.time()
                    vis_frame, results = self.process_frame(frame, conf_thresh=current_conf_thresh)
                    process_time = time.time() - start_time
                    
                    # FPS ê³„ì‚°
                    fps = 1.0 / process_time if process_time > 0 else 0
                    fps_history.append(fps)
                    avg_fps = np.mean(fps_history) if fps_history else 0
                    
                    # ì¶”ê°€ ì •ë³´ í‘œì‹œ
                    info_y = 140
                    cv2.putText(vis_frame, f"Avg FPS: {avg_fps:.1f}", 
                               (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    cv2.putText(vis_frame, f"Provider: {self.pose_provider.split('ExecutionProvider')[0]}", 
                               (10, info_y + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    cv2.putText(vis_frame, f"Mode: {'High Accuracy' if accuracy_mode else 'Standard'}", 
                               (10, info_y + 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    cv2.putText(vis_frame, f"Conf: {current_conf_thresh:.2f}", 
                               (10, info_y + 75), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    
                    # ê²€ì¶œëœ ì‚¬ëŒ ì •ë³´
                    if results is not None and len(results) > 0:
                        person_info = f"Persons: {len(results)}"
                        for i, (_, scores, _) in enumerate(results):
                            valid_kpts = np.sum(scores > 0.3)
                            high_conf_kpts = np.sum(scores > 0.8)
                            person_info += f" | P{i+1}: {valid_kpts}/133 ({high_conf_kpts} high)"
                        cv2.putText(vis_frame, person_info, 
                                   (10, info_y + 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)
                    
                    frame_count += 1
                
                # í™”ë©´ í‘œì‹œ
                cv2.imshow('YOLO11L + RTMW-L ONNX Real-time', vis_frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                
                if key == 27:  # ESC
                    break
                elif key == ord('s') or key == ord('S'):  # ìŠ¤í¬ë¦°ìƒ·
                    screenshot_name = f"yolo11l_rtmw_onnx_screenshot_{screenshot_count:04d}.jpg"
                    cv2.imwrite(screenshot_name, vis_frame)
                    print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_name}")
                    screenshot_count += 1
                elif key == ord(' '):  # ì¼ì‹œì •ì§€/ì¬ìƒ
                    paused = not paused
                    print(f"â¸ï¸ {'ì¼ì‹œì •ì§€' if paused else 'ì¬ìƒ'}")
                elif key == ord('a') or key == ord('A'):  # ì •í™•ë„ ëª¨ë“œ í† ê¸€
                    accuracy_mode = not accuracy_mode
                    if accuracy_mode:
                        current_conf_thresh = 0.4
                        self.detection_img_size = 832
                        print("ğŸ¯ ê³ ì •í™•ë„ ëª¨ë“œ í™œì„±í™”")
                    else:
                        current_conf_thresh = 0.6
                        self.detection_img_size = 640
                        print("âš¡ í‘œì¤€ ì†ë„ ëª¨ë“œ í™œì„±í™”")
                elif key == ord('+') or key == ord('='):  # ì‹ ë¢°ë„ ì¦ê°€
                    current_conf_thresh = min(0.9, current_conf_thresh + 0.05)
                    print(f"ğŸ“ˆ ì‹ ë¢°ë„ ì„ê³„ê°’: {current_conf_thresh:.2f}")
                elif key == ord('-'):  # ì‹ ë¢°ë„ ê°ì†Œ
                    current_conf_thresh = max(0.1, current_conf_thresh - 0.05)
                    print(f"ğŸ“‰ ì‹ ë¢°ë„ ì„ê³„ê°’: {current_conf_thresh:.2f}")
                
                # ì„±ëŠ¥ í†µê³„ ì£¼ê¸°ì  ì¶œë ¥
                if frame_count % 60 == 0 and frame_count > 0:
                    print(f"ğŸ“Š í”„ë ˆì„ {frame_count}: í‰ê·  {avg_fps:.1f}fps, "
                          f"{len(results)}ëª… ê²€ì¶œ (Provider: {self.pose_provider.split('ExecutionProvider')[0]})")
                    
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ìê°€ ì›¹ìº  í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ìµœì¢… í†µê³„
            if fps_history:
                final_avg_fps = np.mean(fps_history)
                print(f"\nğŸ“Š YOLO11L + RTMW-L ONNX ì›¹ìº  í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
                print(f"   - ì²˜ë¦¬ëœ í”„ë ˆì„: {frame_count}")
                print(f"   - í‰ê·  FPS: {final_avg_fps:.1f}")
                print(f"   - ê²€ì¶œ ë””ë°”ì´ìŠ¤: {self.detection_device}")
                print(f"   - í¬ì¦ˆ Provider: {self.pose_provider}")
                print(f"   - ìŠ¤í¬ë¦°ìƒ·: {screenshot_count}ê°œ ì €ì¥")
                print(f"   - ìµœì¢… ì‹ ë¢°ë„ ì„ê³„ê°’: {current_conf_thresh:.2f}")

if __name__ == "__main__":
    main()