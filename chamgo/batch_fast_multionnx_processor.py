#!/usr/bin/env python3
"""
Batch Fast Multi-ONNX Processor - GPU ë°°ì¹˜ 256 ìµœì í™” ë²„ì „ (ì™„ì „ ê°œì„ )
Phase 1: ë°°ì¹˜ ì²˜ë¦¬ ì•„í‚¤í…ì²˜ êµ¬ì¶• âš¡ - 256 í”„ë ˆì„ì”© VRAM ë¯¸ë¦¬ ë¡œë“œ, A6000 x2 ì™„ì „ í™œìš©
Phase 2: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ğŸ“Š - ì‹¤ì‹œê°„ ì§„í–‰ë¥ , GPU ì‚¬ìš©ë¥ , FPS í†µê³„
Phase 3: ê³ ì† íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ğŸš€ - ë¹„ë™ê¸° ë¡œë”©, ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬, ê²°ê³¼ ë²„í¼ë§
"""

import os
import cv2
import h5py
import time
import json
import torch
import queue
import logging
import threading
import traceback
import numpy as np
import multiprocessing as mp
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Tuple, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from collections import deque
from datetime import datetime
import psutil
import GPUtil
import argparse
import sys

from onnx_inferencer import YOLO11LRTMWONNXInferencer as ONNXInferencer

# ===== RTMW ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (streamlined_processor.py ì°¸ê³ ) =====

def bbox_xyxy2cs(bbox: np.ndarray, padding: float = 1.10) -> Tuple[np.ndarray, np.ndarray]:
    """ë°”ìš´ë”©ë°•ìŠ¤ë¥¼ center, scaleë¡œ ë³€í™˜ (íŒ¨ë”© 1.10ìœ¼ë¡œ ìˆ˜ì •)"""
    dim = bbox.ndim
    if dim == 1:
        bbox = bbox[None, :]
    
    scale = (bbox[..., 2:] - bbox[..., :2]) * padding
    center = (bbox[..., 2:] + bbox[..., :2]) * 0.5
    
    if dim == 1:
        center = center[0]
        scale = scale[0]
    
    return center, scale

def _rotate_point(pt: np.ndarray, angle_rad: float) -> np.ndarray:
    """ì ì„ íšŒì „"""
    cos_val = np.cos(angle_rad)
    sin_val = np.sin(angle_rad)
    return np.array([pt[0] * cos_val - pt[1] * sin_val,
                     pt[0] * sin_val + pt[1] * cos_val])

def _get_3rd_point(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    """ì„¸ ë²ˆì§¸ ì ì„ ê³„ì‚° (ì§êµì )"""
    direction = a - b
    return b + np.array([-direction[1], direction[0]])

def get_warp_matrix(center: np.ndarray, scale: np.ndarray, rot: float, 
                   output_size: Tuple[int, int]) -> np.ndarray:
    """ì•„í•€ ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
    src_w, src_h = scale[:2]
    dst_w, dst_h = output_size[:2]
    
    rot_rad = np.deg2rad(rot)
    src_dir = _rotate_point(np.array([src_w * -0.5, 0.]), rot_rad)
    dst_dir = np.array([dst_w * -0.5, 0.])
    
    src = np.zeros((3, 2), dtype=np.float32)
    src[0, :] = center
    src[1, :] = center + src_dir
    
    dst = np.zeros((3, 2), dtype=np.float32)
    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]
    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir
    
    # aspect ratio ê³ ì •
    src[2, :] = _get_3rd_point(src[0, :], src[1, :])
    dst[2, :] = _get_3rd_point(dst[0, :], dst[1, :])
    
    warp_mat = cv2.getAffineTransform(src, dst)
    return warp_mat

def fix_aspect_ratio(bbox_scale: np.ndarray, aspect_ratio: float) -> np.ndarray:
    """bboxë¥¼ ê³ ì • ì¢…íš¡ë¹„ë¡œ ì¡°ì •"""
    w, h = bbox_scale[0], bbox_scale[1]
    if w > h * aspect_ratio:
        new_h = w / aspect_ratio
        bbox_scale = np.array([w, new_h])
    else:
        new_w = h * aspect_ratio
        bbox_scale = np.array([new_w, h])
    return bbox_scale

# ===== Phase 1: ë°°ì¹˜ ì²˜ë¦¬ ì•„í‚¤í…ì²˜ êµ¬ì¶• âš¡ =====

@dataclass
class BatchMetrics:
    """ë°°ì¹˜ ì²˜ë¦¬ ë©”íŠ¸ë¦­ìŠ¤"""
    frames_processed: int = 0
    processing_time: float = 0.0
    gpu_memory_used: float = 0.0
    throughput_fps: float = 0.0
    batch_id: int = 0
    gpu_id: int = 0

class SmartVRAMBuffer:
    """ìŠ¤ë§ˆíŠ¸ VRAM ë²„í¼ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, gpu_id: int, batch_size: int = 256, max_vram_usage: float = 0.85):
        self.gpu_id = gpu_id
        self.batch_size = batch_size
        self.max_vram_usage = max_vram_usage
        self.device = f"cuda:{gpu_id}"
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´
        torch.cuda.set_device(gpu_id)
        gpu_props = torch.cuda.get_device_properties(gpu_id)
        self.total_vram = gpu_props.total_memory / (1024**3)
        self.max_vram = self.total_vram * max_vram_usage
        
        # í”„ë ˆì„ ë°°ì¹˜ ë²„í¼ (VRAMì— ë¯¸ë¦¬ ë¡œë“œ)
        single_frame_size = 384 * 288 * 3 * 4  # float32
        self.batch_memory_mb = (single_frame_size * batch_size) / (1024 * 1024)
        
        print(f"ğŸš€ SmartVRAMBuffer GPU {gpu_id} ì´ˆê¸°í™”")
        print(f"   - ì´ VRAM: {self.total_vram:.1f}GB")
        print(f"   - ìµœëŒ€ ì‚¬ìš©: {self.max_vram:.1f}GB")
        print(f"   - ë°°ì¹˜ ë©”ëª¨ë¦¬: {self.batch_memory_mb:.1f}MB")
        
    def preload_batch(self, frames: List[np.ndarray]) -> torch.Tensor:
        """í”„ë ˆì„ ë°°ì¹˜ë¥¼ VRAMì— ë¯¸ë¦¬ ë¡œë“œ"""
        batch_size = min(len(frames), self.batch_size)
        
        with torch.cuda.device(self.device):
            batch_tensor = torch.zeros((batch_size, 3, 384, 288), 
                                     dtype=torch.float32, device=self.device)
            
            for i, frame in enumerate(frames[:batch_size]):
                if frame is not None:
                    # OpenCV (H,W,C) -> PyTorch (C,H,W) ë³€í™˜
                    frame_tensor = torch.from_numpy(frame.transpose(2, 0, 1)).float()
                    batch_tensor[i] = frame_tensor.to(self.device) / 255.0
            
            return batch_tensor
    
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        allocated = torch.cuda.memory_allocated(self.device) / (1024**3)
        reserved = torch.cuda.memory_reserved(self.device) / (1024**3)
        return {
            'allocated': allocated,
            'reserved': reserved,
            'usage_percent': (allocated / self.total_vram) * 100
        }

# ===== Phase 2: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ğŸ“Š =====

class PerformanceMonitor:
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metrics_history = deque(maxlen=100)  # ìµœê·¼ 100ê°œ ë°°ì¹˜ ê¸°ë¡
        self.start_time = time.time()
        self.frame_count = 0
        self.batch_count = 0
        
        # GPU ëª¨ë‹ˆí„°ë§ ì´ˆê¸°í™”
        try:
            self.gpus = GPUtil.getGPUs()
        except:
            self.gpus = []
        
    def log_batch_metrics(self, metrics: BatchMetrics):
        """ë°°ì¹˜ ë©”íŠ¸ë¦­ìŠ¤ ê¸°ë¡"""
        self.metrics_history.append(metrics)
        self.frame_count += metrics.frames_processed
        self.batch_count += 1
        
    def get_realtime_stats(self) -> Dict:
        """ì‹¤ì‹œê°„ í†µê³„ ë°˜í™˜"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = list(self.metrics_history)[-10:]  # ìµœê·¼ 10ê°œ ë°°ì¹˜
        
        avg_fps = np.mean([m.throughput_fps for m in recent_metrics])
        avg_gpu_usage = np.mean([m.gpu_memory_used for m in recent_metrics])
        
        total_time = time.time() - self.start_time
        overall_fps = self.frame_count / max(total_time, 0.001)
        
        # GPU ìƒíƒœ ì¡°íšŒ
        gpu_stats = []
        for gpu in self.gpus:
            gpu_stats.append({
                'id': gpu.id,
                'name': gpu.name,
                'memory_used': gpu.memoryUsed,
                'memory_total': gpu.memoryTotal,
                'memory_percent': gpu.memoryUtil * 100,
                'gpu_util': gpu.load * 100
            })
        
        return {
            'batch_count': self.batch_count,
            'total_frames': self.frame_count,
            'avg_fps_recent': avg_fps,
            'overall_fps': overall_fps,
            'avg_gpu_memory': avg_gpu_usage,
            'total_processing_time': total_time,
            'gpu_stats': gpu_stats,
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent
        }
    
    def print_progress(self):
        """ì§„í–‰ ìƒí™© ì¶œë ¥"""
        stats = self.get_realtime_stats()
        if stats:
            print(f"\nğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ í†µê³„:")
            print(f"   - ì²˜ë¦¬ëœ ë°°ì¹˜: {stats['batch_count']}")
            print(f"   - ì´ í”„ë ˆì„: {stats['total_frames']}")
            print(f"   - ì „ì²´ FPS: {stats['overall_fps']:.1f}")
            print(f"   - ìµœê·¼ í‰ê·  FPS: {stats['avg_fps_recent']:.1f}")
            print(f"   - CPU ì‚¬ìš©ë¥ : {stats['cpu_percent']:.1f}%")
            print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {stats['memory_percent']:.1f}%")
            
            for gpu_stat in stats['gpu_stats']:
                print(f"   - GPU {gpu_stat['id']} ({gpu_stat['name']}): "
                      f"VRAM {gpu_stat['memory_percent']:.1f}%, "
                      f"ì‚¬ìš©ë¥  {gpu_stat['gpu_util']:.1f}%")

# ===== Phase 3: ê³ ì† íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ğŸš€ =====

class AsyncDataLoader:
    """ë¹„ë™ê¸° ë°ì´í„° ë¡œë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, max_queue_size: int = 4):
        self.max_queue_size = max_queue_size
        self.data_queue = queue.Queue(maxsize=max_queue_size)
        self.loading_thread = None
        self.is_loading = False
        
    def start_loading(self, video_paths: List[str]):
        """ë¹„ë™ê¸° ë°ì´í„° ë¡œë”© ì‹œì‘"""
        self.is_loading = True
        self.loading_thread = threading.Thread(
            target=self._load_videos_async,
            args=(video_paths,),
            daemon=True
        )
        self.loading_thread.start()
        
    def _load_videos_async(self, video_paths: List[str]):
        """ë¹„ë™ê¸°ë¡œ ë¹„ë””ì˜¤ ë¡œë”©"""
        for video_path in video_paths:
            if not self.is_loading:
                break
                
            try:
                # ê°„ë‹¨í•œ ë¹„ë””ì˜¤ ì •ë³´ë§Œ ë¡œë“œ (ì‹¤ì œ í”„ë ˆì„ì€ ë‚˜ì¤‘ì—)
                cap = cv2.VideoCapture(video_path)
                if cap.isOpened():
                    video_info = {
                        'path': video_path,
                        'fps': cap.get(cv2.CAP_PROP_FPS),
                        'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
                        'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
                        'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    }
                    cap.release()
                    self.data_queue.put(video_info, timeout=10)
                else:
                    cap.release()
            except queue.Full:
                print(f"âš ï¸ íê°€ ê°€ë“ì°¬, ë¹„ë””ì˜¤ ìŠ¤í‚µ: {video_path}")
                continue
            except Exception as e:
                print(f"âŒ ë¹„ë””ì˜¤ ì •ë³´ ë¡œë”© ì‹¤íŒ¨: {video_path} - {e}")
                continue
    
    def get_next_video_info(self, timeout: float = 5.0) -> Optional[Dict]:
        """ë‹¤ìŒ ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.data_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def stop_loading(self):
        """ë¡œë”© ì¤‘ì§€"""
        self.is_loading = False
        if self.loading_thread and self.loading_thread.is_alive():
            self.loading_thread.join(timeout=2)

class BatchFastVideoProcessor:
    """ë°°ì¹˜ ê³ ì† ë¹„ë””ì˜¤ ì²˜ë¦¬ê¸° - GPU ë°°ì¹˜ 256 ìµœì í™” (ì™„ì „ ê°œì„ )"""
    
    def __init__(self, 
                 rtmw_model_name: str = "rtmw-dw-x-l_simcc-cocktail14_270e-384x288.onnx",
                 gpu_id: int = 0,
                 batch_size: int = 256,
                 keypoint_scale: int = 8,
                 jpeg_quality: int = 90,
                 max_vram_usage: float = 0.85):
        
        self.rtmw_model_name = rtmw_model_name
        self.gpu_id = gpu_id
        self.batch_size = batch_size
        self.keypoint_scale = keypoint_scale
        self.jpeg_quality = jpeg_quality
        self.max_vram_usage = max_vram_usage
        self.device = f"cuda:{gpu_id}"
        
        # Phase 1: ìŠ¤ë§ˆíŠ¸ VRAM ë²„í¼ ì´ˆê¸°í™”
        self.vram_buffer = SmartVRAMBuffer(
            gpu_id=gpu_id, 
            batch_size=batch_size,
            max_vram_usage=max_vram_usage
        )
        
        # Phase 2: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì´ˆê¸°í™”
        self.monitor = PerformanceMonitor()
        
        # GPU ì„¤ì •
        torch.cuda.set_device(gpu_id)
        
        try:
            # ONNX ì¶”ë¡ ê¸° ì´ˆê¸°í™”
            self.inferencer = ONNXInferencer(
                rtmw_onnx_path=rtmw_model_name,
                detection_device=self.device,
                pose_device=self.device,
                optimize_for_accuracy=True
            )
            
            print(f"ğŸš€ BatchFastVideoProcessor GPU {gpu_id} ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
            print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
            print(f"   - RTMW ëª¨ë¸: {rtmw_model_name}")
            
            # GPU ì›Œë°ì—…
            self._warmup_gpu()
            
        except Exception as e:
            print(f"âŒ BatchFastVideoProcessor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _warmup_gpu(self):
        """GPU ì›Œë°ì—… - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""
        print(f"ğŸ”¥ GPU ì›Œë°ì—… ì‹œì‘ (ë°°ì¹˜ {self.batch_size})")
        start_time = time.time()
        
        try:
            # ë”ë¯¸ ë°°ì¹˜ í…ì„œ ìƒì„±
            dummy_batch = torch.randn(self.batch_size, 3, 384, 288).cuda()
            
            # ëª‡ ë²ˆ ì—°ì‚° ìˆ˜í–‰í•˜ì—¬ GPU í™œì„±í™”
            for _ in range(3):
                _ = dummy_batch * 2.0
                _ = torch.nn.functional.interpolate(dummy_batch, size=(288, 384))
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del dummy_batch
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            
            warmup_time = time.time() - start_time
            vram_used = torch.cuda.memory_allocated() / 1024**3
            
            print(f"âœ… GPU ì›Œë°ì—… ì™„ë£Œ: {warmup_time:.2f}ì´ˆ")
            print(f"   - VRAM ì‚¬ìš©: {vram_used:.2f}GB")
            print(f"   - ë°°ì¹˜ í…ì„œ: {self.batch_size} x 3 x 384 x 288")
            
        except Exception as e:
            print(f"âš ï¸ GPU ì›Œë°ì—… ì‹¤íŒ¨: {e}")

    def process_video_batch_optimized(self, video_path: str, progress_callback=None) -> Optional[Dict]:
        """ë°°ì¹˜ ìµœì í™”ëœ ë¹„ë””ì˜¤ ì²˜ë¦¬ - Production Ready"""
        start_total_time = time.time()
        
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"âŒ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
                return None
            
            # ì „ì²´ í”„ë ˆì„ ìˆ˜ ê³„ì‚°
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            if total_frames == 0:
                cap.release()
                return None
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì •ë³´: {total_frames}í”„ë ˆì„, {fps:.2f}FPS, {width}x{height}")
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í”„ë ˆì„ ë¡œë”©
            frames = []
            frame_count = 0
            
            # í”„ë ˆì„ ë¡œë”© with ì§„í–‰ë¥ 
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(frame)
                frame_count += 1
                
                # ì§„í–‰ë¥  ì½œë°±
                if progress_callback and frame_count % 50 == 0:
                    progress = min(frame_count / total_frames * 0.3, 0.3)  # ë¡œë”© 30%
                    progress_callback(progress)
            
            cap.release()
            actual_frame_count = len(frames)
            
            if actual_frame_count == 0:
                print(f"âŒ ìœ íš¨í•œ í”„ë ˆì„ ì—†ìŒ: {video_path}")
                return None
            
            print(f"âœ… í”„ë ˆì„ ë¡œë“œ ì™„ë£Œ: {actual_frame_count}ê°œ (ë°°ì¹˜ {self.batch_size}ë¡œ ì²˜ë¦¬)")
            
            # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
            jpeg_frames = []
            keypoints_list = []
            scores_list = []
            
            # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
            batch_frames = []
            batch_start_time = time.time()
            processed_frames = 0
            
            for frame_idx, frame in enumerate(frames):
                batch_frames.append(frame)
                
                # ë°°ì¹˜ê°€ ì°¼ê±°ë‚˜ ë§ˆì§€ë§‰ í”„ë ˆì„ì¸ ê²½ìš°
                if len(batch_frames) >= self.batch_size or frame_idx == len(frames) - 1:
                    
                    # GPU ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                    batch_results = self._process_frame_batch(batch_frames)
                    
                    # ê²°ê³¼ ì €ì¥ ë° JPEG ì¸ì½”ë”© (streamlined ë°©ì‹: í¬ë¡­ ì´ë¯¸ì§€ ì €ì¥)
                    for i, (frame, result) in enumerate(zip(batch_frames, batch_results)):
                        
                        if result and len(result) == 3:
                            frame_keypoints, frame_scores, crop_image = result
                            
                            # streamlined ë°©ì‹: í¬ë¡­ ì´ë¯¸ì§€ë¥¼ JPEGë¡œ ì¸ì½”ë”©
                            if crop_image is not None:
                                # í¬ë¡­ ì´ë¯¸ì§€ JPEG ì¸ì½”ë”© (streamlinedì™€ ë™ì¼)
                                encode_params = [cv2.IMWRITE_JPEG_QUALITY, self.jpeg_quality]
                                success, buffer = cv2.imencode('.jpg', crop_image, encode_params)
                                
                                if success:
                                    jpeg_frames.append(buffer)  # numpy ë°°ì—´ ì§ì ‘ ì €ì¥
                                else:
                                    # í´ë°±: ê¸°ë³¸ ì¸ì½”ë”©
                                    _, buffer = cv2.imencode('.jpg', crop_image)
                                    jpeg_frames.append(buffer)
                            else:
                                # í¬ë¡­ ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ í”„ë ˆì„ ì‚¬ìš©
                                _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, self.jpeg_quality])
                                jpeg_frames.append(buffer)
                        else:
                            # ê²€ì¶œ ê²°ê³¼ ì—†ëŠ” ê²½ìš° ì›ë³¸ í”„ë ˆì„ ì‚¬ìš©
                            _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, self.jpeg_quality])
                            jpeg_frames.append(buffer)
                        
                        # í‚¤í¬ì¸íŠ¸ ë°ì´í„° ì²˜ë¦¬
                        if result and len(result) >= 2:
                            frame_keypoints, frame_scores = result[0], result[1]
                            
                            # í‚¤í¬ì¸íŠ¸ ìŠ¤ì¼€ì¼ë§
                            if len(frame_keypoints) > 0 and self.keypoint_scale != 1:
                                scaled_keypoints = []
                                for person_kpts in frame_keypoints:
                                    if isinstance(person_kpts, (list, np.ndarray)) and len(person_kpts) > 0:
                                        scaled_kpts = []
                                        for j in range(0, len(person_kpts), 2):
                                            if j + 1 < len(person_kpts):
                                                x = int(person_kpts[j] * self.keypoint_scale)
                                                y = int(person_kpts[j + 1] * self.keypoint_scale)
                                                scaled_kpts.extend([x, y])
                                        scaled_keypoints.append(scaled_kpts)
                                    else:
                                        scaled_keypoints.append(person_kpts)
                                keypoints_list.append(scaled_keypoints)
                            else:
                                keypoints_list.append(frame_keypoints)
                            
                            scores_list.append(frame_scores)
                        else:
                            # ê¸°ë³¸ê°’ (ê²€ì¶œëœ ì‚¬ëŒ ì—†ìŒ)
                            keypoints_list.append([[0] * 34])  # 17ê°œ í‚¤í¬ì¸íŠ¸ * 2 (x,y)
                            scores_list.append([0.0])
                    
                    processed_frames += len(batch_frames)
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    if progress_callback:
                        progress = 0.3 + (processed_frames / actual_frame_count) * 0.7  # 30% + ì²˜ë¦¬ 70%
                        progress_callback(min(progress, 1.0))
                    
                    # ë°°ì¹˜ ì´ˆê¸°í™”
                    batch_frames = []
                    
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì£¼ê¸°ì )
                    if torch.cuda.is_available() and processed_frames % (self.batch_size * 4) == 0:
                        torch.cuda.empty_cache()
            
            total_processing_time = time.time() - batch_start_time
            total_time = time.time() - start_total_time
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                'total_frames': actual_frame_count,
                'processed_frames': processed_frames,
                'jpeg_frames': jpeg_frames,
                'keypoints': keypoints_list,
                'scores': scores_list,
                'processing_time': total_processing_time,
                'total_time': total_time,
                'fps': actual_frame_count / max(total_processing_time, 0.001),
                'video_info': {
                    'original_fps': fps,
                    'resolution': f"{width}x{height}",
                    'duration': actual_frame_count / max(fps, 1.0)
                }
            }
            
            print(f"âœ… ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ:")
            print(f"   - ì²˜ë¦¬: {actual_frame_count}í”„ë ˆì„ ({total_processing_time:.2f}ì´ˆ)")
            print(f"   - ì†ë„: {result['fps']:.1f} FPS")
            print(f"   - ì „ì²´: {total_time:.2f}ì´ˆ")
            
            return result
            
        except Exception as e:
            print(f"ğŸ’¥ ë°°ì¹˜ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜ ({video_path}): {e}")
            traceback.print_exc()
            return None
    
    def _process_frame_batch(self, frames: List[np.ndarray]) -> List[Optional[Tuple[List[List[int]], List[float], Optional[np.ndarray]]]]:
        """í”„ë ˆì„ ë°°ì¹˜ ì²˜ë¦¬ - Production Ready GPU ë°°ì¹˜ ì¶”ë¡  (í¬ë¡­ ì´ë¯¸ì§€ë„ ë°˜í™˜)"""
        results = []
        batch_start_time = time.time()
        
        try:
            if not frames:
                return []
            
            print(f"ğŸ”¥ ë°°ì¹˜ í”„ë ˆì„ ì²˜ë¦¬: {len(frames)}ê°œ")
            
            # ê° í”„ë ˆì„ë³„ ê²°ê³¼ ì´ˆê¸°í™”
            for _ in range(len(frames)):
                results.append(None)
            
            # GPU ë°°ì¹˜ ì¶”ë¡ ì„ ìœ„í•œ ì¤€ë¹„
            all_crops = []  # ëª¨ë“  í¬ë¡­ ì´ë¯¸ì§€ë“¤
            all_crop_images = []  # ì›ë³¸ í¬ë¡­ ì´ë¯¸ì§€ë“¤ (JPEG ì¸ì½”ë”©ìš©)
            crop_frame_mapping = []  # ê° í¬ë¡­ì´ ì–´ëŠ í”„ë ˆì„ì—ì„œ ì™”ëŠ”ì§€
            frame_person_counts = []  # ê° í”„ë ˆì„ì—ì„œ ê²€ì¶œëœ ì‚¬ëŒ ìˆ˜
            
            # 1ë‹¨ê³„: ëª¨ë“  í”„ë ˆì„ì—ì„œ ì‚¬ëŒ ê²€ì¶œ (YOLO)
            detection_start = time.time()
            for frame_idx, frame in enumerate(frames):
                try:
                    # YOLO ì‚¬ëŒ ê²€ì¶œ (ê³ ì •í™•ë„)
                    person_boxes = self.inferencer.detect_persons_high_accuracy(frame)
                    person_count = len(person_boxes)
                    frame_person_counts.append(person_count)
                    
                    if person_count == 0:
                        continue
                    
                    # ê° ì‚¬ëŒ ì˜ì—­ì„ í¬ë¡­í•˜ì—¬ ë°°ì¹˜ì— ì¶”ê°€
                    for person_idx, bbox in enumerate(person_boxes):
                        try:
                            x1, y1, x2, y2 = map(int, bbox[:4])
                            x1, y1 = max(0, x1), max(0, y1)
                            x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)
                            
                            if x2 > x1 + 10 and y2 > y1 + 10:  # ìµœì†Œ í¬ê¸° ì²´í¬
                                crop = frame[y1:y2, x1:x2]
                                
                                # í¬ë¡­ í¬ê¸° í™•ì¸
                                if crop.shape[0] > 0 and crop.shape[1] > 0:
                                    # RTMW ì „ì²˜ë¦¬ ì ìš©
                                    processed_crop = self.inferencer._preprocess_image_for_pose(crop)
                                    if processed_crop is not None:
                                        all_crops.append(processed_crop)
                                        all_crop_images.append(crop)  # ì›ë³¸ í¬ë¡­ ì´ë¯¸ì§€ ì €ì¥
                                        crop_frame_mapping.append(frame_idx)
                                else:
                                    print(f"âš ï¸ ë¹ˆ í¬ë¡­ ì´ë¯¸ì§€: frame {frame_idx}, person {person_idx}, bbox ({x1},{y1},{x2},{y2})")
                                    
                        except Exception as e:
                            print(f"âš ï¸ í”„ë ˆì„ {frame_idx} ì‚¬ëŒ {person_idx} í¬ë¡­ ì‹¤íŒ¨: {e}")
                            continue
                    
                except Exception as e:
                    print(f"âš ï¸ í”„ë ˆì„ {frame_idx} YOLO ê²€ì¶œ ì‹¤íŒ¨: {e}")
                    frame_person_counts.append(0)
            
            detection_time = time.time() - detection_start
            print(f"   YOLO ê²€ì¶œ: {len(all_crops)}ê°œ ì‚¬ëŒ ({detection_time:.2f}ì´ˆ)")
            
            # 2ë‹¨ê³„: GPU ë°°ì¹˜ í¬ì¦ˆ ì¶”ì • (ONNX)
            pose_start = time.time()
            batch_keypoints = []
            batch_scores = []
            
            if all_crops:
                # ë°°ì¹˜ í¬ê¸° ì¡°ì • (GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ)
                pose_batch_size = min(64, len(all_crops))  # A6000 ê¸°ì¤€ ìµœì í™”
                
                for i in range(0, len(all_crops), pose_batch_size):
                    batch_slice = all_crops[i:i+pose_batch_size]
                    
                    try:
                        # ONNX ë°°ì¹˜ ì¶”ë¡ 
                        kpts_batch, scores_batch = self.inferencer.estimate_pose_batch(batch_slice)
                        
                        batch_keypoints.extend(kpts_batch)
                        batch_scores.extend(scores_batch)
                        
                    except Exception as e:
                        print(f"âš ï¸ ë°°ì¹˜ í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨, ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±: {e}")
                        
                        # í´ë°±: ê°œë³„ ì²˜ë¦¬
                        for crop in batch_slice:
                            try:
                                kpts, scores = self.inferencer.estimate_pose_on_crop(crop)
                                batch_keypoints.append(kpts)
                                batch_scores.append(scores)
                            except Exception as e2:
                                print(f"âš ï¸ ê°œë³„ í¬ì¦ˆ ì¶”ì •ë„ ì‹¤íŒ¨: {e2}")
                                batch_keypoints.append(np.zeros((17, 2)))  # ê¸°ë³¸ê°’
                                batch_scores.append(np.zeros(17))
            
            pose_time = time.time() - pose_start
            print(f"   í¬ì¦ˆ ì¶”ì •: {len(batch_keypoints)}ê°œ ê²°ê³¼ ({pose_time:.2f}ì´ˆ)")
            
            # 3ë‹¨ê³„: ê²°ê³¼ë¥¼ í”„ë ˆì„ë³„ë¡œ ì¬êµ¬ì„±
            if batch_keypoints:
                crop_idx = 0
                
                for frame_idx in range(len(frames)):
                    person_count = frame_person_counts[frame_idx]
                    
                    if person_count == 0:
                        results[frame_idx] = ([], [], None)  # ë¹ˆ ê²°ê³¼
                        continue
                    
                    frame_keypoints = []
                    frame_scores = []
                    first_crop_image = None  # ì²« ë²ˆì§¸ ì‚¬ëŒì˜ í¬ë¡­ ì´ë¯¸ì§€ (streamlined ë°©ì‹)
                    
                    # í•´ë‹¹ í”„ë ˆì„ì˜ ëª¨ë“  ì‚¬ëŒ ê²°ê³¼ ìˆ˜ì§‘
                    for person_idx in range(person_count):
                        if crop_idx < len(batch_keypoints):
                            kpts = batch_keypoints[crop_idx]
                            scores = batch_scores[crop_idx]
                            
                            # ì²« ë²ˆì§¸ ì‚¬ëŒì˜ í¬ë¡­ ì´ë¯¸ì§€ ì €ì¥ (streamlinedì™€ ë™ì¼)
                            if person_idx == 0 and crop_idx < len(all_crop_images):
                                first_crop_image = all_crop_images[crop_idx]
                            
                            # í‚¤í¬ì¸íŠ¸ í˜•ì‹ ë³€í™˜ (ë¦¬ìŠ¤íŠ¸ë¡œ)
                            if isinstance(kpts, np.ndarray):
                                if kpts.ndim == 2:  # (17, 2) í˜•íƒœ
                                    kpts_flat = []
                                    for joint in kpts:
                                        kpts_flat.extend([float(joint[0]), float(joint[1])])
                                    frame_keypoints.append(kpts_flat)
                                else:
                                    frame_keypoints.append(kpts.flatten().tolist())
                            else:
                                frame_keypoints.append(kpts)
                            
                            if isinstance(scores, np.ndarray):
                                frame_scores.append(scores.tolist())
                            else:
                                frame_scores.append(scores)
                            
                            crop_idx += 1
                        else:
                            # ë°ì´í„° ë¶€ì¡± ì‹œ ê¸°ë³¸ê°’
                            frame_keypoints.append([0.0] * 34)  # 17 joints * 2 coords
                            frame_scores.append([0.0] * 17)
                    
                    results[frame_idx] = (frame_keypoints, frame_scores, first_crop_image)
            
            # ë¹ˆ ê²°ê³¼ë“¤ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ì›€
            for i, result in enumerate(results):
                if result is None:
                    results[i] = ([[0.0] * 34], [[0.0] * 17], None)
            
            total_time = time.time() - batch_start_time
            print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(frames)}í”„ë ˆì„ ({total_time:.2f}ì´ˆ)")
            
            return results
            
        except Exception as e:
            print(f"ğŸ’¥ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            
            # ì˜¤ë¥˜ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return [([[0.0] * 34], [[0.0] * 17], None)] * len(frames)
    
    def _process_single_frame(self, frame: np.ndarray) -> Optional[Tuple[List[List[int]], List[float], Optional[np.ndarray]]]:
        """ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬ (í´ë°± ìš©ë„) - í¬ë¡­ ì´ë¯¸ì§€ë„ ë°˜í™˜"""
        try:
            # ì¶”ë¡  ì‹¤í–‰
            result = self.inferencer.process_frame(frame)
            
            # ê²°ê³¼ ì²˜ë¦¬
            frame_keypoints = []
            frame_scores = []
            crop_image = None
            
            if isinstance(result, tuple) and len(result) > 1:
                _, pose_results = result
            else:
                pose_results = result if result is not None else []
            
            if pose_results and len(pose_results) > 0:
                for person in pose_results:
                    if isinstance(person, dict) and 'keypoints' in person:
                        kpts = person['keypoints']
                        if len(kpts) >= 34:  # 17 keypoints * 2
                            frame_keypoints.append(kpts[:34])
                            frame_scores.append(person.get('score', 1.0))
                
                # ì²« ë²ˆì§¸ ì‚¬ëŒì˜ í¬ë¡­ ì´ë¯¸ì§€ ìƒì„± (streamlined ë°©ì‹)
                try:
                    person_boxes = self.inferencer.detect_persons_high_accuracy(frame)
                    if len(person_boxes) > 0:
                        bbox = person_boxes[0]
                        x1, y1, x2, y2 = map(int, bbox[:4])
                        x1, y1 = max(0, x1), max(0, y1)
                        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)
                        
                        if x2 > x1 + 10 and y2 > y1 + 10:
                            crop_image = frame[y1:y2, x1:x2]
                except:
                    pass
            
            # í‚¤í¬ì¸íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
            if not frame_keypoints:
                frame_keypoints.append([0.0] * 34)
                frame_scores.append(0.0)
            
            return frame_keypoints, frame_scores, crop_image
            
        except Exception as e:
            print(f"âš ï¸ ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return ([[0.0] * 34], [0.0], None)

def batch_gpu_worker(
    gpu_id: int,
    task_queue: mp.Queue, 
    result_queue: mp.Queue, 
    progress_queue: mp.Queue,
    config: Dict
):
    """ë°°ì¹˜ GPU ì›Œì»¤ - ë°°ì¹˜ 256 ìµœì í™” + ì§„í–‰ë¥  ì¶”ì """
    try:
        # GPU ì„¤ì • - ê°•ì œë¡œ íŠ¹ì • GPUë§Œ ë³´ì´ë„ë¡ ì„¤ì •
        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        
        # PyTorch CUDA ì´ˆê¸°í™”
        torch.cuda.init()
        torch.cuda.set_device(0)  # ì—¬ê¸°ì„œëŠ” 0ì´ ì‹¤ì œ gpu_idì— í•´ë‹¹
        
        print(f"ğŸš€ GPU {gpu_id} ì›Œì»¤ ì‹œì‘")
        print(f"   - CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.current_device()}")
        print(f"   - GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
        print(f"   - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        processor = BatchFastVideoProcessor(
            rtmw_model_name=config['rtmw_model_name'],
            gpu_id=0,  # ì›Œì»¤ì—ì„œëŠ” í•­ìƒ 0 (CUDA_VISIBLE_DEVICESë¡œ ì œì–´)
            batch_size=config['batch_size'],
            keypoint_scale=config.get('keypoint_scale', 8),
            jpeg_quality=config.get('jpeg_quality', 90),
            max_vram_usage=config.get('max_vram_usage', 0.85)
        )
        
        total_processed = 0
        worker_start_time = time.time()
        
        while True:
            try:
                # ì‘ì—… íì—ì„œ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                try:
                    task = task_queue.get(timeout=5.0)
                    if task is None:  # ì¢…ë£Œ ì‹ í˜¸
                        break
                except queue.Empty:
                    continue
                
                video_path, task_id = task
                
                # ì§„í–‰ë¥  ì¶”ì ì„ ìœ„í•œ ì½œë°±
                def progress_callback(progress):
                    try:
                        progress_queue.put({
                            'task_id': task_id,
                            'gpu_id': gpu_id,
                            'progress': progress,
                            'status': 'processing'
                        })
                    except Exception as e:
                        print(f"âš ï¸ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                print(f"ğŸ”¥ GPU {gpu_id} ì²˜ë¦¬ ì‹œì‘: {Path(video_path).name}")
                start_time = time.time()
                
                # ë¹„ë””ì˜¤ ì²˜ë¦¬
                result = processor.process_video_batch_optimized(video_path, progress_callback)
                
                processing_time = time.time() - start_time
                
                if result:
                    fps_achieved = result.get('fps', 0)
                    frame_count = result.get('total_frames', 0)
                    
                    print(f"âœ… GPU {gpu_id} ì™„ë£Œ: {frame_count}í”„ë ˆì„, {fps_achieved:.1f}FPS ({processing_time:.2f}ì´ˆ)")
                    
                    # ê²°ê³¼ íì— ì €ì¥
                    result_item = {
                        'task_id': task_id,
                        'gpu_id': gpu_id,
                        'video_path': video_path,
                        'result': result,
                        'processing_time': processing_time,
                        'fps_achieved': fps_achieved,
                        'frame_count': frame_count,
                        'status': 'completed'
                    }
                    
                else:
                    print(f"âŒ GPU {gpu_id} ì²˜ë¦¬ ì‹¤íŒ¨: {Path(video_path).name}")
                    result_item = {
                        'task_id': task_id,
                        'gpu_id': gpu_id,
                        'video_path': video_path,
                        'result': None,
                        'processing_time': processing_time,
                        'status': 'failed'
                    }
                
                # ê²°ê³¼ ì „ì†¡
                result_queue.put(result_item)
                total_processed += 1
                
                # ì§„í–‰ë¥  ì™„ë£Œ ì‹ í˜¸
                try:
                    progress_queue.put({
                        'task_id': task_id,
                        'gpu_id': gpu_id,
                        'progress': 1.0,
                        'status': 'completed'
                    })
                except Exception as e:
                    print(f"âš ï¸ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ ì‹¤íŒ¨: {e}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"ğŸ’¥ GPU {gpu_id} ì‘ì—… ì˜¤ë¥˜: {e}")
                traceback.print_exc()
                
                # ì˜¤ë¥˜ ê²°ê³¼ ì „ì†¡
                try:
                    result_queue.put({
                        'task_id': task_id if 'task_id' in locals() else -1,
                        'gpu_id': gpu_id,
                        'video_path': video_path if 'video_path' in locals() else 'unknown',
                        'result': None,
                        'status': 'error',
                        'error': str(e)
                    })
                except:
                    pass
        
        worker_time = time.time() - worker_start_time
        print(f"ğŸ GPU {gpu_id} ì›Œì»¤ ì¢…ë£Œ: {total_processed}ê°œ ì²˜ë¦¬ ({worker_time:.2f}ì´ˆ)")
        
    except Exception as e:
        print(f"ğŸ’¥ GPU {gpu_id} ì›Œì»¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        traceback.print_exc()

def process_videos_dual_gpu_async_batch(
    video_paths: List[str],
    output_dir: str,
    config: Dict,
    progress_callback=None
) -> Dict:
    """ë“€ì–¼ GPU ì™„ì „ ë¹„ë™ê¸° ë°°ì¹˜ 256 ì²˜ë¦¬ - í™•ì‹¤í•œ ë³‘ë ¬ ì²˜ë¦¬"""
    
    start_time = time.time()
    total_videos = len(video_paths)
    
    print(f"ğŸš€ ë“€ì–¼ GPU ì™„ì „ ë¹„ë™ê¸° ë°°ì¹˜ 256 ì²˜ë¦¬ ì‹œì‘")
    print(f"   - ì´ ë¹„ë””ì˜¤: {total_videos}ê°œ")
    print(f"   - ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
    print(f"   - GPU ë³‘ë ¬ ëª¨ë“œ: ì™„ì „ ë¹„ë™ê¸° (0ë²ˆ, 1ë²ˆ ë™ì‹œ ì²˜ë¦¬)")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì • (í™•ì‹¤í•œ ë¶„ë¦¬ë¥¼ ìœ„í•´ spawn)
    mp.set_start_method('spawn', force=True)
    
    # ê° GPUë³„ë¡œ ë…ë¦½ì ì¸ íì™€ í”„ë¡œì„¸ìŠ¤ ìƒì„±
    gpu_configs = []
    for gpu_id in range(2):
        gpu_config = {
            'task_queue': mp.Queue(),
            'result_queue': mp.Queue(), 
            'progress_queue': mp.Queue(),
            'process': None,
            'gpu_id': gpu_id,
            'tasks_assigned': 0
        }
        gpu_configs.append(gpu_config)
    
    # ë¹„ë””ì˜¤ë¥¼ ë‘ GPUì— ë¼ìš´ë“œë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ê· ë“± ë¶„ë°°
    for idx, video_path in enumerate(video_paths):
        gpu_id = idx % 2  # 0, 1, 0, 1, 0, 1...
        gpu_configs[gpu_id]['task_queue'].put((video_path, idx))
        gpu_configs[gpu_id]['tasks_assigned'] += 1
    
    # ê° GPUì— ì¢…ë£Œ ì‹ í˜¸ ì¶”ê°€
    for gpu_config in gpu_configs:
        gpu_config['task_queue'].put(None)
    
    print(f"ğŸ“Š ì‘ì—… ë¶„ë°°:")
    for gpu_config in gpu_configs:
        print(f"   - GPU {gpu_config['gpu_id']}: {gpu_config['tasks_assigned']}ê°œ ë¹„ë””ì˜¤")
    
    # ê° GPUë³„ë¡œ ë…ë¦½ì ì¸ ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    for gpu_config in gpu_configs:
        worker = mp.Process(
            target=async_batch_gpu_worker,
            args=(
                gpu_config['gpu_id'],
                gpu_config['task_queue'],
                gpu_config['result_queue'],
                gpu_config['progress_queue'],
                config
            )
        )
        worker.start()
        gpu_config['process'] = worker
        print(f"ğŸš€ GPU {gpu_config['gpu_id']} ì›Œì»¤ ì‹œì‘ (PID: {worker.pid})")
    
    # ê²°ê³¼ ìˆ˜ì§‘ì„ ìœ„í•œ í†µí•© ì²˜ë¦¬
    results = {}
    completed_count = 0
    failed_count = 0
    
    print(f"â³ ë¹„ë™ê¸° ì²˜ë¦¬ ì§„í–‰ ì¤‘...")
    
    # ì§„í–‰ë¥  ì¶”ì ì„ ìœ„í•œ ë¹„ë™ê¸° ëª¨ë‹ˆí„°ë§
    def async_progress_monitor():
        """ë¹„ë™ê¸° ì§„í–‰ë¥  ëª¨ë‹ˆí„° - ê°„ì†Œí™”"""
        last_update = time.time()
        
        while completed_count + failed_count < total_videos:
            try:
                # ì£¼ê¸°ì ìœ¼ë¡œ ì „ì²´ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_time = time.time()
                if current_time - last_update > 3.0:  # 3ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
                    total_progress = (completed_count + failed_count) / total_videos
                    
                    if progress_callback:
                        progress_callback(min(total_progress, 1.0))
                    
                    # ê°„ë‹¨í•œ ì§„í–‰ë¥  ì¶œë ¥
                    print(f"ğŸ“Š ì§„í–‰ë¥ : {completed_count + failed_count}/{total_videos} ({total_progress:.1%}) - GPUë³„ ì‘ì—… ì§„í–‰ ì¤‘...")
                    
                    last_update = current_time
                
                time.sleep(1.0)  # 1ì´ˆë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                print(f"âš ï¸ ë¹„ë™ê¸° ì§„í–‰ë¥  ëª¨ë‹ˆí„° ì˜¤ë¥˜: {e}")
                break
    
    # ì§„í–‰ë¥  ëª¨ë‹ˆí„° ìŠ¤ë ˆë“œ ì‹œì‘
    monitor_thread = threading.Thread(target=async_progress_monitor, daemon=True)
    monitor_thread.start()
    
    # ê° GPUì˜ ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘
    def collect_results_from_gpu(gpu_config):
        """íŠ¹ì • GPUë¡œë¶€í„° ê²°ê³¼ ìˆ˜ì§‘"""
        gpu_completed = 0
        gpu_failed = 0
        
        while gpu_completed + gpu_failed < gpu_config['tasks_assigned']:
            try:
                result_item = gpu_config['result_queue'].get(timeout=30.0)  # íƒ€ì„ì•„ì›ƒì„ 30ì´ˆë¡œ ì¦ê°€
                
                task_id = result_item['task_id']
                status = result_item['status']
                
                results[task_id] = result_item
                
                if status == 'completed':
                    gpu_completed += 1
                    
                    # Streamlined HDF5 íŒŒì¼ë¡œ ì €ì¥
                    video_path = result_item['video_path']
                    result_data = result_item['result']
                    
                    if result_data:
                        output_filename = Path(video_path).stem + '.h5'
                        output_path = os.path.join(output_dir, output_filename)
                        
                        try:
                            video_id = Path(video_path).stem.replace('NIA_SL_', '').split('_')[0]
                            save_to_hdf5_streamlined_format(result_data, output_path, video_id)
                            print(f"ğŸ’¾ GPU {gpu_config['gpu_id']} ì €ì¥ ì™„ë£Œ: {video_id}")
                        except Exception as e:
                            print(f"âš ï¸ GPU {gpu_config['gpu_id']} ì €ì¥ ì‹¤íŒ¨ ({Path(video_path).name}): {e}")
                    
                else:
                    gpu_failed += 1
                    print(f"âŒ GPU {gpu_config['gpu_id']} ì²˜ë¦¬ ì‹¤íŒ¨: {Path(result_item.get('video_path', 'unknown')).name}")
                
            except queue.Empty:
                print(f"âš ï¸ GPU {gpu_config['gpu_id']} ê²°ê³¼ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ")
                break
            except Exception as e:
                print(f"ï¿½ GPU {gpu_config['gpu_id']} ê²°ê³¼ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                break
        
        return gpu_completed, gpu_failed
    
    # ê° GPUë³„ë¡œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ê²°ê³¼ ìˆ˜ì§‘
    result_threads = []
    gpu_results = {}
    
    for gpu_config in gpu_configs:
        def make_collector(config):
            def collector():
                gpu_results[config['gpu_id']] = collect_results_from_gpu(config)
            return collector
        
        thread = threading.Thread(target=make_collector(gpu_config), daemon=False)
        thread.start()
        result_threads.append(thread)
    
    # ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘ ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
    for thread in result_threads:
        thread.join()
    
    # ê²°ê³¼ ì§‘ê³„
    for gpu_id, (gpu_completed, gpu_failed) in gpu_results.items():
        completed_count += gpu_completed
        failed_count += gpu_failed
        print(f"ï¿½ GPU {gpu_id} ì™„ë£Œ: ì„±ê³µ {gpu_completed}ê°œ, ì‹¤íŒ¨ {gpu_failed}ê°œ")
    
    # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
    for gpu_config in gpu_configs:
        worker = gpu_config['process']
        worker.join(timeout=10.0)
        if worker.is_alive():
            print(f"âš ï¸ GPU {gpu_config['gpu_id']} ì›Œì»¤ ê°•ì œ ì¢…ë£Œ")
            worker.terminate()
    
    total_time = time.time() - start_time
    
    # ìµœì¢… ê²°ê³¼ ì •ë¦¬
    summary = {
        'total_videos': total_videos,
        'completed': completed_count,
        'failed': failed_count,
        'total_time': total_time,
        'average_time_per_video': total_time / max(total_videos, 1),
        'gpu_distribution': {gpu_config['gpu_id']: gpu_config['tasks_assigned'] for gpu_config in gpu_configs},
        'results': results
    }
    
    print(f"ğŸ ë“€ì–¼ GPU ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"   - ì„±ê³µ: {completed_count}ê°œ")
    print(f"   - ì‹¤íŒ¨: {failed_count}ê°œ")
    print(f"   - ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"   - í‰ê·  ì‹œê°„: {summary['average_time_per_video']:.2f}ì´ˆ/ë¹„ë””ì˜¤")
    print(f"   - GPUë³„ ì²˜ë¦¬ëŸ‰: {summary['gpu_distribution']}")
    
    return summary

def async_batch_gpu_worker(
    gpu_id: int,
    task_queue: mp.Queue, 
    result_queue: mp.Queue, 
    progress_queue: mp.Queue,
    config: Dict
):
    """ì™„ì „ ë¹„ë™ê¸° ë°°ì¹˜ GPU ì›Œì»¤ - GPUë³„ ë…ë¦½ì  ì²˜ë¦¬"""
    try:
        # GPU ì„¤ì • - í™•ì‹¤í•œ GPU ë¶„ë¦¬
        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        
        # PyTorch CUDA ì´ˆê¸°í™”
        import torch
        torch.cuda.init()
        torch.cuda.set_device(0)  # CUDA_VISIBLE_DEVICESë¡œ ì œì–´ë˜ë¯€ë¡œ 0ì´ ì‹¤ì œ gpu_id
        
        print(f"ğŸš€ ë¹„ë™ê¸° GPU {gpu_id} ì›Œì»¤ ì‹œì‘")
        print(f"   - CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.current_device()}")
        print(f"   - GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
        print(f"   - VRAM ì´ëŸ‰: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # GPUë³„ë¡œ ë…ë¦½ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        processor = BatchFastVideoProcessor(
            rtmw_model_name=config['rtmw_model_name'],
            gpu_id=0,  # ì›Œì»¤ì—ì„œëŠ” í•­ìƒ 0 (CUDA_VISIBLE_DEVICESë¡œ ì œì–´)
            batch_size=config['batch_size'],
            keypoint_scale=config.get('keypoint_scale', 8),
            jpeg_quality=config.get('jpeg_quality', 90),
            max_vram_usage=config.get('max_vram_usage', 0.8)  # ë¹„ë™ê¸° ì²˜ë¦¬ì‹œ ì—¬ìœ  í™•ë³´
        )
        
        total_processed = 0
        worker_start_time = time.time()
        
        while True:
            try:
                # ì‘ì—… íì—ì„œ ê°€ì ¸ì˜¤ê¸°
                try:
                    task = task_queue.get(timeout=5.0)
                    if task is None:  # ì¢…ë£Œ ì‹ í˜¸
                        print(f"ğŸ”š GPU {gpu_id} ì›Œì»¤ ì¢…ë£Œ ì‹ í˜¸ ë°›ìŒ")
                        break
                except queue.Empty:
                    continue
                
                video_path, task_id = task
                print(f"ğŸ”¥ GPU {gpu_id} ì‘ì—… ì‹œì‘: {Path(video_path).name} (Task {task_id})")
                
                # ì§„í–‰ë¥  ì½œë°± ì •ì˜ - ì˜¬ë°”ë¥¸ ë“¤ì—¬ì“°ê¸°
                def progress_callback(progress):
                    try:
                        progress_queue.put_nowait({
                            'task_id': task_id,
                            'gpu_id': gpu_id,
                            'progress': progress,
                            'status': 'processing',
                            'timestamp': time.time()
                        })
                    except queue.Full:
                        pass  # ì§„í–‰ë¥  íê°€ ê°€ë“ ì°¨ë©´ ë¬´ì‹œ
                
                # ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤í–‰
                task_start_time = time.time()
                result = processor.process_video_batch_optimized(video_path, progress_callback)
                processing_time = time.time() - task_start_time
                
                # ê²°ê³¼ ì²˜ë¦¬
                if result:
                    fps_achieved = result.get('fps', 0)
                    frame_count = result.get('total_frames', 0)
                    
                    print(f"âœ… GPU {gpu_id} ì™„ë£Œ: {Path(video_path).name} - {frame_count}í”„ë ˆì„, {fps_achieved:.1f}FPS ({processing_time:.2f}ì´ˆ)")
                    
                    result_item = {
                        'task_id': task_id,
                        'gpu_id': gpu_id,
                        'video_path': video_path,
                        'result': result,
                        'processing_time': processing_time,
                        'fps_achieved': fps_achieved,
                        'frame_count': frame_count,
                        'status': 'completed',
                        'timestamp': time.time()
                    }
                else:
                    print(f"âŒ GPU {gpu_id} ì‹¤íŒ¨: {Path(video_path).name}")
                    result_item = {
                        'task_id': task_id,
                        'gpu_id': gpu_id,
                        'video_path': video_path,
                        'result': None,
                        'processing_time': processing_time,
                        'status': 'failed',
                        'timestamp': time.time()
                    }
                
                # ê²°ê³¼ ì „ì†¡
                result_queue.put(result_item)
                total_processed += 1
                
                print(f"ğŸ“¤ GPU {gpu_id} ê²°ê³¼ ì „ì†¡ ì™„ë£Œ: Task {task_id}")
                
                # ì™„ë£Œ ì§„í–‰ë¥  ì‹ í˜¸
                try:
                    progress_queue.put_nowait({
                        'task_id': task_id,
                        'gpu_id': gpu_id,
                        'progress': 1.0,
                        'status': 'completed',
                        'timestamp': time.time()
                    })
                except queue.Full:
                    pass
                
                # ì£¼ê¸°ì  GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if total_processed % 3 == 0:
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"ğŸ’¥ GPU {gpu_id} ì‘ì—… ì˜¤ë¥˜: {e}")
                traceback.print_exc()
                
                # ì˜¤ë¥˜ ê²°ê³¼ ì „ì†¡
                try:
                    error_result = {
                        'task_id': task_id if 'task_id' in locals() else -1,
                        'gpu_id': gpu_id,
                        'video_path': video_path if 'video_path' in locals() else 'unknown',
                        'result': None,
                        'processing_time': 0.0,
                        'status': 'error',
                        'error': str(e),
                        'timestamp': time.time()
                    }
                    result_queue.put(error_result)
                    total_processed += 1
                except:
                    pass
        
        worker_time = time.time() - worker_start_time
        print(f"ğŸ GPU {gpu_id} ì›Œì»¤ ì¢…ë£Œ: {total_processed}ê°œ ì²˜ë¦¬ ({worker_time:.2f}ì´ˆ)")
        
    except Exception as e:
        print(f"ğŸ’¥ GPU {gpu_id} ì›Œì»¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        traceback.print_exc()

def save_to_hdf5_streamlined_format(result_data: Dict, output_path: str, video_id: str):
    """Streamlined ë°©ì‹ìœ¼ë¡œ HDF5 ì €ì¥ - í”„ë ˆì„ê³¼ í¬ì¦ˆ ë¶„ë¦¬ (ì™„ë²½ í˜¸í™˜)"""
    try:
        output_path_obj = Path(output_path)
        
        # í”„ë ˆì„ê³¼ í¬ì¦ˆ íŒŒì¼ ë¶„ë¦¬ (streamlined í˜•ì‹)
        frames_h5_path = output_path_obj.parent / f"{output_path_obj.stem}_frames.h5"
        poses_h5_path = output_path_obj.parent / f"{output_path_obj.stem}_poses.h5"
        
        # JPEG ê°€ë³€ ê¸¸ì´ íƒ€ì… (streamlinedì™€ ë™ì¼)
        jpeg_vlen_dtype = h5py.vlen_dtype(np.uint8)
        
        with h5py.File(frames_h5_path, 'w') as f_frames, \
             h5py.File(poses_h5_path, 'w') as f_poses:
            
            # ë°°ì¹˜ ë©”íƒ€ë°ì´í„° (streamlinedì™€ ë™ì¼í•œ êµ¬ì¡°)
            batch_metadata = {
                'folder_name': 'batch_processing',
                'folder_batch_idx': 0,
                'item_range': video_id,
                'item_types': ['WORD'],
                'direction': 'F',
                'video_count': 1,
                'creation_time': str(datetime.now())
            }
            f_frames.attrs.update(batch_metadata)
            f_poses.attrs.update(batch_metadata)
            
            # streamlinedì™€ ë™ì¼í•œ ê·¸ë£¹ ì´ë¦„ í˜•ì‹
            video_group_name = f"video_{video_id.lower()}"
            
            # === í”„ë ˆì„ íŒŒì¼ ì €ì¥ (streamlined ë°©ì‹) ===
            frame_group = f_frames.create_group(video_group_name)
            
            # JPEG í”„ë ˆì„ ë°ì´í„° ì²˜ë¦¬ (streamlinedì™€ ì™„ì „ ë™ì¼)
            jpeg_frames = result_data.get('jpeg_frames', [])
            if jpeg_frames:
                # streamlinedì™€ ë™ì¼: cv2.imencode ê²°ê³¼ì¸ numpy ë°°ì—´ ì§ì ‘ ì €ì¥
                frame_group.create_dataset("frames_jpeg", data=jpeg_frames, dtype=jpeg_vlen_dtype)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥ (streamlinedì™€ ì™„ì „ ë™ì¼í•œ í˜•ì‹)
            metadata = {
                'item_type': 'WORD',
                'item_id': int(video_id.replace('WORD', '').replace('word', '')) if 'word' in video_id.lower() else 0,
                'video_path': result_data.get('video_path', ''),
                'video_filename': Path(result_data.get('video_path', '')).name,
                'frame_count': result_data['total_frames'],
                'processing_time': result_data.get('processing_time', 0.0),
                'keypoint_scale': 8,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            f_frames.create_dataset(f"{video_group_name}/metadata", data=json.dumps(metadata))
            
            # === í¬ì¦ˆ íŒŒì¼ ì €ì¥ (streamlined ë°©ì‹) ===
            pose_group = f_poses.create_group(video_group_name)
            
            # í‚¤í¬ì¸íŠ¸ ë°ì´í„° ë³€í™˜ (streamlinedì™€ ì™„ì „ ë™ì¼)
            keypoints = result_data.get('keypoints', [])
            scores = result_data.get('scores', [])
            
            if keypoints and scores:
                # streamlinedì™€ ë™ì¼í•œ ë°ì´í„° ì²˜ë¦¬
                num_frames = len(keypoints)
                
                # RTMW 133ê°œ í‚¤í¬ì¸íŠ¸ ë°°ì—´ ì´ˆê¸°í™”
                keypoints_array = np.zeros((num_frames, 133, 2), dtype=np.float32)
                scores_array = np.zeros((num_frames, 133), dtype=np.float32)
                
                for frame_idx, (frame_kpts, frame_scores) in enumerate(zip(keypoints, scores)):
                    # ì²« ë²ˆì§¸ ì‚¬ëŒë§Œ ì‚¬ìš© (streamlinedì™€ ë™ì¼)
                    if isinstance(frame_kpts, list) and len(frame_kpts) > 0:
                        person_kpts = frame_kpts[0]
                        person_scores = frame_scores[0] if len(frame_scores) > 0 else []
                        
                        # í‚¤í¬ì¸íŠ¸ ë³€í™˜ (x,y,x,y... -> [[x,y], [x,y], ...])
                        if isinstance(person_kpts, list) and len(person_kpts) >= 266:  # 133*2
                            for joint_idx in range(133):
                                x_idx = joint_idx * 2
                                y_idx = joint_idx * 2 + 1
                                if y_idx < len(person_kpts):
                                    keypoints_array[frame_idx, joint_idx, 0] = person_kpts[x_idx]
                                    keypoints_array[frame_idx, joint_idx, 1] = person_kpts[y_idx]
                        
                        # ìŠ¤ì½”ì–´ ë³€í™˜ (streamlinedì™€ ë™ì¼)
                        if isinstance(person_scores, list) and len(person_scores) >= 133:
                            scores_array[frame_idx, :] = person_scores[:133]
                
                # í‚¤í¬ì¸íŠ¸ 8ë°° ìŠ¤ì¼€ì¼ë§ í›„ int32ë¡œ ì €ì¥ (streamlinedì™€ ì™„ì „ ë™ì¼)
                keypoints_scaled = np.round(keypoints_array * 8).astype(np.int32)
                
                # streamlinedì™€ ë™ì¼í•œ ë°ì´í„°ì…‹ ì´ë¦„ê³¼ ì••ì¶•
                pose_group.create_dataset("keypoints_scaled", data=keypoints_scaled, compression='lzf')
                pose_group.create_dataset("scores", data=scores_array, compression='lzf')
        
        print(f"âœ… Streamlined HDF5 ì €ì¥ ì™„ë£Œ: {frames_h5_path.name}, {poses_h5_path.name}")
        
    except Exception as e:
        print(f"ğŸ’¥ Streamlined HDF5 ì €ì¥ ì‹¤íŒ¨ ({video_id}): {e}")
        traceback.print_exc()
        raise


# ===== ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ =====

def find_test_videos(data_root: str = "data/1.Training/videos") -> List[str]:
    """í…ŒìŠ¤íŠ¸ìš© ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°"""
    video_paths = []
    
    # ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
    if not os.path.exists(data_root):
        data_root = "/workspace01/team03/data/mmpose/jy/data/1.Training/videos"
    
    if not os.path.exists(data_root):
        print(f"âŒ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_root}")
        return []
    
    print(f"ğŸ” ë¹„ë””ì˜¤ íŒŒì¼ íƒìƒ‰: {data_root}")
    
    # .mp4 íŒŒì¼ ì°¾ê¸°
    for root, dirs, files in os.walk(data_root):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mov')) and 'F.mp4' in file:
                file_path = os.path.join(root, file)
                video_paths.append(file_path)
    
    # ì²˜ë¦¬ ê°€ëŠ¥í•œ ê°œìˆ˜ë¡œ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
    video_paths = sorted(video_paths)[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
    
    print(f"âœ… ë°œê²¬ëœ ë¹„ë””ì˜¤: {len(video_paths)}ê°œ")
    for i, path in enumerate(video_paths[:5]):  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        file_size = os.path.getsize(path) / (1024*1024)  # MB
        print(f"   {i+1}. {os.path.basename(path)} ({file_size:.1f}MB)")
    
    if len(video_paths) > 5:
        print(f"   ... ë° {len(video_paths) - 5}ê°œ ì¶”ê°€ íŒŒì¼")
    
    return video_paths


def main():
    """ë“€ì–¼ GPU ë°°ì¹˜ 256 ì²˜ë¦¬ê¸° ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - í…ŒìŠ¤íŠ¸ ë˜ëŠ” ì „ì²´ í´ë” ì²˜ë¦¬"""
    import argparse
    import sys
    
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹± (ì„ íƒì )
    parser = argparse.ArgumentParser(description="Batch Fast Multi-ONNX Processor", add_help=False)
    parser.add_argument("--input_folder", type=str, 
                        help="ì „ì²´ í´ë” ì²˜ë¦¬ì‹œ ì…ë ¥ í´ë” ê²½ë¡œ")
    parser.add_argument("--output_folder", type=str,
                        help="ì „ì²´ í´ë” ì²˜ë¦¬ì‹œ ì¶œë ¥ í´ë” ê²½ë¡œ") 
    parser.add_argument("--batch_size", type=int, default=250,
                        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 250)")
    parser.add_argument("--test", action="store_true",
                        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œ ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬)")
    parser.add_argument("--help", "-h", action="store_true",
                        help="ë„ì›€ë§ í‘œì‹œ")
    
    # ì¸ìê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ íŒŒì‹±
    args = None
    if len(sys.argv) > 1:
        try:
            args = parser.parse_args()
        except SystemExit:
            pass
    
    if args and args.help:
        print("ğŸš€ Batch Fast Multi-ONNX Processor")
        print("=" * 60)
        print("ì‚¬ìš©ë²•:")
        print("  1. í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œ ë¹„ë””ì˜¤):")
        print("     python batch_fast_multionnx_processor.py")
        print("     python batch_fast_multionnx_processor.py --test")
        print()
        print("  2. ì „ì²´ í´ë” ì²˜ë¦¬ (250ê°œì”© ë°°ì¹˜):")
        print("     python batch_fast_multionnx_processor.py \\")
        print("       --input_folder /path/to/videos \\")
        print("       --output_folder /path/to/output \\")
        print("       --batch_size 250")
        print()
        print("ì˜µì…˜:")
        print("  --input_folder   ì…ë ¥ ë¹„ë””ì˜¤ í´ë” ê²½ë¡œ")
        print("  --output_folder  ì¶œë ¥ HDF5 í´ë” ê²½ë¡œ")
        print("  --batch_size     ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 250)")
        print("  --test          í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œë§Œ ì²˜ë¦¬)")
        print("  --help, -h      ì´ ë„ì›€ë§ í‘œì‹œ")
        return 0
    
    print("ğŸš€ Batch Fast Multi-ONNX Processor ì‹œì‘")
    print("=" * 60)
    
    # ì „ì²´ í´ë” ì²˜ë¦¬ ëª¨ë“œì¸ì§€ í™•ì¸
    if args and args.input_folder and args.output_folder:
        print("ğŸ“‚ ì „ì²´ í´ë” ì²˜ë¦¬ ëª¨ë“œ")
        print(f"   - ì…ë ¥ í´ë”: {args.input_folder}")
        print(f"   - ì¶œë ¥ í´ë”: {args.output_folder}")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {args.batch_size}ê°œì”© ì²˜ë¦¬")
        print(f"   - Streamlined ë„¤ì´ë°: batch_XX_F_frames.h5, batch_XX_F_poses.h5")
        
        try:
            result = process_full_folder_production(
                input_folder=args.input_folder,
                output_folder=args.output_folder,
                batch_size=args.batch_size,
                processing_batch_size=128,
                max_vram_usage=0.75
            )
            
            if result['status'] == 'completed':
                print("\nğŸ‰ ì „ì²´ í´ë” ì²˜ë¦¬ ì„±ê³µ!")
                return 0
            else:
                print(f"\nâŒ ì „ì²´ í´ë” ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('reason', 'unknown')}")
                return 1
                
        except Exception as e:
            print(f"\nğŸ’¥ ì „ì²´ í´ë” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            return 1
    
    else:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œ ë¹„ë””ì˜¤ ì²˜ë¦¬)")
        
        # ì„¤ì •
        config = {
            'rtmw_model_name': 'rtmw-dw-x-l_simcc-cocktail14_270e-384x288.onnx',
            'batch_size': 128,  # ì•ˆì •ì„±ì„ ìœ„í•´ 128ë¡œ ì„¤ì •
            'keypoint_scale': 8,
            'jpeg_quality': 90,
            'max_vram_usage': 0.75
        }
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = "/tmp/batch_fast_multionnx_test_output"
        
        # í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ ì°¾ê¸°
        video_paths = find_test_videos()
        
        if not video_paths:
            print("âŒ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë‹¤ìŒ ìœ„ì¹˜ì— ë¹„ë””ì˜¤ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”:")
            print("   - data/1.Training/videos")
            print("   - /workspace01/team03/data/mmpose/jy/data/1.Training/videos")
            print()
            print("ğŸ’¡ ì „ì²´ í´ë” ì²˜ë¦¬ë¥¼ ì›í•˜ì‹œë©´:")
            print("   python batch_fast_multionnx_processor.py \\")
            print("     --input_folder /path/to/videos \\")
            print("     --output_folder /path/to/output")
            return 1
        
        print(f"\nâš™ï¸ ì²˜ë¦¬ ì„¤ì •:")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {config['batch_size']}")
        print(f"   - RTMW ëª¨ë¸: {config['rtmw_model_name']}")
        print(f"   - VRAM ì‚¬ìš©ë¥ : {config['max_vram_usage']*100}%")
        print(f"   - ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
        
        # GPU í™•ì¸
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                print(f"\nğŸ–¥ï¸ GPU ì •ë³´:")
                for i in range(min(gpu_count, 2)):  # ìµœëŒ€ 2ê°œ GPUë§Œ í‘œì‹œ
                    gpu_name = torch.cuda.get_device_name(i)
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    print(f"   - GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            else:
                print("âš ï¸ CUDA GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        except ImportError:
            print("âš ï¸ PyTorchë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"\nğŸ”¥ ë“€ì–¼ GPU ì²˜ë¦¬ ì‹œì‘...")
        print(f"   - ì´ ë¹„ë””ì˜¤: {len(video_paths)}ê°œ")
        print(f"   - GPU 0ë²ˆê³¼ 1ë²ˆì´ ë™ì‹œì— ì²˜ë¦¬ë©ë‹ˆë‹¤")
        print(f"   - ê° GPUëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤")
        
        # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
        def progress_callback(progress):
            if hasattr(progress_callback, 'last_progress'):
                if progress - progress_callback.last_progress >= 0.1:  # 10%ì”© ì—…ë°ì´íŠ¸
                    print(f"ğŸ“Š ì „ì²´ ì§„í–‰ë¥ : {progress*100:.1f}%")
                    progress_callback.last_progress = progress
            else:
                progress_callback.last_progress = 0.0
        
        try:
            # ë“€ì–¼ GPU ë¹„ë™ê¸° ì²˜ë¦¬ ì‹¤í–‰
            start_time = time.time()
            
            summary = process_videos_dual_gpu_async_batch(
                video_paths=video_paths,
                output_dir=output_dir,
                config=config,
                progress_callback=progress_callback
            )
            
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ë¶„ì„
            print(f"\nğŸ ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„")
            print("=" * 60)
            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ:")
            print(f"   - ì„±ê³µ: {summary['completed']}ê°œ")
            print(f"   - ì‹¤íŒ¨: {summary['failed']}ê°œ")
            print(f"   - ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
            print(f"   - í‰ê·  ì‹œê°„: {summary['average_time_per_video']:.2f}ì´ˆ/ë¹„ë””ì˜¤")
            
            if summary['completed'] > 0:
                print(f"\nğŸ–¥ï¸ GPU ë³‘ë ¬ ì²˜ë¦¬ í™•ì¸:")
                gpu_dist = summary['gpu_distribution']
                for gpu_id, task_count in gpu_dist.items():
                    print(f"   - GPU {gpu_id}: {task_count}ê°œ ì‘ì—… ì²˜ë¦¬")
                
                # ì‘ì—… ë¶„ë°° ê· í˜•ë„ ê³„ì‚°
                if len(gpu_dist) > 1:
                    task_counts = list(gpu_dist.values())
                    balance = min(task_counts) / max(task_counts) * 100 if max(task_counts) > 0 else 0
                    print(f"   - ì‘ì—… ê· í˜•ë„: {balance:.1f}%")
                    
                    if balance > 80:
                        print("   âœ… GPU ì‘ì—… ë¶„ë°°ê°€ ê· ë“±í•©ë‹ˆë‹¤")
                    else:
                        print("   âš ï¸ GPU ì‘ì—… ë¶„ë°°ê°€ ë¶ˆê· ë“±í•©ë‹ˆë‹¤")
            
            # ì €ì¥ëœ íŒŒì¼ í™•ì¸
            if os.path.exists(output_dir):
                h5_files = [f for f in os.listdir(output_dir) if f.endswith('.h5')]
                frames_files = [f for f in h5_files if 'frames' in f]
                poses_files = [f for f in h5_files if 'poses' in f]
                
                print(f"\nğŸ’¾ ì €ì¥ëœ HDF5 íŒŒì¼:")
                print(f"   - í”„ë ˆì„ íŒŒì¼: {len(frames_files)}ê°œ")
                print(f"   - í¬ì¦ˆ íŒŒì¼: {len(poses_files)}ê°œ")
                
                # íŒŒì¼ í¬ê¸° ì •ë³´ (ì²˜ìŒ 3ê°œë§Œ)
                for f in frames_files[:3]:
                    file_path = os.path.join(output_dir, f)
                    file_size = os.path.getsize(file_path) / (1024*1024)  # MB
                    print(f"     ğŸ“„ {f} ({file_size:.1f}MB)")
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            if summary['completed'] > 0:
                total_videos = summary['completed']
                avg_fps = 0
                
                for result_data in summary['results'].values():
                    if result_data.get('status') == 'completed' and result_data.get('result'):
                        fps = result_data['result'].get('fps', 0)
                        avg_fps += fps
                
                if total_videos > 0:
                    avg_fps /= total_videos
                    print(f"\nâš¡ ì„±ëŠ¥ ì§€í‘œ:")
                    print(f"   - í‰ê·  FPS: {avg_fps:.1f}")
                    print(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
                    
                    if avg_fps > 10:
                        print("   âœ… ìš°ìˆ˜í•œ ì²˜ë¦¬ ì„±ëŠ¥")
                    elif avg_fps > 5:
                        print("   âœ… ì–‘í˜¸í•œ ì²˜ë¦¬ ì„±ëŠ¥")
                    else:
                        print("   âš ï¸ ì„±ëŠ¥ ìµœì í™” í•„ìš”")
            
            # GPU ë³‘ë ¬ ì²˜ë¦¬ ê²€ì¦
            gpu_used = len([gpu_id for gpu_id, count in summary['gpu_distribution'].items() if count > 0])
            print(f"\nğŸ” GPU ë³‘ë ¬ ì²˜ë¦¬ ê²€ì¦:")
            if gpu_used > 1:
                print(f"   âœ… {gpu_used}ê°œ GPUê°€ ëª¨ë‘ ì‘ì—…ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤")
                print("   âœ… ë“€ì–¼ GPU ë³‘ë ¬ ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤")
            else:
                print("   âš ï¸ ë‹¨ì¼ GPUë§Œ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            # Streamlined HDF5 í˜•ì‹ ê²€ì¦
            if frames_files and poses_files:
                print(f"\nğŸ“‹ Streamlined HDF5 í˜•ì‹ ê²€ì¦:")
                print("   âœ… í”„ë ˆì„ê³¼ í¬ì¦ˆ íŒŒì¼ì´ ë¶„ë¦¬ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
                print("   âœ… Streamlined í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥ ì™„ë£Œ")
            
            print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ ì²˜ë¦¬ ì„±ê³µ!")
            print(f"   - ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ")
            print(f"   - GPU ë³‘ë ¬ ì²˜ë¦¬ í™•ì¸")  
            print(f"   - Streamlined HDF5 ì €ì¥ ì™„ë£Œ")
            
            # ì „ì²´ í´ë” ì²˜ë¦¬ ì•ˆë‚´
            print(f"\nğŸ’¡ ì „ì²´ í´ë” ì²˜ë¦¬ë¥¼ ì›í•˜ì‹œë©´:")
            print(f"   python {sys.argv[0]} \\")
            print(f"     --input_folder /path/to/videos \\") 
            print(f"     --output_folder /path/to/output \\")
            print(f"     --batch_size 250")
            
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
            return 1
        except Exception as e:
            print(f"\nğŸ’¥ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            return 1
    
    print("=" * 60)
    print("ì²˜ë¦¬ ì™„ë£Œ")
    return 0


# ===== Production ì „ì²´ í´ë” ì²˜ë¦¬ í•¨ìˆ˜ë“¤ =====

def find_all_videos_in_folder(folder_path: str) -> List[str]:
    """í´ë”ì—ì„œ ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°"""
    video_paths = []
    
    if not os.path.exists(folder_path):
        print(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder_path}")
        return []
    
    print(f"ğŸ” ë¹„ë””ì˜¤ íŒŒì¼ íƒìƒ‰: {folder_path}")
    
    # ì§€ì›ë˜ëŠ” ë¹„ë””ì˜¤ í™•ì¥ì
    video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm'}
    
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            file_lower = file.lower()
            file_ext = Path(file).suffix.lower()
            
            # ë¹„ë””ì˜¤ íŒŒì¼ì´ê³  '_F.mp4' íŒ¨í„´ì„ í¬í•¨í•˜ëŠ” ê²½ìš°
            if file_ext in video_extensions and '_F.mp4' in file:
                file_path = os.path.join(root, file)
                video_paths.append(file_path)
    
    # ì •ë ¬
    video_paths = sorted(video_paths)
    
    print(f"âœ… ë°œê²¬ëœ ë¹„ë””ì˜¤: {len(video_paths)}ê°œ")
    return video_paths

def create_batches(video_paths: List[str], batch_size: int = 250) -> List[List[str]]:
    """ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°"""
    batches = []
    
    for i in range(0, len(video_paths), batch_size):
        batch = video_paths[i:i + batch_size]
        batches.append(batch)
    
    print(f"ğŸ“¦ ë°°ì¹˜ ìƒì„±: {len(batches)}ê°œ ë°°ì¹˜ (ë°°ì¹˜ë‹¹ ìµœëŒ€ {batch_size}ê°œ)")
    for i, batch in enumerate(batches):
        print(f"   - ë°°ì¹˜ {i:02d}: {len(batch)}ê°œ ë¹„ë””ì˜¤")
    
    return batches

def get_streamlined_naming(batch_idx: int, data_type: str) -> str:
    """Streamlined ë„¤ì´ë° ê·œì¹™ì— ë”°ë¥¸ íŒŒì¼ëª… ìƒì„±
    
    Args:
        batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)
        data_type: 'frames' ë˜ëŠ” 'poses'
    
    Returns:
        íŒŒì¼ëª… (ì˜ˆ: batch_00_F_frames.h5, batch_01_F_poses.h5)
    """
    return f"batch_{batch_idx:02d}_F_{data_type}.h5"

def save_batch_to_streamlined_hdf5(batch_results: List[Dict], batch_idx: int, output_dir: str):
    """ë°°ì¹˜ ê²°ê³¼ë¥¼ Streamlined HDF5 í˜•ì‹ìœ¼ë¡œ ì €ì¥ (250ê°œì”©)"""
    try:
        # Streamlined ë„¤ì´ë° ê·œì¹™ ì ìš©
        frames_filename = get_streamlined_naming(batch_idx, 'frames')
        poses_filename = get_streamlined_naming(batch_idx, 'poses')
        
        frames_h5_path = os.path.join(output_dir, frames_filename)
        poses_h5_path = os.path.join(output_dir, poses_filename)
        
        print(f"ğŸ’¾ ë°°ì¹˜ {batch_idx:02d} HDF5 ì €ì¥ ì‹œì‘:")
        print(f"   - í”„ë ˆì„: {frames_filename}")
        print(f"   - í¬ì¦ˆ: {poses_filename}")
        print(f"   - ë¹„ë””ì˜¤ ìˆ˜: {len(batch_results)}ê°œ")
        
        # JPEG ê°€ë³€ ê¸¸ì´ íƒ€ì…
        jpeg_vlen_dtype = h5py.vlen_dtype(np.uint8)
        
        with h5py.File(frames_h5_path, 'w') as f_frames, \
             h5py.File(poses_h5_path, 'w') as f_poses:
            
            # ë°°ì¹˜ ë©”íƒ€ë°ì´í„°
            batch_metadata = {
                'folder_name': f'batch_{batch_idx:02d}',
                'folder_batch_idx': batch_idx,
                'item_range': f'batch_{batch_idx:02d}',
                'item_types': ['WORD'],
                'direction': 'F',
                'video_count': len(batch_results),
                'creation_time': str(datetime.now()),
                'processing_version': 'batch_fast_multionnx_v1.0'
            }
            f_frames.attrs.update(batch_metadata)
            f_poses.attrs.update(batch_metadata)
            
            # ê° ë¹„ë””ì˜¤ ê²°ê³¼ ì²˜ë¦¬
            for video_idx, result_item in enumerate(batch_results):
                video_path = result_item['video_path']
                result_data = result_item['result']
                
                if not result_data:
                    print(f"âš ï¸ ë¹ˆ ê²°ê³¼ ìŠ¤í‚µ: {Path(video_path).name}")
                    continue
                
                # ë¹„ë””ì˜¤ ID ì¶”ì¶œ (NIA_SL_WORD_01_01_F.mp4 -> word_01_01)
                video_filename = Path(video_path).stem
                try:
                    if 'WORD' in video_filename.upper():
                        # NIA_SL_WORD_01_01_F -> word_01_01
                        parts = video_filename.split('_')
                        word_part_idx = None
                        for i, part in enumerate(parts):
                            if 'WORD' in part.upper():
                                word_part_idx = i
                                break
                        
                        if word_part_idx is not None and len(parts) > word_part_idx + 2:
                            video_id = f"word_{parts[word_part_idx + 1]}_{parts[word_part_idx + 2]}"
                        else:
                            video_id = f"video_{batch_idx:02d}_{video_idx:03d}"
                    else:
                        video_id = f"video_{batch_idx:02d}_{video_idx:03d}"
                except:
                    video_id = f"video_{batch_idx:02d}_{video_idx:03d}"
                
                video_group_name = f"video_{video_id.lower()}"
                
                print(f"   ğŸ“¹ ì²˜ë¦¬ ì¤‘: {video_filename} -> {video_group_name}")
                
                # === í”„ë ˆì„ íŒŒì¼ ì €ì¥ ===
                frame_group = f_frames.create_group(video_group_name)
                
                jpeg_frames = result_data.get('jpeg_frames', [])
                if jpeg_frames:
                    frame_group.create_dataset("frames_jpeg", data=jpeg_frames, dtype=jpeg_vlen_dtype)
                
                # í”„ë ˆì„ ë©”íƒ€ë°ì´í„°
                frame_metadata = {
                    'item_type': 'WORD',
                    'item_id': video_idx,
                    'video_path': video_path,
                    'video_filename': Path(video_path).name,
                    'frame_count': result_data.get('total_frames', 0),
                    'processing_time': result_data.get('processing_time', 0.0),
                    'keypoint_scale': 8,
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'gpu_id': result_item.get('gpu_id', -1),
                    'fps_achieved': result_item.get('fps_achieved', 0.0)
                }
                frame_group.create_dataset("metadata", data=json.dumps(frame_metadata))
                
                # === í¬ì¦ˆ íŒŒì¼ ì €ì¥ ===
                pose_group = f_poses.create_group(video_group_name)
                
                keypoints = result_data.get('keypoints', [])
                scores = result_data.get('scores', [])
                
                if keypoints and scores:
                    num_frames = len(keypoints)
                    
                    # RTMW 133ê°œ í‚¤í¬ì¸íŠ¸ ë°°ì—´ ì´ˆê¸°í™”
                    keypoints_array = np.zeros((num_frames, 133, 2), dtype=np.float32)
                    scores_array = np.zeros((num_frames, 133), dtype=np.float32)
                    
                    for frame_idx, (frame_kpts, frame_scores) in enumerate(zip(keypoints, scores)):
                        # ì²« ë²ˆì§¸ ì‚¬ëŒë§Œ ì‚¬ìš©
                        if isinstance(frame_kpts, list) and len(frame_kpts) > 0:
                            person_kpts = frame_kpts[0]
                            person_scores = frame_scores[0] if len(frame_scores) > 0 else []
                            
                            # í‚¤í¬ì¸íŠ¸ ë³€í™˜ (17ê°œ -> 133ê°œ ë§¤í•‘)
                            if isinstance(person_kpts, list) and len(person_kpts) >= 34:  # 17*2
                                # 17ê°œ í‚¤í¬ì¸íŠ¸ë§Œ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ
                                for joint_idx in range(min(17, 133)):
                                    x_idx = joint_idx * 2
                                    y_idx = joint_idx * 2 + 1
                                    if y_idx < len(person_kpts):
                                        keypoints_array[frame_idx, joint_idx, 0] = person_kpts[x_idx]
                                        keypoints_array[frame_idx, joint_idx, 1] = person_kpts[y_idx]
                            
                            # ìŠ¤ì½”ì–´ ë³€í™˜
                            if isinstance(person_scores, list) and len(person_scores) >= 17:
                                scores_array[frame_idx, :min(17, 133)] = person_scores[:min(17, 133)]
                    
                    # 8ë°° ìŠ¤ì¼€ì¼ë§ í›„ int32ë¡œ ì €ì¥ (Streamlined ê·œê²©)
                    keypoints_scaled = np.round(keypoints_array * 8).astype(np.int32)
                    
                    pose_group.create_dataset("keypoints_scaled", data=keypoints_scaled, compression='lzf')
                    pose_group.create_dataset("scores", data=scores_array, compression='lzf')
        
        # íŒŒì¼ í¬ê¸° ì •ë³´
        frames_size = os.path.getsize(frames_h5_path) / (1024*1024)
        poses_size = os.path.getsize(poses_h5_path) / (1024*1024)
        
        print(f"âœ… ë°°ì¹˜ {batch_idx:02d} ì €ì¥ ì™„ë£Œ:")
        print(f"   - {frames_filename}: {frames_size:.1f}MB")
        print(f"   - {poses_filename}: {poses_size:.1f}MB")
        print(f"   - ì´ ìš©ëŸ‰: {frames_size + poses_size:.1f}MB")
        
    except Exception as e:
        print(f"ğŸ’¥ ë°°ì¹˜ {batch_idx:02d} ì €ì¥ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        raise

def process_full_folder_production(
    input_folder: str,
    output_folder: str,
    batch_size: int = 250,
    processing_batch_size: int = 128,
    max_vram_usage: float = 0.75
) -> Dict:
    """ì „ì²´ í´ë”ë¥¼ 250ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ Production ì²˜ë¦¬"""
    
    print("ğŸš€ Production ì „ì²´ í´ë” ì²˜ë¦¬ ì‹œì‘")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥ í´ë”: {input_folder}")
    print(f"ğŸ“ ì¶œë ¥ í´ë”: {output_folder}")
    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œ")
    print(f"âš™ï¸ ì²˜ë¦¬ ë°°ì¹˜ í¬ê¸°: {processing_batch_size}")
    print(f"ğŸ–¥ï¸ ìµœëŒ€ VRAM ì‚¬ìš©ë¥ : {max_vram_usage*100}%")
    
    start_total_time = time.time()
    
    # ì¶œë ¥ í´ë” ìƒì„±
    os.makedirs(output_folder, exist_ok=True)
    
    # 1. ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
    all_video_paths = find_all_videos_in_folder(input_folder)
    
    if not all_video_paths:
        print("âŒ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {'status': 'failed', 'reason': 'no_videos'}
    
    total_videos = len(all_video_paths)
    print(f"\nğŸ“Š ë°œê²¬ëœ ì´ ë¹„ë””ì˜¤: {total_videos}ê°œ")
    
    # 2. 250ê°œì”© ë°°ì¹˜ ìƒì„±
    batches = create_batches(all_video_paths, batch_size)
    total_batches = len(batches)
    
    print(f"ğŸ“¦ ì´ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ")
    
    # GPU í™•ì¸
    try:
        import torch
        gpu_count = torch.cuda.device_count()
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {gpu_count}ê°œ")
        
        if gpu_count < 2:
            print("âš ï¸ ë“€ì–¼ GPUê°€ ì•„ë‹™ë‹ˆë‹¤. ê°€ìš© GPUë¡œ ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    except:
        print("âš ï¸ GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        gpu_count = 1
    
    # Processing ì„¤ì •
    config = {
        'rtmw_model_name': 'rtmw-dw-x-l_simcc-cocktail14_270e-384x288.onnx',
        'batch_size': processing_batch_size,
        'keypoint_scale': 8,
        'jpeg_quality': 90,
        'max_vram_usage': max_vram_usage
    }
    
    # ë°°ì¹˜ë³„ ì²˜ë¦¬ ê²°ê³¼
    batch_results = []
    successful_batches = 0
    failed_batches = 0
    
    print(f"\nğŸ”¥ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...")
    print("=" * 80)
    
    for batch_idx, batch_videos in enumerate(batches):
        batch_start_time = time.time()
        
        print(f"\nğŸ“¦ ë°°ì¹˜ {batch_idx:02d}/{total_batches-1:02d} ì²˜ë¦¬ ì‹œì‘")
        print(f"   - ë¹„ë””ì˜¤ ìˆ˜: {len(batch_videos)}ê°œ")
        print(f"   - ì§„í–‰ë¥ : {(batch_idx)/total_batches*100:.1f}%")
        
        try:
            # ì„ì‹œ ì¶œë ¥ ë””ë ‰í† ë¦¬
            temp_output_dir = os.path.join(output_folder, f"temp_batch_{batch_idx:02d}")
            os.makedirs(temp_output_dir, exist_ok=True)
            
            # ë“€ì–¼ GPU ì²˜ë¦¬ (ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ GPUë¡œ)
            def batch_progress_callback(progress):
                batch_progress = (batch_idx + progress) / total_batches
                print(f"ğŸ“Š ì „ì²´ ì§„í–‰ë¥ : {batch_progress*100:.1f}% (ë°°ì¹˜ {batch_idx:02d}: {progress*100:.1f}%)")
            
            summary = process_videos_dual_gpu_async_batch(
                video_paths=batch_videos,
                output_dir=temp_output_dir,
                config=config,
                progress_callback=batch_progress_callback
            )
            
            batch_processing_time = time.time() - batch_start_time
            
            # ê°œë³„ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°°ì¹˜ ê²°ê³¼ë¡œ ìˆ˜ì§‘
            current_batch_results = []
            for task_id, result_item in summary['results'].items():
                if result_item['status'] == 'completed':
                    current_batch_results.append(result_item)
            
            if current_batch_results:
                # Streamlined HDF5 ë°°ì¹˜ ì €ì¥
                save_batch_to_streamlined_hdf5(current_batch_results, batch_idx, output_folder)
                
                successful_batches += 1
                
                print(f"âœ… ë°°ì¹˜ {batch_idx:02d} ì™„ë£Œ:")
                print(f"   - ì„±ê³µ: {len(current_batch_results)}ê°œ")
                print(f"   - ì‹¤íŒ¨: {len(batch_videos) - len(current_batch_results)}ê°œ") 
                print(f"   - ì²˜ë¦¬ ì‹œê°„: {batch_processing_time:.2f}ì´ˆ")
                print(f"   - í‰ê·  ì‹œê°„: {batch_processing_time/len(batch_videos):.2f}ì´ˆ/ë¹„ë””ì˜¤")
            else:
                print(f"âŒ ë°°ì¹˜ {batch_idx:02d} ì²˜ë¦¬ ì‹¤íŒ¨: ì„±ê³µí•œ ë¹„ë””ì˜¤ê°€ ì—†ìŒ")
                failed_batches += 1
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            try:
                import shutil
                shutil.rmtree(temp_output_dir)
            except:
                pass
            
            batch_results.append({
                'batch_idx': batch_idx,
                'video_count': len(batch_videos),
                'successful_count': len(current_batch_results),
                'failed_count': len(batch_videos) - len(current_batch_results),
                'processing_time': batch_processing_time,
                'summary': summary
            })
            
        except Exception as e:
            print(f"ğŸ’¥ ë°°ì¹˜ {batch_idx:02d} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            failed_batches += 1
            
            batch_results.append({
                'batch_idx': batch_idx,
                'video_count': len(batch_videos),
                'successful_count': 0,
                'failed_count': len(batch_videos),
                'processing_time': time.time() - batch_start_time,
                'error': str(e)
            })
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    total_time = time.time() - start_total_time
    
    # ìµœì¢… ê²°ê³¼ ì •ë¦¬
    total_successful_videos = sum(r.get('successful_count', 0) for r in batch_results)
    total_failed_videos = sum(r.get('failed_count', 0) for r in batch_results)
    
    print(f"\nğŸ Production ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ")
    print("=" * 80)
    print(f"âœ… ìµœì¢… ê²°ê³¼:")
    print(f"   - ì´ ë°°ì¹˜: {total_batches}ê°œ")
    print(f"   - ì„±ê³µ ë°°ì¹˜: {successful_batches}ê°œ") 
    print(f"   - ì‹¤íŒ¨ ë°°ì¹˜: {failed_batches}ê°œ")
    print(f"   - ì´ ë¹„ë””ì˜¤: {total_videos}ê°œ")
    print(f"   - ì„±ê³µ ë¹„ë””ì˜¤: {total_successful_videos}ê°œ")
    print(f"   - ì‹¤íŒ¨ ë¹„ë””ì˜¤: {total_failed_videos}ê°œ")
    print(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/3600:.1f}ì‹œê°„)")
    print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/max(total_videos, 1):.2f}ì´ˆ/ë¹„ë””ì˜¤")
    
    # ìƒì„±ëœ HDF5 íŒŒì¼ í™•ì¸
    h5_files = [f for f in os.listdir(output_folder) if f.endswith('.h5')]
    frames_files = [f for f in h5_files if 'frames' in f]
    poses_files = [f for f in h5_files if 'poses' in f]
    
    print(f"\nğŸ’¾ ìƒì„±ëœ Streamlined HDF5 íŒŒì¼:")
    print(f"   - í”„ë ˆì„ íŒŒì¼: {len(frames_files)}ê°œ")
    print(f"   - í¬ì¦ˆ íŒŒì¼: {len(poses_files)}ê°œ")
    print(f"   - ë„¤ì´ë° ê·œì¹™: batch_XX_F_frames.h5, batch_XX_F_poses.h5")
    
    # íŒŒì¼ í¬ê¸° ì •ë³´
    total_size = 0
    for f in h5_files:
        file_path = os.path.join(output_folder, f)
        file_size = os.path.getsize(file_path)
        total_size += file_size
    
    print(f"   - ì´ íŒŒì¼ í¬ê¸°: {total_size / (1024*1024*1024):.2f}GB")
    
    # ì²˜ë¦¬ ì„±ëŠ¥ ì •ë³´
    if total_successful_videos > 0:
        overall_fps = total_successful_videos / max(total_time, 1)
        print(f"\nâš¡ ì²˜ë¦¬ ì„±ëŠ¥:")
        print(f"   - ì „ì²´ ì²˜ë¦¬ ì†ë„: {overall_fps:.2f} ë¹„ë””ì˜¤/ì´ˆ")
        
        if successful_batches == total_batches:
            print("   âœ… ëª¨ë“  ë°°ì¹˜ê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            success_rate = 100.0
        else:
            success_rate = (successful_batches / total_batches) * 100
            print(f"   ğŸ“Š ë°°ì¹˜ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        video_success_rate = (total_successful_videos / total_videos) * 100
        print(f"   ğŸ“Š ë¹„ë””ì˜¤ ì„±ê³µë¥ : {video_success_rate:.1f}%")
    
    return {
        'status': 'completed',
        'total_batches': total_batches,
        'successful_batches': successful_batches,
        'failed_batches': failed_batches,
        'total_videos': total_videos,
        'successful_videos': total_successful_videos,
        'failed_videos': total_failed_videos,
        'total_time': total_time,
        'batch_results': batch_results,
        'output_files': {
            'frames_files': frames_files,
            'poses_files': poses_files,
            'total_size_gb': total_size / (1024*1024*1024)
        }
    }

def main_production():
    """Production ë©”ì¸ í•¨ìˆ˜ - ì „ì²´ í´ë”ë¥¼ 250ê°œì”© ë°°ì¹˜ ì²˜ë¦¬"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Batch Fast Multi-ONNX Processor - Production")
    parser.add_argument("--input_folder", type=str, required=True,
                        help="ì…ë ¥ ë¹„ë””ì˜¤ í´ë” ê²½ë¡œ")
    parser.add_argument("--output_folder", type=str, required=True, 
                        help="ì¶œë ¥ HDF5 í´ë” ê²½ë¡œ")
    parser.add_argument("--batch_size", type=int, default=250,
                        help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 250)")
    parser.add_argument("--processing_batch_size", type=int, default=128,
                        help="GPU ì²˜ë¦¬ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 128)")
    parser.add_argument("--max_vram_usage", type=float, default=0.75,
                        help="ìµœëŒ€ VRAM ì‚¬ìš©ë¥  (ê¸°ë³¸ê°’: 0.75)")
    
    args = parser.parse_args()
    
    print("ğŸš€ Production Mode - Batch Fast Multi-ONNX Processor")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥ í´ë”: {args.input_folder}")
    print(f"ğŸ“ ì¶œë ¥ í´ë”: {args.output_folder}")
    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {args.batch_size}ê°œ")
    print(f"âš™ï¸ ì²˜ë¦¬ ë°°ì¹˜: {args.processing_batch_size}")
    print(f"ğŸ–¥ï¸ VRAM ì‚¬ìš©ë¥ : {args.max_vram_usage*100}%")
    
    try:
        result = process_full_folder_production(
            input_folder=args.input_folder,
            output_folder=args.output_folder,
            batch_size=args.batch_size,
            processing_batch_size=args.processing_batch_size,
            max_vram_usage=args.max_vram_usage
        )
        
        if result['status'] == 'completed':
            print("\nğŸ‰ Production ì²˜ë¦¬ ì„±ê³µ!")
            return 0
        else:
            print(f"\nâŒ Production ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('reason', 'unknown')}")
            return 1
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        return 1
    except Exception as e:
        print(f"\nğŸ’¥ Production ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # ëª…ë ¹ì¤„ ì¸ìê°€ ìˆìœ¼ë©´ Production ëª¨ë“œ
        exit(main_production())
    else:
        # ì¸ìê°€ ì—†ìœ¼ë©´ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        exit(main())


