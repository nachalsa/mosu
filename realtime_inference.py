#!/usr/bin/env python3
"""
ì‹¤ì‹œê°„ ìˆ˜í™” ì¸ì‹ ì¶”ë¡ ê¸°
"""

import torch
import torch.nn.functional as F
import numpy as np
import cv2
import json
import time
from pathlib import Path
from collections import deque
from typing import Dict, List, Tuple, Optional, Deque
import logging

from sign_language_model import SequenceToSequenceSignModel, RealtimeDecoder

logger = logging.getLogger(__name__)

class AdaptiveWindow:
    """ì ì‘í˜• ìŠ¬ë¼ì´ë”© ìœˆë„ìš°"""
    
    def __init__(self, 
                 max_window: int = 120,    # 4ì´ˆ (30fps ê¸°ì¤€)
                 min_window: int = 15,     # 0.5ì´ˆ 
                 overlap: int = 30):       # 1ì´ˆ ì˜¤ë²„ë©
        self.max_window = max_window
        self.min_window = min_window
        self.overlap = overlap
        
        self.buffer: Deque[np.ndarray] = deque(maxlen=max_window)
        self.frame_count = 0
        
    def add_frame(self, pose_features: np.ndarray):
        """í”„ë ˆì„ ì¶”ê°€"""
        self.buffer.append(pose_features)
        self.frame_count += 1
        
    def should_process(self) -> bool:
        """ì²˜ë¦¬ ì—¬ë¶€ íŒë‹¨"""
        return len(self.buffer) >= self.min_window
    
    def get_window(self) -> np.ndarray:
        """í˜„ì¬ ìœˆë„ìš° ë°˜í™˜"""
        if len(self.buffer) == 0:
            return np.zeros((self.min_window, 133, 3), dtype=np.float32)
        
        window_data = np.array(list(self.buffer))  # [frames, 133, 3]
        
        if len(window_data) < self.min_window:
            # íŒ¨ë”© ì¶”ê°€
            padding = np.zeros((self.min_window - len(window_data), 133, 3), dtype=np.float32)
            window_data = np.concatenate([padding, window_data], axis=0)
        
        return window_data
    
    def reset(self):
        """ë²„í¼ ë¦¬ì…‹"""
        self.buffer.clear()
        self.frame_count = 0

class PoseFeatureExtractor:
    """í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œê¸° (ë”ë¯¸ - MediaPipe ë“±ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”)"""
    
    def __init__(self):
        self.n_keypoints = 133
        
    def extract_pose_features(self, frame: np.ndarray) -> np.ndarray:
        """í”„ë ˆì„ì—ì„œ í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œ"""
        # ì‹¤ì œë¡œëŠ” MediaPipeë‚˜ ë‹¤ë¥¸ í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ë”ë¯¸ ë°ì´í„° ìƒì„±
        
        h, w = frame.shape[:2]
        
        # ë”ë¯¸ í‚¤í¬ì¸íŠ¸ (ì‹¤ì œë¡œëŠ” MediaPipe ê²°ê³¼ ì‚¬ìš©)
        keypoints = np.random.rand(self.n_keypoints, 2) * np.array([w, h])  # [133, 2]
        scores = np.random.rand(self.n_keypoints) * 10  # [133]
        
        # [133, 3] í˜•íƒœë¡œ ê²°í•© (x, y, score)
        pose_features = np.zeros((self.n_keypoints, 3), dtype=np.float32)
        pose_features[:, :2] = keypoints
        pose_features[:, 2] = scores
        
        return pose_features

class RealTimeSignLanguageInference:
    """ì‹¤ì‹œê°„ ìˆ˜í™” ì¸ì‹ ì¶”ë¡ ê¸°"""
    
    def __init__(self,
                 model_path: str,
                 vocab_path: str = None,
                 device: str = "cuda",
                 confidence_threshold: float = 0.7,
                 boundary_threshold: float = 0.8):
        
        self.device = torch.device(device)
        self.confidence_threshold = confidence_threshold
        self.boundary_threshold = boundary_threshold
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model(model_path)
        
        # Vocabulary ë¡œë“œ
        self._load_vocabulary(vocab_path)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.window = AdaptiveWindow()
        self.pose_extractor = PoseFeatureExtractor()
        self.decoder = RealtimeDecoder(
            vocab_size=len(self.words),
            confidence_threshold=confidence_threshold,
            boundary_threshold=boundary_threshold
        )
        
        # í†µê³„
        self.frame_count = 0
        self.inference_times = deque(maxlen=100)
        self.detected_words = []
        
        logger.info(f"âœ… ì‹¤ì‹œê°„ ì¶”ë¡ ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"   - Vocabulary í¬ê¸°: {len(self.words)}")
        logger.info(f"   - ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def _load_model(self, model_path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        model_path = Path(model_path)
        if not model_path.exists():
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # ëª¨ë¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì„¤ì • ì¶”ì¶œ)
        vocab_size = len(checkpoint.get('vocab_words', []))
        if vocab_size == 0:
            vocab_size = 442  # ê¸°ë³¸ê°’
        
        self.model = SequenceToSequenceSignModel(
            vocab_size=vocab_size,
            embed_dim=256,
            num_encoder_layers=6,
            num_decoder_layers=4,
            num_heads=8
        )
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        logger.info(f"ğŸ“š ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    
    def _load_vocabulary(self, vocab_path: str = None):
        """Vocabulary ë¡œë“œ"""
        if vocab_path and Path(vocab_path).exists():
            with open(vocab_path, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
            self.words = vocab_data['words']
            self.word_to_id = vocab_data['word_to_id']
        else:
            # ê¸°ë³¸ vocabulary (ë”ë¯¸)
            self.words = [f"ë‹¨ì–´_{i:03d}" for i in range(442)]
            self.word_to_id = {word: i for i, word in enumerate(self.words)}
        
        logger.info(f"ğŸ“– Vocabulary ë¡œë“œ: {len(self.words)}ê°œ ë‹¨ì–´")
    
    @torch.no_grad()
    def process_frame(self, frame: np.ndarray) -> Optional[str]:
        """í”„ë ˆì„ ì²˜ë¦¬ ë° ì¶”ë¡ """
        self.frame_count += 1
        
        # 1. í¬ì¦ˆ íŠ¹ì§• ì¶”ì¶œ
        pose_features = self.pose_extractor.extract_pose_features(frame)  # [133, 3]
        
        # 2. ìœˆë„ìš°ì— ì¶”ê°€
        self.window.add_frame(pose_features)
        
        # 3. ì²˜ë¦¬ ê°€ëŠ¥í•œì§€ í™•ì¸
        if not self.window.should_process():
            return None
        
        # 4. ëª¨ë¸ ì¶”ë¡ 
        start_time = time.time()
        
        # ìœˆë„ìš° ë°ì´í„° ì¤€ë¹„
        window_data = self.window.get_window()  # [frames, 133, 3]
        
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ ë° í…ì„œ ë³€í™˜
        input_tensor = torch.from_numpy(window_data).unsqueeze(0).to(self.device)  # [1, frames, 133, 3]
        
        # ëª¨ë¸ ì¶”ë¡ 
        outputs = self.model(input_tensor)
        
        # í˜„ì¬ í”„ë ˆì„ (ë§ˆì§€ë§‰ í”„ë ˆì„)ì˜ ì¶œë ¥ ì‚¬ìš©
        current_frame_idx = -1
        word_logits = outputs['word_logits'][0, current_frame_idx]  # [vocab_size]
        boundary_logits = outputs['boundary_logits'][0, current_frame_idx]  # [3]
        confidence_score = outputs['confidence_scores'][0, current_frame_idx]  # scalar
        
        # 5. ì‹¤ì‹œê°„ ë””ì½”ë” ì²˜ë¦¬
        detected_word_id = self.decoder.process_frame_output(
            word_logits, boundary_logits, confidence_score
        )
        
        inference_time = time.time() - start_time
        self.inference_times.append(inference_time)
        
        # 6. ê²°ê³¼ ì²˜ë¦¬
        if detected_word_id is not None:
            detected_word = self.words[detected_word_id]
            self.detected_words.append({
                'word': detected_word,
                'word_id': detected_word_id,
                'frame': self.frame_count,
                'timestamp': time.time(),
                'confidence': confidence_score.item()
            })
            
            logger.info(f"ğŸ¯ ë‹¨ì–´ ê²€ì¶œ: '{detected_word}' (ì‹ ë¢°ë„: {confidence_score:.3f})")
            return detected_word
        
        return None
    
    def get_statistics(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        if len(self.inference_times) == 0:
            avg_fps = 0
        else:
            avg_inference_time = np.mean(self.inference_times)
            avg_fps = 1.0 / avg_inference_time if avg_inference_time > 0 else 0
        
        return {
            'frame_count': self.frame_count,
            'detected_words_count': len(self.detected_words),
            'avg_inference_fps': avg_fps,
            'avg_inference_time_ms': np.mean(self.inference_times) * 1000 if self.inference_times else 0,
            'decoder_state': self.decoder.state,
            'buffer_size': len(self.window.buffer)
        }
    
    def reset(self):
        """ìƒíƒœ ë¦¬ì…‹"""
        self.window.reset()
        self.decoder.reset_state()
        self.frame_count = 0
        self.detected_words = []
        self.inference_times.clear()
        
        logger.info("ğŸ”„ ì¶”ë¡ ê¸° ìƒíƒœ ë¦¬ì…‹")

class WebcamInferenceDemo:
    """ì›¹ìº  ì‹¤ì‹œê°„ ì¶”ë¡  ë°ëª¨"""
    
    def __init__(self, 
                 model_path: str,
                 camera_id: int = 0,
                 display_width: int = 1280,
                 display_height: int = 720):
        
        self.camera_id = camera_id
        self.display_width = display_width
        self.display_height = display_height
        
        # ì¶”ë¡ ê¸° ì´ˆê¸°í™”
        self.inference = RealTimeSignLanguageInference(model_path)
        
        # ì›¹ìº  ì´ˆê¸°í™”
        self.cap = cv2.VideoCapture(camera_id)
        if not self.cap.isOpened():
            raise RuntimeError(f"ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {camera_id}")
        
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, display_width)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, display_height)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
        
        # UI ìƒíƒœ
        self.recent_words = deque(maxlen=5)
        self.show_stats = True
        
        logger.info(f"ğŸ¥ ì›¹ìº  ë°ëª¨ ì´ˆê¸°í™” ì™„ë£Œ: {display_width}x{display_height}")
    
    def draw_ui(self, frame: np.ndarray) -> np.ndarray:
        """UI ìš”ì†Œ ê·¸ë¦¬ê¸°"""
        vis_frame = frame.copy()
        h, w = vis_frame.shape[:2]
        
        # ë°˜íˆ¬ëª… íŒ¨ë„ ë°°ê²½
        overlay = vis_frame.copy()
        cv2.rectangle(overlay, (10, 10), (w-10, 120), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, vis_frame, 0.3, 0, vis_frame)
        
        # ì œëª©
        cv2.putText(vis_frame, "Real-Time Sign Language Recognition", 
                   (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # ìµœê·¼ ê²€ì¶œëœ ë‹¨ì–´ë“¤
        recent_text = "Recent Words: " + " -> ".join(self.recent_words)
        cv2.putText(vis_frame, recent_text,
                   (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # í†µê³„ í‘œì‹œ
        if self.show_stats:
            stats = self.inference.get_statistics()
            stats_text = f"FPS: {stats['avg_inference_fps']:.1f} | " \
                        f"Frame: {stats['frame_count']} | " \
                        f"Detected: {stats['detected_words_count']} | " \
                        f"State: {stats['decoder_state']}"
            cv2.putText(vis_frame, stats_text,
                       (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        
        # í‚¤ ì•ˆë‚´
        cv2.putText(vis_frame, "Press 'q' to quit, 'r' to reset, 's' to toggle stats",
                   (20, h-20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        return vis_frame
    
    def run(self):
        """ë©”ì¸ ë£¨í”„ ì‹¤í–‰"""
        logger.info("ğŸš€ ì›¹ìº  ë°ëª¨ ì‹œì‘ - 'q'ë¡œ ì¢…ë£Œ")
        
        try:
            while True:
                ret, frame = self.cap.read()
                if not ret:
                    logger.error("í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                    break
                
                # ì¶”ë¡  ì‹¤í–‰
                detected_word = self.inference.process_frame(frame)
                
                # ê²€ì¶œëœ ë‹¨ì–´ ì²˜ë¦¬
                if detected_word:
                    self.recent_words.append(detected_word)
                    print(f"ğŸ¯ ê²€ì¶œ: {detected_word}")
                
                # UI ê·¸ë¦¬ê¸°
                vis_frame = self.draw_ui(frame)
                
                # í™”ë©´ í‘œì‹œ
                cv2.imshow("Sign Language Recognition", vis_frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('r'):
                    self.inference.reset()
                    self.recent_words.clear()
                    print("ğŸ”„ ë¦¬ì…‹")
                elif key == ord('s'):
                    self.show_stats = not self.show_stats
                    print(f"ğŸ“Š í†µê³„ í‘œì‹œ: {self.show_stats}")
        
        except KeyboardInterrupt:
            logger.info("í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸ë¡œ ì¢…ë£Œ")
        
        finally:
            self.cleanup()
    
    def cleanup(self):
        """ì •ë¦¬"""
        self.cap.release()
        cv2.destroyAllWindows()
        
        # ìµœì¢… í†µê³„
        stats = self.inference.get_statistics()
        logger.info("ğŸ“Š ìµœì¢… í†µê³„:")
        logger.info(f"   ì´ í”„ë ˆì„: {stats['frame_count']}")
        logger.info(f"   ê²€ì¶œëœ ë‹¨ì–´: {stats['detected_words_count']}")
        logger.info(f"   í‰ê·  FPS: {stats['avg_inference_fps']:.1f}")
        
        # ê²€ì¶œëœ ë‹¨ì–´ ëª©ë¡
        if self.inference.detected_words:
            logger.info("ğŸ¯ ê²€ì¶œëœ ë‹¨ì–´ë“¤:")
            for word_info in self.inference.detected_words:
                logger.info(f"   {word_info['word']} (í”„ë ˆì„ {word_info['frame']}, ì‹ ë¢°ë„ {word_info['confidence']:.3f})")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ì‹¤ì‹œê°„ ìˆ˜í™” ì¸ì‹ ì¶”ë¡ ")
    parser.add_argument("--model", type=str, required=True,
                       help="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ")
    parser.add_argument("--vocab", type=str, default=None,
                       help="Vocabulary JSON íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--camera", type=int, default=0,
                       help="ì¹´ë©”ë¼ ID (ê¸°ë³¸: 0)")
    parser.add_argument("--device", type=str, default="cuda",
                       choices=["cuda", "cpu"], help="ì¶”ë¡  ë””ë°”ì´ìŠ¤")
    parser.add_argument("--width", type=int, default=1280,
                       help="í™”ë©´ ë„ˆë¹„")
    parser.add_argument("--height", type=int, default=720,
                       help="í™”ë©´ ë†’ì´")
    parser.add_argument("--confidence", type=float, default=0.7,
                       help="ì‹ ë¢°ë„ ì„ê³„ê°’")
    
    args = parser.parse_args()
    
    print("ğŸš€ ì‹¤ì‹œê°„ ìˆ˜í™” ì¸ì‹ ë°ëª¨")
    print("=" * 40)
    print(f"ëª¨ë¸: {args.model}")
    print(f"ì¹´ë©”ë¼: {args.camera}")
    print(f"ë””ë°”ì´ìŠ¤: {args.device}")
    print(f"í™”ë©´ í¬ê¸°: {args.width}x{args.height}")
    print("=" * 40)
    
    try:
        # ì›¹ìº  ë°ëª¨ ì‹¤í–‰
        demo = WebcamInferenceDemo(
            model_path=args.model,
            camera_id=args.camera,
            display_width=args.width,
            display_height=args.height
        )
        demo.run()
        
    except Exception as e:
        logger.error(f"âŒ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
