"""
ìˆ˜í™” ì¸ì‹ ì‹œìŠ¤í…œ í†µí•© ì„¤ì • íŒŒì¼
"""

import torch
import os
from dataclasses import dataclass
from typing import Dict, Any, Optional


@dataclass
class ModelConfig:
    """ëª¨ë¸ ì„¤ì •"""
    vocab_size: int = 442
    embed_dim: int = 384  # 256 -> 384 (í†µì¼)
    num_encoder_layers: int = 6  # 4 -> 6 (í†µì¼)
    num_decoder_layers: int = 4  # 3 -> 4 (í†µì¼)
    num_heads: int = 12  # 8 -> 12 (í†µì¼)
    dim_feedforward: int = 1536  # 1024 -> 1536 (í†µì¼)
    max_seq_len: int = 200
    dropout: float = 0.1


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì •"""
    batch_size: int = 32  # 4 -> 32 (8ë°°)
    learning_rate: float = 1e-4
    weight_decay: float = 1e-2
    num_epochs: int = 50
    warmup_steps: int = 500
    gradient_clip_val: float = 1.0
    
    # ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜
    word_loss_weight: float = 1.0
    boundary_loss_weight: float = 0.5
    confidence_loss_weight: float = 0.3
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler_patience: int = 3
    scheduler_factor: float = 0.5
    scheduler_min_lr: float = 1e-6


@dataclass
class DataConfig:
    """ë°ì´í„° ì„¤ì •"""
    annotation_path: str = "./data/sign_language_dataset_only_sen_lzf.h5"
    pose_data_dir: str = "./data"
    sequence_length: int = 150  # í†µì¼
    min_segment_length: int = 10
    max_segment_length: int = 300
    
    # ë°ì´í„° ë¶„í• 
    train_ratio: float = 0.8
    val_ratio: float = 0.2
    
    # í•„í„°ë§ ì„¤ì •
    valid_data_types: tuple = (1,)  # SEN íƒ€ì…ë§Œ
    valid_views: tuple = (0,)       # ì •ë©´(F)ë§Œ
    valid_real_ids: tuple = (1, 2, 3, 4, 5, 6, 7, 8, 10, 14, 15, 16)
    
    # ë°ì´í„° ì¦ê°• ì„¤ì •
    enable_augmentation: bool = False
    augmentation_config: Dict[str, Any] = None
    
    def __post_init__(self):
        """ê¸°ë³¸ ì¦ê°• ì„¤ì • ì´ˆê¸°í™”"""
        if self.augmentation_config is None:
            self.augmentation_config = {
                'enable_horizontal_flip': True,
                'enable_rotation': True,
                'enable_scaling': True,
                'enable_noise': True,
                'horizontal_flip_prob': 0.5,
                'rotation_range': 10.0,        # ë³´ìˆ˜ì  Â±10ë„
                'scaling_range': (0.95, 1.05), # ë³´ìˆ˜ì  Â±5%
                'noise_std': 0.005              # ì‘ì€ ë…¸ì´ì¦ˆ
            }


@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì •"""
    device: str = "auto"
    checkpoint_dir: str = "./checkpoints"
    log_dir: str = "./logs"
    save_every_n_steps: int = 999999  # ì—í¬í¬ ê¸°ë°˜ ì €ì¥ì„ ìœ„í•´ ë¹„í™œì„±í™” (í° ê°’ìœ¼ë¡œ ì„¤ì •)
    log_every_n_steps: int = 100
    num_workers: int = 4
    pin_memory: bool = True
    
    # ì‹œë“œ ì„¤ì •
    random_seed: int = 42
    
    # ì•ˆì „ ì¢…ë£Œ ì„¤ì •
    enable_signal_handler: bool = True
    graceful_shutdown_timeout: int = 30


class DeviceSpecificConfig:
    """ë””ë°”ì´ìŠ¤ë³„ ì„¤ì •"""
    
    @staticmethod
    def get_device_config(device_type: str = "auto") -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ íƒ€ì…ì— ë”°ë¥¸ ìµœì í™”ëœ ì„¤ì • ë°˜í™˜"""
        
        # ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€
        if device_type == "auto":
            device_type = DeviceSpecificConfig.detect_device()
        
        configs = {
            "cuda": {
                "model": ModelConfig(
                    embed_dim=384,  # 512 -> 384 (í†µì¼)
                    num_encoder_layers=6,  # 8 -> 6 (í†µì¼)
                    num_decoder_layers=4,  # 6 -> 4 (í†µì¼)
                    num_heads=12,  # 16 -> 12 (í†µì¼)
                    dim_feedforward=1536,  # 2048 -> 1536 (í†µì¼)
                    max_seq_len=200
                ),
                "training": TrainingConfig(
                    batch_size=128,  # 16 -> 128 (8ë°°)
                    learning_rate=2e-4,
                    num_epochs=100,
                    warmup_steps=1000
                ),
                "data": DataConfig(
                    sequence_length=150  # 200 -> 150 (í†µì¼)
                ),
                "system": SystemConfig(
                    device="cuda",
                    checkpoint_dir="./checkpoints_cuda",
                    log_dir="./logs_cuda",
                    num_workers=8
                )
            },
            "xpu": {
                "model": ModelConfig(
                    embed_dim=384,
                    num_encoder_layers=6,
                    num_decoder_layers=4,
                    num_heads=12,
                    dim_feedforward=1536,
                    max_seq_len=200
                ),
                "training": TrainingConfig(
                    batch_size=48,  # 6 -> 48 (8ë°°)
                    learning_rate=2e-4,
                    num_epochs=75
                ),
                "data": DataConfig(
                    sequence_length=200  # í†µì¼
                ),
                "system": SystemConfig(
                    device="xpu",
                    checkpoint_dir="./checkpoints_xpu",
                    log_dir="./logs_xpu",
                    num_workers=4
                )
            },
            "cpu": {
                "model": ModelConfig(
                    embed_dim=384,  # 256 -> 384 (í†µì¼)
                    num_encoder_layers=6,  # 4 -> 6 (í†µì¼)
                    num_decoder_layers=4,  # 3 -> 4 (í†µì¼)
                    num_heads=12,  # 8 -> 12 (í†µì¼)
                    dim_feedforward=1536,  # 1024 -> 1536 (í†µì¼)
                    max_seq_len=200  # 150 -> 200 (í†µì¼)
                ),
                "training": TrainingConfig(
                    batch_size=16,  # 2 -> 16 (8ë°°)
                    learning_rate=1e-4,
                    num_epochs=30
                ),
                "data": DataConfig(
                    sequence_length=200  # í†µì¼
                ),
                "system": SystemConfig(
                    device="cpu",
                    checkpoint_dir="./checkpoints_cpu",
                    log_dir="./logs_cpu",
                    num_workers=2,
                    pin_memory=False
                )
            }
        }
        
        return configs.get(device_type, configs["cpu"])
    
    @staticmethod
    def detect_device() -> str:
        """ìë™ìœ¼ë¡œ ìµœì  ë””ë°”ì´ìŠ¤ ê°ì§€"""
        # CUDA í™•ì¸
        if torch.cuda.is_available():
            return "cuda"
        
        # XPU í™•ì¸ (ì•ˆì „í•œ ê°ì§€ ë¡œì§)
        if torch.xpu.is_available():
            return "xpu"
        
        # CPUë¡œ í´ë°±
        return "cpu"
    
    @staticmethod
    def optimize_for_device(config: Dict[str, Any], device_type: str) -> Dict[str, Any]:
        """ë””ë°”ì´ìŠ¤ë³„ ì¶”ê°€ ìµœì í™”"""
        if device_type == "xpu":
            # XPU íŠ¹í™” ìµœì í™”
            config["training"].gradient_clip_val = 0.5
            config["model"].dropout = 0.15
            
        elif device_type == "cuda":
            # CUDA íŠ¹í™” ìµœì í™”
            config["system"].pin_memory = True
            config["training"].gradient_clip_val = 1.0
            
        elif device_type == "cpu":
            # CPU íŠ¹í™” ìµœì í™”
            config["system"].pin_memory = False
            config["system"].num_workers = min(2, os.cpu_count() or 1)
            config["model"].dropout = 0.05
        
        return config


def create_config(device_type: str = "auto", custom_overrides: Optional[Dict] = None) -> Dict[str, Any]:
    """í†µí•© ì„¤ì • ìƒì„±"""
    # ë””ë°”ì´ìŠ¤ë³„ ê¸°ë³¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    config = DeviceSpecificConfig.get_device_config(device_type)
    
    # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì ìš©
    actual_device = config["system"].device
    config = DeviceSpecificConfig.optimize_for_device(config, actual_device)
    
    # ì»¤ìŠ¤í…€ ì˜¤ë²„ë¼ì´ë“œ ì ìš©
    if custom_overrides:
        for section, overrides in custom_overrides.items():
            if section in config:
                for key, value in overrides.items():
                    if hasattr(config[section], key):
                        setattr(config[section], key, value)
    
    return config


def print_config(config: Dict[str, Any]):
    """ì„¤ì • ì •ë³´ ì¶œë ¥"""
    print("=" * 60)
    print("ğŸ”§ ìˆ˜í™” ì¸ì‹ ì‹œìŠ¤í…œ ì„¤ì •")
    print("=" * 60)
    
    device_type = config["system"].device
    print(f"ğŸ¯ ë””ë°”ì´ìŠ¤: {device_type.upper()}")
    print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {config['training'].batch_size}")
    print(f"ğŸ§  ëª¨ë¸ ì„ë² ë”© ì°¨ì›: {config['model'].embed_dim}")
    print(f"ğŸ”¢ ì¸ì½”ë” ë ˆì´ì–´: {config['model'].num_encoder_layers}")
    print(f"ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´: {config['data'].sequence_length}")
    print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸: {config['system'].checkpoint_dir}")
    print("=" * 60)


# í™˜ê²½ë³„ ì‚¬ì „ ì •ì˜ëœ ì„¤ì •ë“¤
CONFIGS = {
    "development": {
        "training": {"num_epochs": 10, "save_every_n_steps": 100},
        "model": {"embed_dim": 256}
    },
    "production": {
        "training": {"num_epochs": 100, "save_every_n_steps": 1000},
        "system": {"log_every_n_steps": 200}
    },
    "debug": {
        "training": {"batch_size": 1, "num_epochs": 2},
        "data": {"sequence_length": 50},
        "system": {"log_every_n_steps": 1}
    }
}


if __name__ == "__main__":
    # ì„¤ì • í…ŒìŠ¤íŠ¸
    print("ğŸ§ª ì„¤ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    # ìë™ ê°ì§€
    auto_config = create_config("auto")
    print_config(auto_config)
    
    # ê°œë°œ í™˜ê²½ ì„¤ì • í…ŒìŠ¤íŠ¸
    dev_config = create_config("auto", CONFIGS["development"])
    print(f"\nğŸ”§ ê°œë°œ í™˜ê²½ - ì—í­ ìˆ˜: {dev_config['training'].num_epochs}")
