"""
ê³ ê¸‰ ë°ì´í„° ë¶„í•  ë° ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
"""
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, Subset
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Optional
import logging

from unified_pose_dataloader import UnifiedSignLanguageDataset, collate_fn
from advanced_config import DataSplitConfig, RandomSeedConfig

logger = logging.getLogger(__name__)

class StratifiedDataSplitter:
    """ë‹¨ì–´ë³„ ê· ë“± ë¶„í• ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, config: DataSplitConfig, random_seed: int = 42):
        self.config = config
        self.random_seed = random_seed
        
    def split_dataset_stratified(self, dataset: UnifiedSignLanguageDataset) -> Tuple[List[int], List[int], List[int]]:
        """ë‹¨ì–´ë³„ë¡œ ê· ë“±í•˜ê²Œ ë°ì´í„° ë¶„í• """
        
        # ë‹¨ì–´ë³„ ì¸ë±ìŠ¤ ìˆ˜ì§‘
        word_to_indices = defaultdict(list)
        
        logger.info("ğŸ“Š ë‹¨ì–´ë³„ ë°ì´í„° ë¶„í¬ ë¶„ì„ ì¤‘...")
        for idx, segment in enumerate(dataset.valid_segments):
            for word_id in segment['vocab_ids']:
                if word_id > 0:  # íŒ¨ë”©ì´ ì•„ë‹Œ ì‹¤ì œ ë‹¨ì–´ë§Œ
                    word_to_indices[word_id].append(idx)
        
        # ë‹¨ì–´ë³„ ë¶„í¬ ì¶œë ¥
        word_counts = {word_id: len(indices) for word_id, indices in word_to_indices.items()}
        logger.info(f"ì´ {len(word_counts)}ê°œ ë‹¨ì–´ì˜ ë°ì´í„° ë¶„í¬:")
        
        # ìƒìœ„ 10ê°œ ë‹¨ì–´ ì¶œë ¥
        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        for word_id, count in top_words:
            # vocab ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì ‘ê·¼
            if hasattr(dataset, 'vocab') and dataset.vocab:
                word_name = dataset.vocab.get(word_id, f"ID_{word_id}")
            else:
                word_name = f"ID_{word_id}"
            logger.info(f"  {word_name}: {count}ê°œ")
        
        # ë¶„í•  ì‹¤í–‰
        train_indices, val_indices, test_indices = [], [], []
        insufficient_data_words = []
        
        for word_id, indices in word_to_indices.items():
            # vocab ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì•ˆì „í•˜ê²Œ ì ‘ê·¼
            if hasattr(dataset, 'vocab') and dataset.vocab:
                word_name = dataset.vocab.get(word_id, f"ID_{word_id}")
            else:
                word_name = f"ID_{word_id}"
            
            # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
            min_total = (self.config.min_samples_per_word_train + 
                        self.config.min_samples_per_word_val + 
                        self.config.min_samples_per_word_test)
            
            if len(indices) < min_total:
                insufficient_data_words.append((word_name, len(indices)))
                # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ë‹¨ì–´ëŠ” í›ˆë ¨ ì„¸íŠ¸ì—ë§Œ í¬í•¨
                train_indices.extend(indices)
                continue
            
            # Stratified split
            np.random.seed(self.random_seed)
            shuffled_indices = np.array(indices)
            np.random.shuffle(shuffled_indices)
            
            # ë¹„ìœ¨ì— ë”°ë¥¸ ë¶„í• 
            n_samples = len(shuffled_indices)
            n_train = max(int(n_samples * self.config.train_ratio), 
                         self.config.min_samples_per_word_train)
            n_val = max(int(n_samples * self.config.val_ratio), 
                       self.config.min_samples_per_word_val)
            n_test = max(int(n_samples * self.config.test_ratio), 
                        self.config.min_samples_per_word_test)
            
            # ì˜¤ë²„í”Œë¡œìš° ë°©ì§€
            if n_train + n_val + n_test > n_samples:
                n_train = n_samples - n_val - n_test
                if n_train < self.config.min_samples_per_word_train:
                    n_val = max(1, n_samples - n_train - n_test)
                    n_test = max(1, n_samples - n_train - n_val)
            
            # ì‹¤ì œ ë¶„í• 
            train_indices.extend(shuffled_indices[:n_train].tolist())
            val_indices.extend(shuffled_indices[n_train:n_train+n_val].tolist())
            test_indices.extend(shuffled_indices[n_train+n_val:n_train+n_val+n_test].tolist())
        
        # ê²½ê³  ì¶œë ¥
        if insufficient_data_words:
            logger.warning(f"âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•œ {len(insufficient_data_words)}ê°œ ë‹¨ì–´:")
            for word_name, count in insufficient_data_words[:5]:
                logger.warning(f"  {word_name}: {count}ê°œ (ìµœì†Œ {min_total}ê°œ í•„ìš”)")
        
        # ìµœì¢… ë¶„í•  ê²°ê³¼
        logger.info("âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        logger.info(f"  í›ˆë ¨: {len(train_indices)}ê°œ ({len(train_indices)/len(dataset)*100:.1f}%)")
        logger.info(f"  ê²€ì¦: {len(val_indices)}ê°œ ({len(val_indices)/len(dataset)*100:.1f}%)")
        logger.info(f"  í…ŒìŠ¤íŠ¸: {len(test_indices)}ê°œ ({len(test_indices)/len(dataset)*100:.1f}%)")
        
        return train_indices, val_indices, test_indices

    def create_dataloaders(self, 
                          dataset: UnifiedSignLanguageDataset,
                          batch_size: int = 32,
                          enable_train_augmentation: bool = False,
                          augmentation_config: Optional[Dict] = None) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """ë°ì´í„°ë¡œë” ìƒì„±"""
        
        # ë°ì´í„° ë¶„í• 
        train_indices, val_indices, test_indices = self.split_dataset_stratified(dataset)
        
        # ì¦ê°• ì ìš© ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ ë°ì´í„°ì…‹ ìƒì„±
        if enable_train_augmentation:
            # í›ˆë ¨ìš©: ì¦ê°• í™œì„±í™”
            train_dataset_aug = UnifiedSignLanguageDataset(
                annotation_path=dataset.annotation_path,
                pose_data_dir=dataset.pose_data_dir,
                sequence_length=dataset.sequence_length,
                min_segment_length=dataset.min_segment_length,
                max_segment_length=dataset.max_segment_length,
                enable_augmentation=True,
                augmentation_config=augmentation_config
            )
            train_dataset = Subset(train_dataset_aug, train_indices)
        else:
            train_dataset = Subset(dataset, train_indices)
        
        # ê²€ì¦/í…ŒìŠ¤íŠ¸ìš©: ì¦ê°• ë¹„í™œì„±í™”
        val_dataset = Subset(dataset, val_indices)
        test_dataset = Subset(dataset, test_indices)
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True,
            collate_fn=collate_fn,
            persistent_workers=True
        )
        
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True,
            collate_fn=collate_fn,
            persistent_workers=True
        )
        
        test_dataloader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True,
            collate_fn=collate_fn,
            persistent_workers=True
        )
        
        return train_dataloader, val_dataloader, test_dataloader

class EarlyStopping:
    """ê°œì„ ëœ ì–¼ë¦¬ìŠ¤íƒ‘ í´ë˜ìŠ¤"""
    
    def __init__(self, config):
        self.patience = config.patience
        self.min_delta = config.min_delta
        self.monitor = config.monitor
        self.mode = config.mode
        self.restore_best_weights = config.restore_best_weights
        
        self.best_score = float('inf') if self.mode == 'min' else float('-inf')
        self.best_epoch = 0
        self.wait = 0
        self.stopped_epoch = 0
        self.best_weights = None
        
    def __call__(self, current_score: float, epoch: int, model_state_dict: Dict) -> bool:
        """ì–¼ë¦¬ìŠ¤íƒ‘ ì²´í¬"""
        
        if self.mode == 'min':
            improved = current_score < (self.best_score - self.min_delta)
        else:
            improved = current_score > (self.best_score + self.min_delta)
        
        if improved:
            self.best_score = current_score
            self.best_epoch = epoch
            self.wait = 0
            if self.restore_best_weights:
                self.best_weights = {k: v.clone() for k, v in model_state_dict.items()}
            logger.info(f"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! {self.monitor}: {current_score:.6f}")
        else:
            self.wait += 1
            logger.info(f"â³ ê°œì„  ì—†ìŒ ({self.wait}/{self.patience}) - í˜„ì¬: {current_score:.6f}, ìµœê³ : {self.best_score:.6f}")
        
        if self.wait >= self.patience:
            self.stopped_epoch = epoch
            logger.info(f"â¹ï¸ ì–¼ë¦¬ìŠ¤íƒ‘ ë°œë™! (ì—í¬í¬ {epoch})")
            return True
        
        return False
    
    def get_best_weights(self):
        """ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜ ë°˜í™˜"""
        return self.best_weights