"""
ê³ ê¸‰ í•™ìŠµ ì„¤ì • - ë‹¤ë‹¨ê³„ í•™ìŠµì„ ìœ„í•œ ì„¤ì •
"""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
import random
import numpy as np
import torch
import os

@dataclass
class RandomSeedConfig:
    """ëœë¤ì‹œë“œ ê³ ì • ì„¤ì •"""
    seed: int = 42
    
    def fix_all_seeds(self):
        """ëª¨ë“  ëœë¤ì‹œë“œ ê³ ì •"""
        # Python random
        random.seed(self.seed)
        
        # NumPy random
        np.random.seed(self.seed)
        
        # PyTorch random
        torch.manual_seed(self.seed)
        torch.cuda.manual_seed(self.seed)
        torch.cuda.manual_seed_all(self.seed)
        
        # XPU random (if available)
        try:
            torch.xpu.manual_seed(self.seed)
            torch.xpu.manual_seed_all(self.seed)
        except:
            pass
        
        # í™•ì •ì  ë™ì‘ ì„¤ì •
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        # í•´ì‹œ ì‹œë“œ ê³ ì •
        import os
        os.environ['PYTHONHASHSEED'] = str(self.seed)

@dataclass
class DataSplitConfig:
    """ê°œì„ ëœ ë°ì´í„° ë¶„í•  ì„¤ì •"""
    train_ratio: float = 0.8
    val_ratio: float = 0.15  
    test_ratio: float = 0.05
    
    # Stratified split (ë‹¨ì–´ë³„ ê· ë“± ë¶„í• )
    stratified_split: bool = True
    
    # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ë³´ì¥
    min_samples_per_word_train: int = 10
    min_samples_per_word_val: int = 3
    min_samples_per_word_test: int = 2
    
    def __post_init__(self):
        assert abs(self.train_ratio + self.val_ratio + self.test_ratio - 1.0) < 1e-6

@dataclass  
class EarlyStoppingConfig:
    """ì–¼ë¦¬ìŠ¤íƒ‘ ì„¤ì •"""
    patience: int = 15
    min_delta: float = 1e-4
    monitor: str = "val_loss"  # val_loss, val_accuracy, val_word_accuracy
    mode: str = "min"  # min, max
    restore_best_weights: bool = True
    
@dataclass
class TrainingStageConfig:
    """í•™ìŠµ ë‹¨ê³„ë³„ ì„¤ì •"""
    name: str
    description: str
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    num_epochs: int = 50
    batch_size: int = 32
    learning_rate: float = 1e-4
    weight_decay: float = 1e-2
    
    # ì¦ê°• ì„¤ì •
    enable_augmentation: bool = False
    augmentation_strength: float = 1.0  # ì¦ê°• ê°•ë„ ì¡°ì ˆ
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
    use_warmup: bool = True
    warmup_steps: int = 500
    scheduler_type: str = "reduce_on_plateau"  # reduce_on_plateau, cosine, linear
    
    # ì •ê·œí™” ê¸°ë²•
    dropout_rate: float = 0.1
    label_smoothing: float = 0.0
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜ ìˆ˜ì •
    freeze_encoder: bool = False
    fine_tune_layers: Optional[List[str]] = None

@dataclass
class MultiStageTrainingConfig:
    """ë‹¤ë‹¨ê³„ í•™ìŠµ ì„¤ì •"""
    stages: List[TrainingStageConfig] = field(default_factory=list)
    
    # ë‹¨ê³„ ê°„ ëª¨ë¸ ì „ì´
    transfer_best_weights: bool = True
    transfer_optimizer_state: bool = False
    
    # ì„±ëŠ¥ í–¥ìƒ ì„ê³„ê°’
    improvement_threshold: float = 0.01  # 1% ì´ìƒ í–¥ìƒ ì‹œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
    
    # ìµœëŒ€ ë‹¨ê³„ ìˆ˜
    max_stages: int = 5
    
    def __post_init__(self):
        if not self.stages:
            self.stages = self._create_default_stages()
    
    def _create_default_stages(self) -> List[TrainingStageConfig]:
        """ê¸°ë³¸ í•™ìŠµ ë‹¨ê³„ ìƒì„±"""
        return [
            # Stage 1: ê¸°ë³¸ í•™ìŠµ
            TrainingStageConfig(
                name="baseline",
                description="ê¸°ë³¸ í•™ìŠµ (ì¦ê°• ì—†ìŒ)",
                num_epochs=30,
                batch_size=48,
                learning_rate=1e-4,
                enable_augmentation=False,
                dropout_rate=0.1
            ),
            
            # Stage 2: ë°ì´í„° ì¦ê°• ì ìš©
            TrainingStageConfig(
                name="augmentation",
                description="ë°ì´í„° ì¦ê°• ì ìš©",
                num_epochs=25,
                batch_size=32,
                learning_rate=5e-5,
                enable_augmentation=True,
                augmentation_strength=1.0,
                dropout_rate=0.15
            ),
            
            # Stage 3: ê°•í•œ ì •ê·œí™”
            TrainingStageConfig(
                name="regularization",
                description="ê°•í•œ ì •ê·œí™” (Label Smoothing + Dropout)",
                num_epochs=20,
                batch_size=32,
                learning_rate=2e-5,
                enable_augmentation=True,
                augmentation_strength=0.8,
                dropout_rate=0.2,
                label_smoothing=0.1
            ),
            
            # Stage 4: ë¯¸ì„¸ ì¡°ì •
            TrainingStageConfig(
                name="fine_tuning",
                description="ë¯¸ì„¸ ì¡°ì • (ë‚®ì€ í•™ìŠµë¥ )",
                num_epochs=15,
                batch_size=24,
                learning_rate=1e-5,
                enable_augmentation=True,
                augmentation_strength=0.5,
                dropout_rate=0.1,
                freeze_encoder=False
            ),
            
            # Stage 5: ìµœì¢… í´ë¦¬ì‹±
            TrainingStageConfig(
                name="polishing",
                description="ìµœì¢… í´ë¦¬ì‹± (ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ )",
                num_epochs=10,
                batch_size=16,
                learning_rate=5e-6,
                enable_augmentation=True,
                augmentation_strength=0.3,
                dropout_rate=0.05
            )
        ]

@dataclass
class AdvancedTrainingConfig:
    """ê³ ê¸‰ í•™ìŠµ ì„¤ì • í†µí•©"""
    # ê¸°ë³¸ ì„¤ì •ë“¤
    random_seed: RandomSeedConfig = field(default_factory=RandomSeedConfig)
    data_split: DataSplitConfig = field(default_factory=DataSplitConfig)
    early_stopping: EarlyStoppingConfig = field(default_factory=EarlyStoppingConfig)
    multi_stage: MultiStageTrainingConfig = field(default_factory=MultiStageTrainingConfig)
    
    # ë°ì´í„° ê²½ë¡œ
    annotation_path: str = "./data/sign_language_dataset_only_sen_lzf.h5"
    pose_data_dir: str = "./data"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    checkpoint_dir: str = "./advanced_checkpoints"
    log_dir: str = "./advanced_logs"
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device: str = "auto"
    
    # ì‹¤í—˜ ê´€ë¦¬
    experiment_name: str = "multi_stage_training"
    save_intermediate_results: bool = True
    
    # í‰ê°€ ì„¤ì •
    evaluate_on_test: bool = True
    test_every_n_stages: int = 1

if __name__ == "__main__":
    # ì„¤ì • í…ŒìŠ¤íŠ¸
    config = AdvancedTrainingConfig()
    print("ğŸ§ª ê³ ê¸‰ í•™ìŠµ ì„¤ì • í…ŒìŠ¤íŠ¸")
    print(f"ì´ í•™ìŠµ ë‹¨ê³„: {len(config.multi_stage.stages)}")
    
    for i, stage in enumerate(config.multi_stage.stages):
        print(f"Stage {i+1}: {stage.name} - {stage.description}")