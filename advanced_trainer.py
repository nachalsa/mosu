"""
ê³ ê¸‰ ë‹¤ë‹¨ê³„ í•™ìŠµ íŠ¸ë ˆì´ë„ˆ
"""
import os
import time
import json
import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from tqdm import tqdm
import numpy as np

from sign_language_model import SequenceToSequenceSignModel
from sign_language_trainer import SignLanguageTrainer
from advanced_config import AdvancedTrainingConfig, TrainingStageConfig
from advanced_data_utils import StratifiedDataSplitter, EarlyStopping
from unified_pose_dataloader import UnifiedSignLanguageDataset
from device_utils import DeviceManager

logger = logging.getLogger(__name__)

class AdvancedSignLanguageTrainer:
    """ê³ ê¸‰ ë‹¤ë‹¨ê³„ í•™ìŠµ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, config: AdvancedTrainingConfig):
        self.config = config
        
        # ëœë¤ì‹œë“œ ê³ ì •
        self.config.random_seed.fix_all_seeds()
        logger.info(f"ğŸ² ëœë¤ì‹œë“œ ê³ ì •: {self.config.random_seed.seed}")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.checkpoint_dir = Path(self.config.checkpoint_dir)
        self.log_dir = Path(self.config.log_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.log_dir.mkdir(exist_ok=True)
        
        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
        self.experiment_results = {
            'config': config.__dict__,
            'stages': [],
            'final_performance': {}
        }
        
        # í˜„ì¬ ìµœê³  ì„±ëŠ¥
        self.best_overall_performance = {
            'stage': None,
            'epoch': None,
            'val_loss': float('inf'),
            'val_accuracy': 0.0,
            'test_performance': {}
        }
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì • (ê°œì„ ëœ ë°©ì‹)
        self.device = DeviceManager.detect_best_device(self.config.device, self.config.multi_gpu)
        device_info = DeviceManager.get_device_info(self.device)
        logger.info(f"ğŸš€ ë””ë°”ì´ìŠ¤ ì„¤ì • ì™„ë£Œ: {device_info['name']}")
        
        # ë©€í‹° GPU ì •ë³´
        self.multi_gpu_available = DeviceManager.is_multi_gpu_available()
        if self.config.multi_gpu and self.multi_gpu_available:
            logger.info(f"ğŸš€ ë©€í‹° GPU ëª¨ë“œ í™œì„±í™”: {torch.cuda.device_count()}ê°œ GPU")
        elif self.config.multi_gpu and not self.multi_gpu_available:
            logger.warning("âš ï¸ ë©€í‹° GPU ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ ì‚¬ìš© ë¶ˆê°€ - ë‹¨ì¼ GPU/CPU ì‚¬ìš©")
            self.config.multi_gpu = False
        
        # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™”
        DeviceManager.optimize_for_device(self.device, self.config.multi_gpu)
        
        # Vocabulary ì •ë³´ ì €ì¥ (ë‚˜ì¤‘ì— ì‚¬ìš©)
        self.vocab_words = None
        self.word_to_id = None
    
    def load_and_prepare_data(self) -> Tuple[UnifiedSignLanguageDataset, StratifiedDataSplitter]:
        """ë°ì´í„° ë¡œë“œ ë° ë¶„í•  ì¤€ë¹„"""
        logger.info("ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ë¶„í•  ì¤€ë¹„...")
        
        # ê¸°ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ
        base_dataset = UnifiedSignLanguageDataset(
            annotation_path=self.config.annotation_path,
            pose_data_dir=self.config.pose_data_dir,
            sequence_length=200,
            min_segment_length=20,
            max_segment_length=300,
            enable_augmentation=False  # ê¸°ë³¸ ë°ì´í„°ì…‹ì€ ì¦ê°• ì—†ìŒ
        )
        
        if len(base_dataset) == 0:
            raise ValueError("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        logger.info(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(base_dataset)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        logger.info(f"   ì–´íœ˜ í¬ê¸°: {base_dataset.vocab_size}")
        
        # Vocabulary ì •ë³´ ì €ì¥
        self.vocab_words = getattr(base_dataset, 'words', [])
        self.word_to_id = getattr(base_dataset, 'word_to_id', {})
        if not self.vocab_words and hasattr(base_dataset, 'dataset') and hasattr(base_dataset.dataset, 'words'):
            self.vocab_words = base_dataset.dataset.words
            self.word_to_id = getattr(base_dataset.dataset, 'word_to_id', {})
        
        logger.info(f"   Vocabulary ë‹¨ì–´ ìˆ˜: {len(self.vocab_words)}")
        
        # ë°ì´í„° ë¶„í• ê¸° ìƒì„±
        splitter = StratifiedDataSplitter(
            config=self.config.data_split,
            random_seed=self.config.random_seed.seed
        )
        
        return base_dataset, splitter
    
    def create_model(self, vocab_size: int, stage_config: TrainingStageConfig) -> SequenceToSequenceSignModel:
        """ë‹¨ê³„ë³„ ëª¨ë¸ ìƒì„±"""
        model = SequenceToSequenceSignModel(
            vocab_size=vocab_size,
            embed_dim=384,
            num_encoder_layers=6,
            num_decoder_layers=4,
            num_heads=8,
            dim_feedforward=1024,
            max_seq_len=200,
            dropout=stage_config.dropout_rate
        )
        
        # ë©€í‹° GPU ì„¤ì •
        if self.config.multi_gpu and self.multi_gpu_available:
            model = DeviceManager.setup_multi_gpu(
                model, self.device, self.config.use_data_parallel
            )
        else:
            model = model.to(self.device)
            
        return model
    
    def setup_stage_training(self, 
                           model: SequenceToSequenceSignModel,
                           stage_config: TrainingStageConfig,
                           train_dataloader,
                           val_dataloader) -> Tuple[SignLanguageTrainer, EarlyStopping]:
        """ë‹¨ê³„ë³„ í•™ìŠµ ì„¤ì •"""
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = SignLanguageTrainer(
            model=model,
            train_dataloader=train_dataloader,
            val_dataloader=val_dataloader,
            device=str(self.device),  # ê°œì„ ëœ ë””ë°”ì´ìŠ¤ ì‚¬ìš©
            checkpoint_dir=str(self.checkpoint_dir / stage_config.name),
            log_dir=str(self.log_dir / stage_config.name)
        )
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
        trainer.setup_training(
            learning_rate=stage_config.learning_rate,
            weight_decay=stage_config.weight_decay,
            gradient_clip_val=1.0,
            word_loss_weight=1.0,
            boundary_loss_weight=0.5,
            confidence_loss_weight=0.3
        )
        
        # ì–¼ë¦¬ìŠ¤íƒ‘ ì„¤ì •
        early_stopping = EarlyStopping(self.config.early_stopping)
        
        return trainer, early_stopping
    
    def train_single_stage(self,
                          stage_idx: int,
                          stage_config: TrainingStageConfig,
                          model: SequenceToSequenceSignModel,
                          base_dataset: UnifiedSignLanguageDataset,
                          splitter: StratifiedDataSplitter) -> Dict:
        """ë‹¨ì¼ ë‹¨ê³„ í•™ìŠµ"""
        
        logger.info("="*80)
        logger.info(f"ğŸš€ Stage {stage_idx+1}: {stage_config.name}")
        logger.info(f"ğŸ“ ì„¤ëª…: {stage_config.description}")
        logger.info(f"âš™ï¸ ì„¤ì •: LR={stage_config.learning_rate}, BS={stage_config.batch_size}, "
                   f"Aug={'âœ…' if stage_config.enable_augmentation else 'âŒ'}")
        logger.info("="*80)
        
        # ì¦ê°• ì„¤ì •
        augmentation_config = None
        if stage_config.enable_augmentation:
            augmentation_config = {
                'enable_horizontal_flip': True,
                'horizontal_flip_prob': 0.5 * stage_config.augmentation_strength,
                'enable_rotation': True,
                'rotation_range': 15.0 * stage_config.augmentation_strength,
                'enable_scaling': True,
                'scaling_range': (1.0 - 0.1 * stage_config.augmentation_strength, 
                                 1.0 + 0.1 * stage_config.augmentation_strength),
                'enable_noise': True,
                'noise_std': 0.005 * stage_config.augmentation_strength
            }
        
        # ë°ì´í„°ë¡œë” ìƒì„± (ë©€í‹° GPUì— ë§ëŠ” ë°°ì¹˜ í¬ê¸° ì¡°ì •)
        effective_batch_size = stage_config.batch_size
        if self.config.multi_gpu and self.config.auto_adjust_batch_size:
            effective_batch_size = DeviceManager.get_effective_batch_size(
                stage_config.batch_size, self.device
            )
        
        train_dataloader, val_dataloader, test_dataloader = splitter.create_dataloaders(
            dataset=base_dataset,
            batch_size=effective_batch_size,
            enable_train_augmentation=stage_config.enable_augmentation,
            augmentation_config=augmentation_config
        )
        
        # í•™ìŠµ ì„¤ì •
        trainer, early_stopping = self.setup_stage_training(
            model, stage_config, train_dataloader, val_dataloader
        )
        
        # í•™ìŠµ ì‹¤í–‰
        stage_start_time = time.time()
        best_val_loss = float('inf')
        best_val_accuracy = 0.0
        best_epoch = -1
        
        stage_results = {
            'stage_name': stage_config.name,
            'description': stage_config.description,
            'config': stage_config.__dict__,
            'epochs_trained': 0,
            'best_epoch': -1,
            'best_val_loss': float('inf'),
            'best_val_accuracy': 0.0,
            'training_time': 0.0,
            'early_stopped': False,
            'improvement_from_previous': 0.0
        }
        
        try:
            for epoch in range(stage_config.num_epochs):
                epoch_start_time = time.time()
                
                # í›ˆë ¨ (ì—í¬í¬ ë‹¨ìœ„ ì €ì¥ìœ¼ë¡œ ê°œì„ )
                train_loss, train_metrics = trainer.train_epoch(
                    epoch=epoch,
                    log_every_n_steps=100
                )
                
                # ê²€ì¦
                val_loss, val_metrics = trainer.validate_epoch(epoch)
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                trainer.scheduler.step(val_loss)
                
                # ì„±ëŠ¥ ê¸°ë¡
                val_accuracy = val_metrics.get('word_accuracy', 0.0)
                
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_val_accuracy = val_accuracy
                    best_epoch = epoch
                
                # ë¡œê¹…
                epoch_time = time.time() - epoch_start_time
                logger.info(f"Epoch {epoch:3d}/{stage_config.num_epochs-1} ({epoch_time:.1f}s)")
                logger.info(f"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
                logger.info(f"  Train Acc: {train_metrics.get('word_accuracy', 0):.3f} | "
                           f"Val Acc: {val_accuracy:.3f}")
                
                # ì–¼ë¦¬ìŠ¤íƒ‘ ì²´í¬
                if early_stopping(val_loss, epoch, model.state_dict()):
                    logger.info(f"â¹ï¸ ì–¼ë¦¬ìŠ¤íƒ‘ ë°œë™ - Stage {stage_idx+1} ì¢…ë£Œ")
                    stage_results['early_stopped'] = True
                    
                    # ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜ ë³µì›
                    if early_stopping.best_weights:
                        model.load_state_dict(early_stopping.best_weights)
                        logger.info("âœ… ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜ ë³µì› ì™„ë£Œ")
                    break
                
                stage_results['epochs_trained'] = epoch + 1
            
            # ë‹¨ê³„ ê²°ê³¼ ì—…ë°ì´íŠ¸
            stage_results['best_epoch'] = best_epoch
            stage_results['best_val_loss'] = best_val_loss
            stage_results['best_val_accuracy'] = best_val_accuracy
            stage_results['training_time'] = time.time() - stage_start_time
            
            # ì „ì²´ ìµœê³  ì„±ëŠ¥ ëŒ€ë¹„ ê°œì„ ë„ ê³„ì‚°
            if self.best_overall_performance['val_loss'] < float('inf'):
                improvement = (self.best_overall_performance['val_loss'] - best_val_loss) / self.best_overall_performance['val_loss']
                stage_results['improvement_from_previous'] = improvement
            
            # ì „ì²´ ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
            if best_val_loss < self.best_overall_performance['val_loss']:
                self.best_overall_performance.update({
                    'stage': stage_config.name,
                    'epoch': best_epoch,
                    'val_loss': best_val_loss,
                    'val_accuracy': best_val_accuracy
                })
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                best_model_path = self.checkpoint_dir / f"best_model_stage_{stage_idx+1}.pt"
                torch.save({
                    'stage': stage_config.name,
                    'epoch': best_epoch,
                    'model_state_dict': model.state_dict(),
                    'val_loss': best_val_loss,
                    'val_accuracy': best_val_accuracy,
                    'config': stage_config.__dict__,
                    'vocab_words': self.vocab_words,
                    'word_to_id': self.word_to_id,
                    'vocab_size': len(self.vocab_words) if self.vocab_words else 0,
                    'model_config': {
                        'vocab_size': len(self.vocab_words) if self.vocab_words else 0,
                        'embed_dim': 384,
                        'num_encoder_layers': 6,
                        'num_decoder_layers': 4,
                        'num_heads': 8,
                        'dim_feedforward': 1024,
                        'max_seq_len': 200,
                        'dropout': stage_config.dropout_rate
                    }
                }, best_model_path)
                logger.info(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_model_path}")
                if self.vocab_words:
                    logger.info(f"   ğŸ“š Vocabulary í¬í•¨: {len(self.vocab_words)}ê°œ ë‹¨ì–´")
            
            # í…ŒìŠ¤íŠ¸ í‰ê°€ (ì„ íƒì )
            if self.config.evaluate_on_test and (stage_idx + 1) % self.config.test_every_n_stages == 0:
                logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ì¤‘...")
                test_loss, test_metrics = self._evaluate_on_test(model, test_dataloader, trainer)
                stage_results['test_performance'] = {
                    'test_loss': test_loss,
                    'test_accuracy': test_metrics.get('word_accuracy', 0.0)
                }
                logger.info(f"  í…ŒìŠ¤íŠ¸ Loss: {test_loss:.4f}, í…ŒìŠ¤íŠ¸ Acc: {test_metrics.get('word_accuracy', 0.0):.3f}")
            
            logger.info(f"âœ… Stage {stage_idx+1} ì™„ë£Œ!")
            logger.info(f"  ìµœê³  ì„±ëŠ¥: Val Loss {best_val_loss:.4f}, Val Acc {best_val_accuracy:.3f} (Epoch {best_epoch})")
            logger.info(f"  í•™ìŠµ ì‹œê°„: {stage_results['training_time']:.1f}ì´ˆ")
            
        except Exception as e:
            logger.error(f"âŒ Stage {stage_idx+1} í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
            stage_results['error'] = str(e)
            raise
        
        return stage_results
    
    @torch.no_grad()
    def _evaluate_on_test(self, model, test_dataloader, trainer) -> Tuple[float, Dict]:
        """í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€"""
        model.eval()
        total_loss = 0
        total_metrics = {'word_accuracy': 0, 'boundary_accuracy': 0}
        
        for batch_idx, batch in enumerate(tqdm(test_dataloader, desc="í…ŒìŠ¤íŠ¸ í‰ê°€")):
            # GPUë¡œ ì´ë™
            pose_features = batch['pose_features'].to(trainer.device)
            vocab_ids = batch['vocab_ids'].to(trainer.device)
            frame_masks = batch['frame_masks'].to(trainer.device)
            vocab_masks = batch['vocab_masks'].to(trainer.device)
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ ë°°ì¹˜ ì •ë³´ ë¡œê¹… (ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ)
            if batch_idx == 0:
                logger.debug(f"ğŸ” Test batch debug info:")
                logger.debug(f"  pose_features shape: {pose_features.shape}")
                logger.debug(f"  vocab_ids shape: {vocab_ids.shape}")
                logger.debug(f"  frame_masks shape: {frame_masks.shape}")
                logger.debug(f"  vocab_masks shape: {vocab_masks.shape}")
            
            try:
                # ì¶”ë¡  - í…ŒìŠ¤íŠ¸ì—ì„œë„ Teacher Forcing ëª¨ë“œ ì‚¬ìš© (ì°¨ì› ì¼ì¹˜ë¥¼ ìœ„í•´)
                outputs = model(
                    pose_features=pose_features,
                    vocab_ids=vocab_ids,
                    frame_masks=frame_masks,
                    vocab_masks=vocab_masks,
                    force_training_mode=True  # ì°¨ì› ì¼ì¹˜ë¥¼ ìœ„í•´ ê°•ì œë¡œ teacher forcing ëª¨ë“œ
                )
                
                # ì¶œë ¥ ì°¨ì› ë¡œê¹… (ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ)
                if batch_idx == 0:
                    logger.debug(f"ğŸ” Model outputs debug info:")
                    logger.debug(f"  word_logits shape: {outputs['word_logits'].shape}")
                    logger.debug(f"  boundary_logits shape: {outputs['boundary_logits'].shape}")
            
            except Exception as e:
                logger.error(f"âŒ Model forward pass error at batch {batch_idx}: {e}")
                logger.error(f"  pose_features shape: {pose_features.shape}")
                logger.error(f"  vocab_ids shape: {vocab_ids.shape}")
                raise
            
            try:
                # ì†ì‹¤ ê³„ì‚°
                boundary_labels = trainer.create_boundary_labels(vocab_ids, vocab_masks)
                targets = {'vocab_ids': vocab_ids, 'boundary_labels': boundary_labels}
                losses = model.compute_loss(outputs, targets, vocab_masks)
                
            except Exception as e:
                logger.error(f"âŒ Loss computation error at batch {batch_idx}: {e}")
                logger.error(f"  vocab_ids shape: {vocab_ids.shape}")
                logger.error(f"  outputs word_logits shape: {outputs['word_logits'].shape}")
                logger.error(f"  boundary_labels shape: {boundary_labels.shape if boundary_labels is not None else 'None'}")
                raise
            
            total_loss += losses['total_loss'].item()
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = trainer.compute_metrics(outputs, targets, {'vocab_masks': vocab_masks})
            for key, value in metrics.items():
                if key in total_metrics:
                    total_metrics[key] += value
        
        # í‰ê·  ê³„ì‚°
        avg_loss = total_loss / len(test_dataloader)
        avg_metrics = {key: value / len(test_dataloader) for key, value in total_metrics.items()}
        
        return avg_loss, avg_metrics
    
    def should_continue_training(self, stage_results: List[Dict]) -> bool:
        """ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ì—¬ë¶€ ê²°ì •"""
        if len(stage_results) < 2:
            return True
        
        latest_result = stage_results[-1]
        
        # ì—ëŸ¬ê°€ ë°œìƒí–ˆìœ¼ë©´ ì¤‘ë‹¨
        if 'error' in latest_result:
            return False
        
        # ê°œì„ ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ì¤‘ë‹¨
        improvement = latest_result.get('improvement_from_previous', 0)
        if improvement < self.config.multi_stage.improvement_threshold:
            logger.info(f"â¹ï¸ ê°œì„ ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ìŒ ({improvement:.4f} < {self.config.multi_stage.improvement_threshold:.4f})")
            return False
        
        return True
    
    def train_multi_stage(self) -> Dict:
        """ë‹¤ë‹¨ê³„ í•™ìŠµ ì‹¤í–‰"""
        logger.info("ğŸš€ ê³ ê¸‰ ë‹¤ë‹¨ê³„ í•™ìŠµ ì‹œì‘!")
        logger.info(f"ì‹¤í—˜ëª…: {self.config.experiment_name}")
        
        # ë°ì´í„° ì¤€ë¹„
        base_dataset, splitter = self.load_and_prepare_data()
        
        # ëª¨ë¸ ìƒì„± (ì²« ë²ˆì§¸ ë‹¨ê³„ ì„¤ì •ìœ¼ë¡œ)
        first_stage = self.config.multi_stage.stages[0]
        model = self.create_model(base_dataset.vocab_size, first_stage)
        
        # ë©€í‹° GPU ì„¤ì •
        if self.config.multi_gpu and DeviceManager.is_multi_gpu_available():
            if self.config.use_data_parallel and torch.cuda.device_count() > 1:
                DeviceManager.setup_multi_gpu(model)
                print(f"ğŸš€ DataParallel í™œì„±í™”: {torch.cuda.device_count()}ê°œ GPU ì‚¬ìš©")
            else:
                print("âš ï¸ ë©€í‹° GPU ìš”ì²­ë˜ì—ˆì§€ë§Œ DataParallel ë¹„í™œì„±í™”ë¨")
        
        model.to(self.device)
        
        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"ğŸ“Š ëª¨ë¸ ì •ë³´: {total_params:,} íŒŒë¼ë¯¸í„°")
        
        # ê° ë‹¨ê³„ë³„ í•™ìŠµ
        all_stage_results = []
        
        for stage_idx, stage_config in enumerate(self.config.multi_stage.stages):
            # ìµœëŒ€ ë‹¨ê³„ ìˆ˜ í™•ì¸
            if stage_idx >= self.config.multi_stage.max_stages:
                logger.info(f"â¹ï¸ ìµœëŒ€ ë‹¨ê³„ ìˆ˜ ({self.config.multi_stage.max_stages}) ë„ë‹¬")
                break
            
            # ì´ì „ ë‹¨ê³„ì—ì„œ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¤‘ë‹¨
            if stage_idx > 0 and not self.should_continue_training(all_stage_results):
                logger.info("â¹ï¸ ê°œì„ ì´ ì—†ì–´ ë‹¤ë‹¨ê³„ í•™ìŠµ ì¤‘ë‹¨")
                break
            
            # ëª¨ë¸ ë™ì  ìˆ˜ì • (í•„ìš”í•œ ê²½ìš°)
            if stage_config.dropout_rate != 0.1:  # ê¸°ë³¸ê°’ê³¼ ë‹¤ë¥´ë©´
                self._update_model_dropout(model, stage_config.dropout_rate)
            
            # ë‹¨ê³„ë³„ í•™ìŠµ ì‹¤í–‰
            stage_results = self.train_single_stage(
                stage_idx, stage_config, model, base_dataset, splitter
            )
            
            all_stage_results.append(stage_results)
            self.experiment_results['stages'] = all_stage_results
            
            # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
            if self.config.save_intermediate_results:
                self._save_experiment_results()
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        if self.config.evaluate_on_test:
            logger.info("ğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€ ìˆ˜í–‰...")
            _, _, test_dataloader = splitter.create_dataloaders(
                base_dataset, batch_size=32, enable_train_augmentation=False
            )
            
            # ë”ë¯¸ íŠ¸ë ˆì´ë„ˆ ìƒì„± (í‰ê°€ìš©)
            dummy_trainer = SignLanguageTrainer(model, None, None, device=str(self.device))
            
            final_test_loss, final_test_metrics = self._evaluate_on_test(
                model, test_dataloader, dummy_trainer
            )
            
            self.experiment_results['final_performance'] = {
                'test_loss': final_test_loss,
                'test_accuracy': final_test_metrics.get('word_accuracy', 0.0),
                'test_metrics': final_test_metrics
            }
            
            logger.info(f"ğŸ† ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
            logger.info(f"  í…ŒìŠ¤íŠ¸ Loss: {final_test_loss:.4f}")
            logger.info(f"  í…ŒìŠ¤íŠ¸ Accuracy: {final_test_metrics.get('word_accuracy', 0.0):.3f}")
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        self._save_experiment_results()
        
        # ê²°ê³¼ ìš”ì•½
        self._print_final_summary(all_stage_results)
        
        return self.experiment_results
    
    def _update_model_dropout(self, model: nn.Module, new_dropout_rate: float):
        """ëª¨ë¸ì˜ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ë™ì  ë³€ê²½"""
        for module in model.modules():
            if isinstance(module, nn.Dropout):
                module.p = new_dropout_rate
        logger.info(f"ğŸ”§ ëª¨ë¸ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ë³€ê²½: {new_dropout_rate}")
    
    def _save_experiment_results(self):
        """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
        results_path = self.checkpoint_dir / f"{self.config.experiment_name}_results.json"
        
        # NumPy ë°°ì—´ê³¼ Tensorë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
        def convert_for_json(obj):
            if isinstance(obj, dict):
                return {k: convert_for_json(v) for k, v in obj.items()}
            elif isinstance(obj, (list, tuple)):
                return [convert_for_json(item) for item in obj]
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, torch.Tensor):
                return obj.cpu().numpy().tolist()
            elif isinstance(obj, np.float32):
                return float(obj)
            elif hasattr(obj, '__dict__'):
                # ì„¤ì • ê°ì²´ë“¤ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (private ì†ì„± ì œì™¸)
                try:
                    result = {}
                    for k, v in obj.__dict__.items():
                        if not k.startswith('_'):  # private ì†ì„± ì œì™¸
                            result[k] = convert_for_json(v)
                    return result
                except Exception as e:
                    return str(obj)
            elif hasattr(obj, '_asdict'):  # namedtupleì¸ ê²½ìš°
                return convert_for_json(obj._asdict())
            elif callable(obj):
                return str(obj)
            elif isinstance(obj, (int, float, str, bool, type(None))):
                return obj
            else:
                try:
                    json.dumps(obj)  # JSON ì§ë ¬í™” ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
                    return obj
                except (TypeError, ValueError):
                    return str(obj)
        
        serializable_results = convert_for_json(self.experiment_results)
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ ì‹¤í—˜ ê²°ê³¼ ì €ì¥: {results_path}")
    
    def _print_final_summary(self, stage_results: List[Dict]):
        """ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ‰ ë‹¤ë‹¨ê³„ í•™ìŠµ ì™„ë£Œ! ìµœì¢… ê²°ê³¼ ìš”ì•½")
        logger.info("="*80)
        
        total_training_time = sum(stage['training_time'] for stage in stage_results)
        logger.info(f"ì´ í•™ìŠµ ì‹œê°„: {total_training_time:.1f}ì´ˆ ({total_training_time/60:.1f}ë¶„)")
        logger.info(f"ìˆ˜í–‰ëœ ë‹¨ê³„ ìˆ˜: {len(stage_results)}")
        
        logger.info("\nğŸ“Š ë‹¨ê³„ë³„ ì„±ëŠ¥:")
        for i, stage in enumerate(stage_results):
            logger.info(f"  Stage {i+1} ({stage['stage_name']}):")
            logger.info(f"    Val Loss: {stage['best_val_loss']:.4f}")
            logger.info(f"    Val Acc: {stage['best_val_accuracy']:.3f}")
            logger.info(f"    ì—í¬í¬: {stage['epochs_trained']}")
            if 'improvement_from_previous' in stage:
                logger.info(f"    ê°œì„ ë„: {stage['improvement_from_previous']:.4f}")
        
        logger.info(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {self.best_overall_performance['stage']} ë‹¨ê³„")
        logger.info(f"  Val Loss: {self.best_overall_performance['val_loss']:.4f}")
        logger.info(f"  Val Acc: {self.best_overall_performance['val_accuracy']:.3f}")
        
        if 'final_performance' in self.experiment_results:
            final_perf = self.experiment_results['final_performance']
            logger.info(f"\nğŸ¯ ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
            logger.info(f"  Test Loss: {final_perf['test_loss']:.4f}")
            logger.info(f"  Test Acc: {final_perf['test_accuracy']:.3f}")
        
        logger.info("="*80)

if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    from advanced_config import AdvancedTrainingConfig
    
    logging.basicConfig(level=logging.INFO)
    
    config = AdvancedTrainingConfig()
    config.multi_stage.stages = config.multi_stage.stages[:2]  # ì²˜ìŒ 2ë‹¨ê³„ë§Œ í…ŒìŠ¤íŠ¸
    
    trainer = AdvancedSignLanguageTrainer(config)
    results = trainer.train_multi_stage()
    
    print("âœ… ë‹¤ë‹¨ê³„ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")