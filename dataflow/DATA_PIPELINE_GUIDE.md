# ğŸ¯ ìˆ˜í™” ë°ì´í„° ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ê°€ì´ë“œ

## ğŸ“Š ì „ì²´ ë°ì´í„° íë¦„

### 1ï¸âƒ£ ì›ë³¸ ë°ì´í„° êµ¬ì¡°
```
ğŸ“ morpheme/
â”œâ”€â”€ word_morpheme/morpheme/01~16/    # WORD ë°ì´í„° (3,000ê°œ Ã— 16ëª… = 48,000ê°œ)
â”‚   â””â”€â”€ NIA_SL_WORD####_REAL##_[F/U/D/L/R]_morpheme.json
â””â”€â”€ sen_morpheme/morpheme/01~16/     # SEN ë°ì´í„° (2,000ê°œ Ã— 16ëª… = 32,000ê°œ)
    â””â”€â”€ NIA_SL_SEN####_REAL##_[F/U/D/L/R]_morpheme.json
```

**ê° JSON íŒŒì¼ ë‚´ìš©:**
- `metaData`: ì˜ìƒ ì •ë³´ (ì‹œê°„, URL ë“±)
- `data`: ì„¸ê·¸ë¨¼íŠ¸ ë°°ì—´ (start, end, attributes.name)
- ê° ì„¸ê·¸ë¨¼íŠ¸ëŠ” ìˆ˜í™” ë‹¨ì–´ì™€ ì‹œê°„ ì •ë³´ í¬í•¨

### 2ï¸âƒ£ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê³¼ì •

#### ğŸ”„ 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ (`complete_data_pipeline.py`)
- **ìš°ì„ ìˆœìœ„ ì‹œì  ì„ íƒ**: F > U > D > L > R
- **ì¤‘ë³µ ì œê±°**: ë™ì¼í•œ WORD/SEN+REAL ì¡°í•©ë‹¹ 1ê°œë§Œ ì„ íƒ
- **ê²°ê³¼**: 80,000ê°œ íŒŒì¼ â†’ 80,000ê°œ ì¸ìŠ¤í„´ìŠ¤ (ì¤‘ë³µ ì—†ìŒ)

#### ğŸ“– 2ë‹¨ê³„: Vocabulary êµ¬ì¶•
- **ë¹ˆë„ìˆœ ì •ë ¬**: ê°€ì¥ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë¶€í„° ID í• ë‹¹
- **ì–‘ë°©í–¥ ë§¤í•‘**: `word â†” vocab_id`
- **ê²°ê³¼**: 3,303ê°œ ê³ ìœ  ë‹¨ì–´ + ID ë§¤í•‘

#### ğŸ¯ 3ë‹¨ê³„: í•™ìŠµ ìµœì í™” êµ¬ì¡° ìƒì„±
```python
# ê° ì„¸ê·¸ë¨¼íŠ¸ êµ¬ì¡°
segment = {
    'data_type': 0,      # 0=WORD, 1=SEN
    'data_id': 1234,     # WORD1234 ë˜ëŠ” SEN1234
    'real_id': 5,        # REAL05
    'view': 0,           # 0=F, 1=U, 2=D, 3=L, 4=R
    'start_frame': 52,   # ì‹œì‘ í”„ë ˆì„
    'end_frame': 93,     # ë í”„ë ˆì„
    'duration': 42,      # ì§€ì† ì‹œê°„(í”„ë ˆì„)
    'vocab_ids': [513],  # vocabulary ID ë°°ì—´
    'vocab_len': 1       # ë‹¨ì–´ ê°œìˆ˜
}
```

#### ğŸ’¾ 4ë‹¨ê³„: LZF ì••ì¶• HDF5 ì €ì¥
- **ì••ì¶• ë°©ì‹**: LZF (34.4ë°° ë¹ ë¥¸ ì ‘ê·¼ ì†ë„)
- **êµ¬ì¡°**: ê³„ì¸µì  HDF5 (metadata, vocabulary, segments)
- **ìµœì í™”**: ì •ìˆ˜ ë°°ì—´, íŒ¨ë”© ì²˜ë¦¬, ì••ì¶•

### 3ï¸âƒ£ ìµœì¢… ë°ì´í„° í†µê³„

| í•­ëª© | ê°œìˆ˜ |
|------|------|
| **WORD ë°ì´í„°** | 3,000ê°œ |
| **SEN ë°ì´í„°** | 2,000ê°œ |
| **ì´ ì„¸ê·¸ë¨¼íŠ¸** | 149,874ê°œ |
| **ê³ ìœ  ë‹¨ì–´** | 3,303ê°œ |
| **íŒŒì¼ í¬ê¸°** | 1.07MB (LZF ì••ì¶•) |
| **ì‹œì  ë¶„í¬** | F: 100% (ìµœìš°ì„  ì„ íƒë¨) |

## ğŸ PyTorch í•™ìŠµ ì‚¬ìš©ë²•

### DataLoader ê¸°ë³¸ ì‚¬ìš©
```python
from torch.utils.data import DataLoader
from test_lzf_dataset import SignLanguageDataset

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = SignLanguageDataset('sign_language_dataset_lzf.h5')

# DataLoader ìƒì„±
dataloader = DataLoader(
    dataset, 
    batch_size=32, 
    shuffle=True, 
    collate_fn=collate_fn
)

# í›ˆë ¨ ë£¨í”„
for batch in dataloader:
    # batch['data_type']: 0=WORD, 1=SEN
    # batch['vocab_ids']: vocabulary ID í…ì„œ
    # batch['duration']: í”„ë ˆì„ ê¸¸ì´
    model_output = model(batch)
```

### ì„±ëŠ¥ íŠ¹ì§•
- **ì´ˆê³ ì† ë¡œë“œ**: 6,466,704ê°œ/ì´ˆ (LZF ë•ë¶„)
- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: íŒ¨ë”©ëœ vocab_idsë¡œ ë°°ì¹˜ ì²˜ë¦¬
- **ìœ ì—°í•œ êµ¬ì¡°**: WORD/SEN êµ¬ë¶„, ë‹¤ì–‘í•œ ê¸¸ì´ ì§€ì›

## ğŸ† LZFê°€ ìµœê³ ì¸ ì´ìœ 

### ì„±ëŠ¥ ë¹„êµ (ì´ì „ í…ŒìŠ¤íŠ¸ ê²°ê³¼)
- **LZF vs NPZ**: 34.4ë°° ë¹ ë¥¸ ì ‘ê·¼ ì†ë„
- **íŒŒì¼ í¬ê¸°**: 52.7% ê°ì†Œ
- **ì‹¤ì‹œê°„ ë¡œë“œ**: 0.0002ì´ˆ/1000ê°œ

### í•™ìŠµ ìµœì í™” ìš”ì†Œ
1. **Vocabulary ID ê¸°ë°˜**: ë¬¸ìì—´ â†’ ì •ìˆ˜ ë³€í™˜ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬
2. **ë°°ì¹˜ íŒ¨ë”©**: ê°€ë³€ ê¸¸ì´ íš¨ìœ¨ì  ì²˜ë¦¬
3. **ì••ì¶• ìµœì í™”**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
4. **PyTorch í˜¸í™˜**: ì§ì ‘ í…ì„œ ë³€í™˜ ì§€ì›

## ğŸ“ í•™ìŠµ ëª¨ë¸ ì œì•ˆ

### 1. ê¸°ë³¸ ë¶„ë¥˜ ëª¨ë¸
```python
import torch.nn as nn

class SignLanguageClassifier(nn.Module):
    def __init__(self, vocab_size, hidden_dim=256):
        super().__init__()
        self.vocab_size = vocab_size
        
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        
        # LSTM for sequence modeling
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        
        # Classification head
        self.classifier = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, batch):
        # vocab_ids: [batch_size, seq_len]
        vocab_ids = batch['vocab_ids']
        vocab_lens = batch['vocab_len']
        
        # Embedding
        embedded = self.embedding(vocab_ids)  # [batch_size, seq_len, hidden_dim]
        
        # LSTM
        lstm_out, _ = self.lstm(embedded)
        
        # Use last valid output
        last_outputs = []
        for i, length in enumerate(vocab_lens):
            if length > 0:
                last_outputs.append(lstm_out[i, length-1])
            else:
                last_outputs.append(torch.zeros_like(lstm_out[i, 0]))
        
        last_hidden = torch.stack(last_outputs)
        
        # Classification
        output = self.classifier(last_hidden)
        return output
```

### 2. ê³ ê¸‰ ê¸°ëŠ¥
- **Multi-task Learning**: WORD/SEN ë™ì‹œ í•™ìŠµ
- **Attention Mechanism**: ì¤‘ìš”í•œ ë‹¨ì–´ì— ì§‘ì¤‘
- **Temporal Modeling**: í”„ë ˆì„ ì‹œê°„ ì •ë³´ í™œìš©

## ğŸ“ˆ í•™ìŠµ ì „ëµ

### ë°ì´í„° ë¶„í• 
```python
# WORD/SEN ë³„ë„ í•™ìŠµ ë˜ëŠ” í†µí•© í•™ìŠµ
word_mask = batch['data_type'] == 0
sen_mask = batch['data_type'] == 1

word_loss = criterion(output[word_mask], target[word_mask])
sen_loss = criterion(output[sen_mask], target[sen_mask])
total_loss = word_loss + sen_loss
```

### í‰ê°€ ì§€í‘œ
- **ì •í™•ë„**: ë‹¨ì–´/ë¬¸ì¥ ì˜ˆì¸¡ ì •í™•ë„
- **Coverage**: vocabulary ì»¤ë²„ë¦¬ì§€
- **Temporal Consistency**: ì‹œê°„ì  ì¼ê´€ì„±

## ğŸ”§ í™•ì¥ ê°€ëŠ¥ì„±

1. **í¬ì¦ˆ ë°ì´í„° í†µí•©**: morpheme + pose ë°ì´í„° ê²°í•©
2. **ë‹¤ì¤‘ ì‹œì **: F/U/D/L/R ì‹œì  í™œìš©
3. **ì‹¤ì‹œê°„ ì¶”ë¡ **: LZF ë¹ ë¥¸ ì ‘ê·¼ìœ¼ë¡œ ì‹¤ì‹œê°„ ê°€ëŠ¥
4. **Transfer Learning**: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ í™œìš©

---

## âœ… ê²°ë¡ 

**ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!**

1. âœ… **ì›ë³¸ ë°ì´í„°**: morpheme JSON â†’ êµ¬ì¡°í™”ëœ ë°ì´í„°
2. âœ… **Vocabulary**: 3,303ê°œ ë‹¨ì–´ + ID ë§¤í•‘
3. âœ… **LZF ì••ì¶•**: ì´ˆê³ ì† ì ‘ê·¼ (34.4ë°° ë¹ ë¦„)
4. âœ… **PyTorch í˜¸í™˜**: ë°”ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ êµ¬ì¡°
5. âœ… **í™•ì¥ì„±**: í¬ì¦ˆ ë°ì´í„°, ë‹¤ì¤‘ ì‹œì  ë“± í™•ì¥ ê°€ëŠ¥

**ì´ì œ `sign_language_dataset_lzf.h5` íŒŒì¼ê³¼ `pytorch_dataloader.py`ë¥¼ ì‚¬ìš©í•´ì„œ ìˆ˜í™” í•™ìŠµ ëª¨ë¸ì„ ë°”ë¡œ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!** ğŸš€
