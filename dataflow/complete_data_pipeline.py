#!/usr/bin/env python3
"""
ì™„ì „í•œ ìˆ˜í™” ë°ì´í„° íŒŒì´í”„ë¼ì¸ - LZF ìµœì í™” ë²„ì „
ì›ë³¸ morpheme ë°ì´í„° â†’ vocabulary ê¸°ë°˜ í•™ìŠµìš© LZF ì••ì¶• íŒŒì¼

ë°ì´í„° íë¦„:
1. ì›ë³¸ morpheme JSON íŒŒì¼ë“¤ ìˆ˜ì§‘
2. vocabulary ID ë§¤í•‘ìœ¼ë¡œ êµ¬ì¡°í™”
3. í•™ìŠµ ìµœì í™”ëœ êµ¬ì¡°ë¡œ ë³€í™˜
4. LZF ì••ì¶•ìœ¼ë¡œ ìµœì¢… ì €ì¥

íŠ¹ì§•:
- vocabulary ID ê¸°ë°˜ ë§¤í•‘ (í•™ìŠµ íš¨ìœ¨ì„±)
- ìš°ì„ ìˆœìœ„ ì‹œì  ì„ íƒ (F>U>D>L>R)
- LZF ì••ì¶• (34.4ë°° ë¹ ë¥¸ ì ‘ê·¼)
- PyTorch í˜¸í™˜ êµ¬ì¡°
"""

import json
import h5py
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Set, Any, Tuple, Optional
from collections import Counter, defaultdict
from tqdm import tqdm
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class CompletePipeline:
    """ì™„ì „í•œ ìˆ˜í™” ë°ì´í„° íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, 
                 morpheme_root: str = "/home/jy/gitwork/mmpose/jy/morpheme", 
                 fps: int = 30,
                 output_dir: str = "/home/jy/gitwork/mmpose/jy"):
        """
        Args:
            morpheme_root: morpheme ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ
            fps: ì˜ìƒ í”„ë ˆì„ìœ¨ (30fps)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.morpheme_root = Path(morpheme_root)
        self.fps = fps
        self.output_dir = Path(output_dir)
        
        # ìš°ì„ ìˆœìœ„ ì‹œì  (F>U>D>L>R)
        self.view_priority = ['F', 'U', 'D', 'L', 'R']
        
        # ë°ì´í„° ì €ì¥ êµ¬ì¡°
        self.raw_data = {
            'word_data': {},     # {word_id: [instances]}
            'sentence_data': {}  # {sen_id: [instances]}
        }
        
        # vocabulary êµ¬ì¡° (ë¹ˆë„ìˆœ)
        self.vocabulary = {}     # {word: vocab_id}
        self.vocab_list = []     # [word1, word2, ...] ë¹ˆë„ìˆœ
        self.word_counter = Counter()
        
        # í†µê³„
        self.stats = {
            'total_files_found': 0,
            'files_processed': 0,
            'files_failed': 0,
            'view_distribution': Counter(),
            'unique_words': 0,
            'total_segments': 0
        }
        
        logger.info("ğŸš€ Complete Pipeline ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ğŸ“ ì›ë³¸ ê²½ë¡œ: {self.morpheme_root}")
        logger.info(f"   ğŸ¯ ì¶œë ¥ ê²½ë¡œ: {self.output_dir}")

    def time_to_frame(self, time_seconds: float) -> int:
        """ì‹œê°„(ì´ˆ) â†’ í”„ë ˆì„ ë²ˆí˜¸ ë³€í™˜"""
        return round(time_seconds * self.fps)

    def extract_file_info(self, filename: str) -> Optional[Dict[str, Any]]:
        """íŒŒì¼ëª… ì •ë³´ ì¶”ì¶œ
        
        ì˜ˆ: NIA_SL_WORD2947_REAL02_F_morpheme.json
        â†’ {'type': 'WORD', 'id': 2947, 'real_id': 2, 'view': 'F'}
        """
        try:
            parts = filename.replace('.json', '').split('_')
            
            # ë°ì´í„° íƒ€ì…ê³¼ ID ì°¾ê¸°
            data_type = None
            data_id = None
            real_id = None
            view = None
            
            for i, part in enumerate(parts):
                if part.startswith('WORD'):
                    data_type = 'WORD'
                    data_id = int(part[4:])
                elif part.startswith('SEN'):
                    data_type = 'SEN'
                    data_id = int(part[3:])
                elif part.startswith('REAL'):
                    real_id = int(part[4:])
                elif part in ['F', 'U', 'D', 'L', 'R']:
                    view = part
            
            if all([data_type, data_id is not None, real_id is not None, view]):
                return {
                    'type': data_type,
                    'id': data_id,
                    'real_id': real_id,
                    'view': view
                }
                
        except (ValueError, IndexError) as e:
            logger.warning(f"íŒŒì¼ëª… íŒŒì‹± ì‹¤íŒ¨: {filename} - {e}")
        
        return None

    def find_best_view_file(self, data_type: str, data_id: int, real_id: int, folder_path: Path) -> Optional[Tuple[Path, str]]:
        """ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ìµœì ì˜ ì‹œì  íŒŒì¼ ì°¾ê¸°"""
        
        for view in self.view_priority:
            pattern = f"NIA_SL_{data_type}{data_id:04d}_REAL{real_id:02d}_{view}_morpheme.json"
            candidate_file = folder_path / pattern
            
            if candidate_file.exists():
                return candidate_file, view
        
        return None, None

    def process_morpheme_file(self, file_path: Path, file_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ morpheme íŒŒì¼ ì²˜ë¦¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë©”íƒ€ë°ì´í„°
            metadata = data.get('metaData', {})
            morpheme_segments = data.get('data', [])
            
            # ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
            processed_segments = []
            for segment in morpheme_segments:
                start_time = segment.get('start', 0.0)
                end_time = segment.get('end', 0.0)
                
                # í”„ë ˆì„ ë³€í™˜
                start_frame = self.time_to_frame(start_time)
                end_frame = self.time_to_frame(end_time)
                
                # ë‹¨ì–´ ì¶”ì¶œ ë° vocabulary ì—…ë°ì´íŠ¸
                words = []
                vocab_ids = []
                
                if 'attributes' in segment:
                    for attr in segment['attributes']:
                        word = attr.get('name', '').strip()
                        if word:
                            words.append(word)
                            self.word_counter[word] += 1
                
                processed_segments.append({
                    'start_time': start_time,
                    'end_time': end_time,
                    'start_frame': start_frame,
                    'end_frame': end_frame,
                    'duration_frames': end_frame - start_frame + 1,
                    'words': words,
                    'vocab_ids': []  # ë‚˜ì¤‘ì— vocabulary ìƒì„± í›„ í• ë‹¹
                })
            
            return {
                'data_type': file_info['type'],
                'data_id': file_info['id'],
                'real_id': file_info['real_id'],
                'view': file_info['view'],
                'file_path': str(file_path),
                'metadata': {
                    'duration': metadata.get('duration', 0.0),
                    'duration_frames': self.time_to_frame(metadata.get('duration', 0.0)),
                    'url': metadata.get('url', ''),
                    'exported_on': metadata.get('exportedOn', '')
                },
                'segments': processed_segments,
                'total_segments': len(processed_segments)
            }
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {e}")
            return None

    def collect_all_data(self):
        """ëª¨ë“  morpheme ë°ì´í„° ìˆ˜ì§‘ (ìš°ì„ ìˆœìœ„ ì‹œì  ì„ íƒ)"""
        logger.info("ğŸ“Š ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        # WORD ë°ì´í„° ìˆ˜ì§‘ (01~16 í´ë”)
        word_dir = self.morpheme_root / "word_morpheme"
        logger.info("ğŸ“š WORD ë°ì´í„° ìˆ˜ì§‘...")
        
        for folder_num in range(1, 17):  # 01~16
            folder_path = word_dir / f"{folder_num:02d}"
            if not folder_path.exists():
                continue
            
            logger.info(f"   ğŸ“‚ í´ë” {folder_num:02d} ì²˜ë¦¬...")
            
            # íŒŒì¼ ê·¸ë£¹í™” (ë™ì¼ WORD+REAL ì¡°í•©)
            file_groups = defaultdict(list)
            
            for json_file in folder_path.glob("*.json"):
                file_info = self.extract_file_info(json_file.name)
                if file_info and file_info['type'] == 'WORD':
                    key = (file_info['id'], file_info['real_id'])
                    file_groups[key].append((json_file, file_info))
            
            # ê° ê·¸ë£¹ì—ì„œ ìµœì  ì‹œì  ì„ íƒ
            for (word_id, real_id), files in file_groups.items():
                # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬
                files.sort(key=lambda x: self.view_priority.index(x[1]['view']))
                best_file, best_info = files[0]
                
                processed = self.process_morpheme_file(best_file, best_info)
                if processed:
                    if word_id not in self.raw_data['word_data']:
                        self.raw_data['word_data'][word_id] = []
                    self.raw_data['word_data'][word_id].append(processed)
                    
                    self.stats['files_processed'] += 1
                    self.stats['view_distribution'][best_info['view']] += 1
                    self.stats['total_segments'] += processed['total_segments']
        
        # SEN ë°ì´í„° ìˆ˜ì§‘ (01~16 í´ë”)
        sen_dir = self.morpheme_root / "sen_morpheme"
        logger.info("ğŸ“ SEN ë°ì´í„° ìˆ˜ì§‘...")
        
        for folder_num in range(1, 17):  # 01~16
            folder_path = sen_dir / f"{folder_num:02d}"
            if not folder_path.exists():
                continue
            
            logger.info(f"   ğŸ“‚ í´ë” {folder_num:02d} ì²˜ë¦¬...")
            
            # íŒŒì¼ ê·¸ë£¹í™”
            file_groups = defaultdict(list)
            
            for json_file in folder_path.glob("*.json"):
                file_info = self.extract_file_info(json_file.name)
                if file_info and file_info['type'] == 'SEN':
                    key = (file_info['id'], file_info['real_id'])
                    file_groups[key].append((json_file, file_info))
            
            # ê° ê·¸ë£¹ì—ì„œ ìµœì  ì‹œì  ì„ íƒ
            for (sen_id, real_id), files in file_groups.items():
                files.sort(key=lambda x: self.view_priority.index(x[1]['view']))
                best_file, best_info = files[0]
                
                processed = self.process_morpheme_file(best_file, best_info)
                if processed:
                    if sen_id not in self.raw_data['sentence_data']:
                        self.raw_data['sentence_data'][sen_id] = []
                    self.raw_data['sentence_data'][sen_id].append(processed)
                    
                    self.stats['files_processed'] += 1
                    self.stats['view_distribution'][best_info['view']] += 1
                    self.stats['total_segments'] += processed['total_segments']
        
        logger.info(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ:")
        logger.info(f"   ğŸ“š WORD í•­ëª©: {len(self.raw_data['word_data'])}ê°œ")
        logger.info(f"   ğŸ“ SEN í•­ëª©: {len(self.raw_data['sentence_data'])}ê°œ")
        logger.info(f"   ğŸ“Š ì²˜ë¦¬ëœ íŒŒì¼: {self.stats['files_processed']}ê°œ")
        logger.info(f"   ğŸ¯ ì‹œì  ë¶„í¬: {dict(self.stats['view_distribution'])}")

    def build_vocabulary(self):
        """ë¹ˆë„ìˆœ vocabulary êµ¬ì¶• ë° ID í• ë‹¹"""
        logger.info("ğŸ“– Vocabulary êµ¬ì¶•...")
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        sorted_words = self.word_counter.most_common()
        
        # vocabulary ë§¤í•‘ ìƒì„±
        self.vocab_list = [word for word, count in sorted_words]
        self.vocabulary = {word: idx for idx, word in enumerate(self.vocab_list)}
        
        # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì˜ vocab_ids ì—…ë°ì´íŠ¸
        self.update_vocab_ids()
        
        self.stats['unique_words'] = len(self.vocabulary)
        
        logger.info(f"âœ… Vocabulary êµ¬ì¶• ì™„ë£Œ:")
        logger.info(f"   ğŸ“– ê³ ìœ  ë‹¨ì–´: {len(self.vocabulary)}ê°œ")
        logger.info(f"   ğŸ” ìµœë¹ˆ ë‹¨ì–´ TOP 10: {self.vocab_list[:10]}")

    def update_vocab_ids(self):
        """ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì˜ vocab_ids ì—…ë°ì´íŠ¸"""
        
        def update_data_vocab_ids(data_dict):
            for data_id, instances in data_dict.items():
                for instance in instances:
                    for segment in instance['segments']:
                        vocab_ids = []
                        for word in segment['words']:
                            if word in self.vocabulary:
                                vocab_ids.append(self.vocabulary[word])
                            else:
                                logger.warning(f"ë‹¨ì–´ '{word}'ê°€ vocabularyì— ì—†ìŒ")
                        segment['vocab_ids'] = vocab_ids
        
        update_data_vocab_ids(self.raw_data['word_data'])
        update_data_vocab_ids(self.raw_data['sentence_data'])

    def create_learning_structure(self) -> Dict[str, Any]:
        """í•™ìŠµ ìµœì í™”ëœ ë°ì´í„° êµ¬ì¡° ìƒì„±"""
        logger.info("ğŸ¯ í•™ìŠµìš© ë°ì´í„° êµ¬ì¡° ìƒì„±...")
        
        learning_data = {
            'metadata': {
                'version': '1.0',
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'fps': self.fps,
                'vocabulary_size': len(self.vocabulary),
                'compression': 'lzf'
            },
            'vocabulary': {
                'words': self.vocab_list,
                'word_to_id': self.vocabulary,
                'id_to_word': {idx: word for word, idx in self.vocabulary.items()},
                'word_frequencies': dict(self.word_counter.most_common())
            },
            'statistics': self.stats
        }
        
        # í•™ìŠµìš© segments êµ¬ì¡°
        all_segments = []
        
        # WORD ë°ì´í„° ë³€í™˜
        for word_id, instances in self.raw_data['word_data'].items():
            for instance in instances:
                for segment in instance['segments']:
                    all_segments.append({
                        'data_type': 'WORD',
                        'data_id': word_id,
                        'real_id': instance['real_id'],
                        'view': instance['view'],
                        'start_frame': segment['start_frame'],
                        'end_frame': segment['end_frame'],
                        'duration_frames': segment['duration_frames'],
                        'vocab_ids': segment['vocab_ids'],
                        'words': segment['words']  # ë””ë²„ê¹…ìš©
                    })
        
        # SEN ë°ì´í„° ë³€í™˜
        for sen_id, instances in self.raw_data['sentence_data'].items():
            for instance in instances:
                for segment in instance['segments']:
                    all_segments.append({
                        'data_type': 'SEN',
                        'data_id': sen_id,
                        'real_id': instance['real_id'],
                        'view': instance['view'],
                        'start_frame': segment['start_frame'],
                        'end_frame': segment['end_frame'],
                        'duration_frames': segment['duration_frames'],
                        'vocab_ids': segment['vocab_ids'],
                        'words': segment['words']  # ë””ë²„ê¹…ìš©
                    })
        
        learning_data['segments'] = all_segments
        
        logger.info(f"âœ… í•™ìŠµìš© êµ¬ì¡° ìƒì„± ì™„ë£Œ:")
        logger.info(f"   ğŸ“Š ì´ ì„¸ê·¸ë¨¼íŠ¸: {len(all_segments)}ê°œ")
        
        return learning_data

    def save_as_lzf_hdf5(self, learning_data: Dict[str, Any]) -> str:
        """LZF ì••ì¶• HDF5 í˜•íƒœë¡œ ì €ì¥ (ìµœê³  ì„±ëŠ¥)"""
        output_file = self.output_dir / "sign_language_dataset_lzf.h5"
        
        logger.info("ğŸ’¾ LZF ì••ì¶• HDF5 ì €ì¥...")
        
        start_time = time.time()
        
        with h5py.File(output_file, 'w') as f:
            # ë©”íƒ€ë°ì´í„° ê·¸ë£¹
            meta_group = f.create_group('metadata')
            for key, value in learning_data['metadata'].items():
                if isinstance(value, str):
                    meta_group.attrs[key] = value
                else:
                    meta_group.attrs[key] = value
            
            # vocabulary ê·¸ë£¹ (LZF ì••ì¶•)
            vocab_group = f.create_group('vocabulary')
            
            # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ë¬¸ìì—´ ë°°ì—´)
            words_array = np.array(learning_data['vocabulary']['words'], dtype=h5py.string_dtype())
            vocab_group.create_dataset('words', data=words_array, compression='lzf')
            
            # ë‹¨ì–´ ë¹ˆë„ (ì •ìˆ˜ ë°°ì—´)
            frequencies = np.array([learning_data['vocabulary']['word_frequencies'][word] 
                                  for word in learning_data['vocabulary']['words']], dtype=np.int32)
            vocab_group.create_dataset('frequencies', data=frequencies, compression='lzf')
            
            # segments ë°ì´í„° (LZF ì••ì¶•)
            segments_group = f.create_group('segments')
            
            segments = learning_data['segments']
            n_segments = len(segments)
            
            # íš¨ìœ¨ì ì¸ ë°°ì—´ êµ¬ì¡°
            data_types = np.array([0 if s['data_type'] == 'WORD' else 1 for s in segments], dtype=np.int8)
            data_ids = np.array([s['data_id'] for s in segments], dtype=np.int16)
            real_ids = np.array([s['real_id'] for s in segments], dtype=np.int8)
            views = np.array([self.view_priority.index(s['view']) for s in segments], dtype=np.int8)
            start_frames = np.array([s['start_frame'] for s in segments], dtype=np.int32)
            end_frames = np.array([s['end_frame'] for s in segments], dtype=np.int32)
            duration_frames = np.array([s['duration_frames'] for s in segments], dtype=np.int32)
            
            # vocab_idsëŠ” ê°€ë³€ ê¸¸ì´ì´ë¯€ë¡œ íŠ¹ë³„ ì²˜ë¦¬
            max_vocab_len = max(len(s['vocab_ids']) for s in segments) if segments else 0
            vocab_ids_padded = np.full((n_segments, max_vocab_len), -1, dtype=np.int16)  # -1ë¡œ íŒ¨ë”©
            vocab_lens = np.zeros(n_segments, dtype=np.int8)
            
            for i, segment in enumerate(segments):
                vocab_ids = segment['vocab_ids']
                vocab_lens[i] = len(vocab_ids)
                if vocab_ids:
                    vocab_ids_padded[i, :len(vocab_ids)] = vocab_ids
            
            # LZF ì••ì¶•ìœ¼ë¡œ ì €ì¥
            segments_group.create_dataset('data_types', data=data_types, compression='lzf')
            segments_group.create_dataset('data_ids', data=data_ids, compression='lzf')
            segments_group.create_dataset('real_ids', data=real_ids, compression='lzf')
            segments_group.create_dataset('views', data=views, compression='lzf')
            segments_group.create_dataset('start_frames', data=start_frames, compression='lzf')
            segments_group.create_dataset('end_frames', data=end_frames, compression='lzf')
            segments_group.create_dataset('duration_frames', data=duration_frames, compression='lzf')
            segments_group.create_dataset('vocab_ids', data=vocab_ids_padded, compression='lzf')
            segments_group.create_dataset('vocab_lens', data=vocab_lens, compression='lzf')
            
            # í†µê³„ ì •ë³´
            stats_group = f.create_group('statistics')
            for key, value in learning_data['statistics'].items():
                if isinstance(value, dict):
                    # Counter ê°ì²´ ë“± ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
                    subgroup = stats_group.create_group(key)
                    for subkey, subvalue in value.items():
                        subgroup.attrs[str(subkey)] = subvalue
                else:
                    stats_group.attrs[key] = value
        
        save_time = time.time() - start_time
        file_size = output_file.stat().st_size / (1024*1024)
        
        logger.info(f"âœ… LZF HDF5 ì €ì¥ ì™„ë£Œ:")
        logger.info(f"   ğŸ“ íŒŒì¼: {output_file}")
        logger.info(f"   ğŸ“Š í¬ê¸°: {file_size:.2f}MB")
        logger.info(f"   â±ï¸  ì €ì¥ ì‹œê°„: {save_time:.2f}ì´ˆ")
        
        return str(output_file)

    def create_pytorch_dataloader_code(self) -> str:
        """PyTorch DataLoader ì½”ë“œ ìƒì„±"""
        dataloader_code = '''
# PyTorch DataLoader for LZF Sign Language Dataset
import h5py
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

class SignLanguageDataset(Dataset):
    """ìˆ˜í™” ë°ì´í„°ì…‹ (LZF HDF5 ê¸°ë°˜)"""
    
    def __init__(self, hdf5_path: str):
        self.hdf5_path = hdf5_path
        
        # ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ
        with h5py.File(hdf5_path, 'r') as f:
            self.n_segments = len(f['segments']['data_types'])
            self.vocab_size = f.attrs['vocabulary_size']
            
            # vocabulary ë¡œë“œ
            self.words = [w.decode() for w in f['vocabulary']['words'][:]]
            self.word_to_id = {word: idx for idx, word in enumerate(self.words)}
            
            # view ë§¤í•‘
            self.view_names = ['F', 'U', 'D', 'L', 'R']
    
    def __len__(self):
        return self.n_segments
    
    def __getitem__(self, idx):
        with h5py.File(self.hdf5_path, 'r') as f:
            # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´
            data_type = f['segments']['data_types'][idx]  # 0=WORD, 1=SEN
            data_id = f['segments']['data_ids'][idx]
            real_id = f['segments']['real_ids'][idx]
            view = f['segments']['views'][idx]
            start_frame = f['segments']['start_frames'][idx]
            end_frame = f['segments']['end_frames'][idx]
            duration = f['segments']['duration_frames'][idx]
            
            # vocab_ids (íŒ¨ë”© ì œê±°)
            vocab_len = f['segments']['vocab_lens'][idx]
            vocab_ids = f['segments']['vocab_ids'][idx, :vocab_len].tolist()
            
            return {
                'data_type': data_type,  # 0=WORD, 1=SEN
                'data_id': data_id,
                'real_id': real_id,
                'view': view,  # 0=F, 1=U, 2=D, 3=L, 4=R
                'start_frame': start_frame,
                'end_frame': end_frame,
                'duration': duration,
                'vocab_ids': torch.tensor(vocab_ids, dtype=torch.long),
                'vocab_len': vocab_len
            }

# ì‚¬ìš© ì˜ˆì‹œ
def create_dataloader(hdf5_path: str, batch_size: int = 32, shuffle: bool = True):
    """DataLoader ìƒì„±"""
    dataset = SignLanguageDataset(hdf5_path)
    
    def collate_fn(batch):
        """ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜"""
        # ê°€ë³€ ê¸¸ì´ vocab_ids ì²˜ë¦¬
        max_vocab_len = max(item['vocab_len'] for item in batch)
        
        batch_data = {}
        for key in batch[0].keys():
            if key == 'vocab_ids':
                # íŒ¨ë”© ì²˜ë¦¬
                padded_ids = []
                for item in batch:
                    ids = item[key]
                    padded = torch.cat([ids, torch.zeros(max_vocab_len - len(ids), dtype=torch.long)])
                    padded_ids.append(padded)
                batch_data[key] = torch.stack(padded_ids)
            else:
                batch_data[key] = torch.tensor([item[key] for item in batch])
        
        return batch_data
    
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)

# ì‚¬ìš©ë²•
if __name__ == "__main__":
    # DataLoader ìƒì„±
    dataloader = create_dataloader("sign_language_dataset_lzf.h5", batch_size=16)
    
    # ì²« ë²ˆì§¸ ë°°ì¹˜ í™•ì¸
    for batch in dataloader:
        print("ë°°ì¹˜ í¬ê¸°:", batch['data_type'].shape[0])
        print("ë°ì´í„° íƒ€ì… ë¶„í¬:", torch.bincount(batch['data_type']))
        print("vocabulary ID í˜•íƒœ:", batch['vocab_ids'].shape)
        print("í‰ê·  ë‹¨ì–´ ê¸¸ì´:", batch['vocab_len'].float().mean())
        break
'''
        
        # ì½”ë“œ íŒŒì¼ë¡œ ì €ì¥
        code_file = self.output_dir / "pytorch_dataloader.py"
        with open(code_file, 'w', encoding='utf-8') as f:
            f.write(dataloader_code)
        
        logger.info(f"ğŸ“ PyTorch DataLoader ì½”ë“œ ìƒì„±: {code_file}")
        
        return str(code_file)

    def run_complete_pipeline(self):
        """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info("=" * 60)
        
        try:
            # 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘
            self.collect_all_data()
            
            # 2ë‹¨ê³„: Vocabulary êµ¬ì¶•
            self.build_vocabulary()
            
            # 3ë‹¨ê³„: í•™ìŠµìš© êµ¬ì¡° ìƒì„±
            learning_data = self.create_learning_structure()
            
            # 4ë‹¨ê³„: LZF HDF5 ì €ì¥
            lzf_file = self.save_as_lzf_hdf5(learning_data)
            
            # 5ë‹¨ê³„: PyTorch DataLoader ì½”ë“œ ìƒì„±
            dataloader_file = self.create_pytorch_dataloader_code()
            
            # ìµœì¢… ë¦¬í¬íŠ¸
            logger.info("\n" + "ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!" + "\n" + "=" * 60)
            logger.info(f"ğŸ“Š ìµœì¢… í†µê³„:")
            logger.info(f"   ğŸ“š WORD ë°ì´í„°: {len(self.raw_data['word_data'])}ê°œ")
            logger.info(f"   ğŸ“ SEN ë°ì´í„°: {len(self.raw_data['sentence_data'])}ê°œ")
            logger.info(f"   ğŸ“– ê³ ìœ  ë‹¨ì–´: {len(self.vocabulary)}ê°œ")
            logger.info(f"   ğŸ¯ ì´ ì„¸ê·¸ë¨¼íŠ¸: {self.stats['total_segments']}ê°œ")
            logger.info(f"   ğŸ“ LZF íŒŒì¼: {lzf_file}")
            logger.info(f"   ğŸ PyTorch ì½”ë“œ: {dataloader_file}")
            
            return {
                'lzf_file': lzf_file,
                'dataloader_file': dataloader_file,
                'statistics': self.stats,
                'vocabulary_size': len(self.vocabulary)
            }
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    pipeline = CompletePipeline()
    result = pipeline.run_complete_pipeline()
    
    print("\nğŸ¯ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"LZF íŒŒì¼: {result['lzf_file']}")
    print(f"DataLoader ì½”ë“œ: {result['dataloader_file']}")


if __name__ == "__main__":
    main()
