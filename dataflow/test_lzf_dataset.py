#!/usr/bin/env python3
"""
LZF HDF5 íŒŒì¼ êµ¬ì¡° í™•ì¸ ë° PyTorch DataLoader í…ŒìŠ¤íŠ¸
"""
import h5py
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import time

def test_lzf_file_structure():
    """LZF HDF5 íŒŒì¼ êµ¬ì¡° í™•ì¸"""
    print("ğŸ” LZF HDF5 íŒŒì¼ êµ¬ì¡° ë¶„ì„")
    print("=" * 50)
    
    with h5py.File('sign_language_dataset_lzf.h5', 'r') as f:
        # ê¸°ë³¸ ì •ë³´
        print("ğŸ“Š ê¸°ë³¸ ì •ë³´:")
        print(f"   íŒŒì¼ í¬ê¸°: {sum(f[name].size * f[name].dtype.itemsize for name in f.keys() if isinstance(f[name], h5py.Dataset)) / 1024**2:.2f}MB")
        
        # ë©”íƒ€ë°ì´í„°
        print("\nğŸ“‹ ë©”íƒ€ë°ì´í„°:")
        if hasattr(f, 'attrs'):
            for key in f.attrs.keys():
                print(f"   {key}: {f.attrs[key]}")
        
        # vocabulary ì •ë³´
        print("\nğŸ“– Vocabulary:")
        if 'vocabulary' in f:
            vocab_size = len(f['vocabulary/words'])
            print(f"   ì „ì²´ ë‹¨ì–´ ìˆ˜: {vocab_size:,}ê°œ")
            
            # ìƒìœ„ ë‹¨ì–´ë“¤
            top_words = [w.decode() if hasattr(w, 'decode') else str(w) for w in f['vocabulary/words'][:10]]
            top_freqs = f['vocabulary/frequencies'][:10] if 'frequencies' in f['vocabulary'] else []
            print(f"   ìµœë¹ˆ ë‹¨ì–´ TOP 10: {top_words}")
            if len(top_freqs) > 0:
                print(f"   ë¹ˆë„ìˆ˜: {top_freqs.tolist()}")
        
        # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´
        print("\nğŸ¯ ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„°:")
        if 'segments' in f:
            n_segments = len(f['segments/data_types'])
            print(f"   ì´ ì„¸ê·¸ë¨¼íŠ¸: {n_segments:,}ê°œ")
            
            # ë°ì´í„° íƒ€ì… ë¶„í¬
            data_types = f['segments/data_types'][:]
            word_count = np.sum(data_types == 0)
            sen_count = np.sum(data_types == 1)
            print(f"   WORD ì„¸ê·¸ë¨¼íŠ¸: {word_count:,}ê°œ")
            print(f"   SEN ì„¸ê·¸ë¨¼íŠ¸: {sen_count:,}ê°œ")
            
            # ì‹œì  ë¶„í¬
            views = f['segments/views'][:]
            view_names = ['F', 'U', 'D', 'L', 'R']
            view_counts = {view_names[i]: np.sum(views == i) for i in range(5)}
            print(f"   ì‹œì  ë¶„í¬: {view_counts}")
            
            # vocab ê¸¸ì´ í†µê³„
            vocab_lens = f['segments/vocab_lens'][:]
            print(f"   í‰ê·  ë‹¨ì–´ ê¸¸ì´: {np.mean(vocab_lens):.2f}ê°œ")
            print(f"   ìµœëŒ€ ë‹¨ì–´ ê¸¸ì´: {np.max(vocab_lens)}ê°œ")
            
            # ìƒ˜í”Œ ë°ì´í„°
            print(f"\n   ì²« 10ê°œ ì„¸ê·¸ë¨¼íŠ¸ ìƒ˜í”Œ:")
            for i in range(min(10, n_segments)):
                dtype_name = 'WORD' if data_types[i] == 0 else 'SEN'
                data_id = f['segments/data_ids'][i]
                vocab_len = vocab_lens[i]
                duration = f['segments/duration_frames'][i]
                print(f"     {i+1:2d}. {dtype_name}{data_id:04d} - {vocab_len}ë‹¨ì–´, {duration}í”„ë ˆì„")


class SignLanguageDataset(Dataset):
    """ìˆ˜í™” ë°ì´í„°ì…‹ (LZF HDF5 ê¸°ë°˜)"""
    
    def __init__(self, hdf5_path: str):
        self.hdf5_path = hdf5_path
        
        # ë°ì´í„°ì…‹ ì •ë³´ ë¡œë“œ
        with h5py.File(hdf5_path, 'r') as f:
            self.n_segments = len(f['segments']['data_types'])
            
            # vocabulary ë¡œë“œ
            self.words = [w.decode() if hasattr(w, 'decode') else str(w) 
                         for w in f['vocabulary']['words'][:]]
            self.word_to_id = {word: idx for idx, word in enumerate(self.words)}
            self.vocab_size = len(self.words)
            
            # view ë§¤í•‘
            self.view_names = ['F', 'U', 'D', 'L', 'R']
    
    def __len__(self):
        return self.n_segments
    
    def __getitem__(self, idx):
        with h5py.File(self.hdf5_path, 'r') as f:
            # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´
            data_type = int(f['segments']['data_types'][idx])  # 0=WORD, 1=SEN
            data_id = int(f['segments']['data_ids'][idx])
            real_id = int(f['segments']['real_ids'][idx])
            view = int(f['segments']['views'][idx])
            start_frame = int(f['segments']['start_frames'][idx])
            end_frame = int(f['segments']['end_frames'][idx])
            duration = int(f['segments']['duration_frames'][idx])
            
            # vocab_ids (íŒ¨ë”© ì œê±°)
            vocab_len = int(f['segments']['vocab_lens'][idx])
            vocab_ids = f['segments']['vocab_ids'][idx, :vocab_len].tolist()
            
            return {
                'data_type': data_type,  # 0=WORD, 1=SEN
                'data_id': data_id,
                'real_id': real_id,
                'view': view,  # 0=F, 1=U, 2=D, 3=L, 4=R
                'start_frame': start_frame,
                'end_frame': end_frame,
                'duration': duration,
                'vocab_ids': torch.tensor(vocab_ids, dtype=torch.long),
                'vocab_len': vocab_len
            }


def test_pytorch_dataloader():
    """PyTorch DataLoader í…ŒìŠ¤íŠ¸"""
    print("\nğŸ PyTorch DataLoader í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    try:
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = SignLanguageDataset('sign_language_dataset_lzf.h5')
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: {len(dataset):,}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        print(f"   Vocabulary í¬ê¸°: {dataset.vocab_size:,}ê°œ")
        
        # DataLoader ìƒì„± (ì‘ì€ ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸)
        def collate_fn(batch):
            """ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜"""
            # ê°€ë³€ ê¸¸ì´ vocab_ids ì²˜ë¦¬
            max_vocab_len = max(item['vocab_len'] for item in batch) if batch else 0
            
            batch_data = {}
            for key in batch[0].keys():
                if key == 'vocab_ids':
                    # íŒ¨ë”© ì²˜ë¦¬
                    padded_ids = []
                    for item in batch:
                        ids = item[key]
                        padded = torch.cat([ids, torch.zeros(max_vocab_len - len(ids), dtype=torch.long)])
                        padded_ids.append(padded)
                    batch_data[key] = torch.stack(padded_ids)
                else:
                    batch_data[key] = torch.tensor([item[key] for item in batch])
            
            return batch_data
        
        dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        print("\nğŸ”¬ ì²« ë²ˆì§¸ ë°°ì¹˜ ë¶„ì„:")
        batch = next(iter(dataloader))
        
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch['data_type'].shape[0]}")
        print(f"   ë°ì´í„° íƒ€ì…: {batch['data_type'].tolist()}")
        print(f"   í‰ê·  ë‹¨ì–´ ê¸¸ì´: {batch['vocab_len'].float().mean():.2f}")
        print(f"   ìµœëŒ€ ë‹¨ì–´ ê¸¸ì´: {batch['vocab_len'].max()}")
        print(f"   vocab_ids í˜•íƒœ: {batch['vocab_ids'].shape}")
        
        # ê°œë³„ ìƒ˜í”Œ í™•ì¸
        print(f"\nğŸ“‹ ì²« ë²ˆì§¸ ìƒ˜í”Œ ìƒì„¸:")
        sample = dataset[0]
        dtype_name = 'WORD' if sample['data_type'] == 0 else 'SEN'
        view_name = dataset.view_names[sample['view']]
        print(f"   íƒ€ì…: {dtype_name}{sample['data_id']:04d}")
        print(f"   ì‹œì : {view_name}")
        print(f"   í”„ë ˆì„: {sample['start_frame']} ~ {sample['end_frame']} ({sample['duration']}í”„ë ˆì„)")
        print(f"   ë‹¨ì–´ ID: {sample['vocab_ids'].tolist()}")
        
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print(f"\nâš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:")
        start_time = time.time()
        for i, batch in enumerate(dataloader):
            if i >= 10:  # 10ê°œ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
                break
        load_time = time.time() - start_time
        
        print(f"   10ê°œ ë°°ì¹˜ ë¡œë“œ ì‹œê°„: {load_time:.4f}ì´ˆ")
        print(f"   ë°°ì¹˜ë‹¹ í‰ê·  ì‹œê°„: {load_time/10:.4f}ì´ˆ")
        
        print("\nâœ… PyTorch DataLoader í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ PyTorch DataLoader í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


def performance_comparison():
    """ì„±ëŠ¥ ë¹„êµ (LZF vs ì¼ë°˜ ë°©ì‹)"""
    print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ")
    print("=" * 30)
    
    try:
        # LZF ì½ê¸° ì„±ëŠ¥
        lzf_times = []
        for i in range(5):
            start = time.time()
            with h5py.File('sign_language_dataset_lzf.h5', 'r') as f:
                data = f['segments/data_types'][:1000]  # 1000ê°œ ì½ê¸°
            lzf_times.append(time.time() - start)
        
        avg_lzf_time = np.mean(lzf_times)
        
        print(f"ğŸš€ LZF ì••ì¶•:")
        print(f"   í‰ê·  ì½ê¸° ì‹œê°„ (1000ê°œ): {avg_lzf_time:.4f}ì´ˆ")
        print(f"   ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰: {1000/avg_lzf_time:.0f}ê°œ/ì´ˆ")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        import os
        file_size = os.path.getsize('sign_language_dataset_lzf.h5') / (1024**2)
        print(f"   íŒŒì¼ í¬ê¸°: {file_size:.2f}MB")
        
    except Exception as e:
        print(f"ì„±ëŠ¥ ë¹„êµ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    # 1. íŒŒì¼ êµ¬ì¡° ë¶„ì„
    test_lzf_file_structure()
    
    # 2. PyTorch DataLoader í…ŒìŠ¤íŠ¸
    test_pytorch_dataloader()
    
    # 3. ì„±ëŠ¥ ë¹„êµ
    performance_comparison()
    
    print("\n" + "ğŸ‰ ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!" + "\n" + "=" * 50)
