#!/usr/bin/env python3
"""
ì—£ì§€ ì„œë²„: ì›¹ìº  ì…ë ¥ â†’ YOLO ê²€ì¶œ â†’ ë°•ìŠ¤ í¬ë¡­ â†’ JPEG ìŠ¤íŠ¸ë¦¼ ì „ì†¡
"""

import cv2
import numpy as np
import requests
import time
import json
import argparse
from typing import List, Optional, Tuple
from pathlib import Path
import logging
from collections import deque

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    print("âš ï¸ ultralytics ë¯¸ì„¤ì¹˜ - pip install ultralytics")
    YOLO_AVAILABLE = False

# RTMW ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (streamlined_processor.pyì—ì„œ ê°€ì ¸ì˜´)
def bbox_xyxy2cs(bbox: np.ndarray, padding: float = 1.10) -> Tuple[np.ndarray, np.ndarray]:
    """ë°”ìš´ë”©ë°•ìŠ¤ë¥¼ center, scaleë¡œ ë³€í™˜"""
    dim = bbox.ndim
    if dim == 1:
        bbox = bbox[None, :]
    
    scale = (bbox[..., 2:] - bbox[..., :2]) * padding
    center = (bbox[..., 2:] + bbox[..., :2]) * 0.5
    
    if dim == 1:
        center = center[0]
        scale = scale[0]
    
    return center, scale

def _rotate_point(pt: np.ndarray, angle_rad: float) -> np.ndarray:
    """ì ì„ íšŒì „"""
    cos_val = np.cos(angle_rad)
    sin_val = np.sin(angle_rad)
    return np.array([pt[0] * cos_val - pt[1] * sin_val,
                     pt[0] * sin_val + pt[1] * cos_val])

def _get_3rd_point(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    """ì„¸ ë²ˆì§¸ ì ì„ ê³„ì‚° (ì§êµì )"""
    direction = a - b
    return b + np.array([-direction[1], direction[0]])

def get_warp_matrix(center: np.ndarray, scale: np.ndarray, rot: float, 
                   output_size: Tuple[int, int]) -> np.ndarray:
    """ì•„í•€ ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
    src_w, src_h = scale[:2]
    dst_w, dst_h = output_size[:2]
    
    rot_rad = np.deg2rad(rot)
    src_dir = _rotate_point(np.array([src_w * -0.5, 0.]), rot_rad)
    dst_dir = np.array([dst_w * -0.5, 0.])
    
    src = np.zeros((3, 2), dtype=np.float32)
    src[0, :] = center
    src[1, :] = center + src_dir
    
    dst = np.zeros((3, 2), dtype=np.float32)
    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]
    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir
    
    src[2, :] = _get_3rd_point(src[0, :], src[1, :])
    dst[2, :] = _get_3rd_point(dst[0, :], dst[1, :])
    
    warp_mat = cv2.getAffineTransform(src, dst)
    return warp_mat

def fix_aspect_ratio(bbox_scale: np.ndarray, aspect_ratio: float) -> np.ndarray:
    """bboxë¥¼ ê³ ì • ì¢…íš¡ë¹„ë¡œ ì¡°ì •"""
    w, h = bbox_scale[0], bbox_scale[1]
    if w > h * aspect_ratio:
        new_h = w / aspect_ratio
        bbox_scale = np.array([w, new_h])
    else:
        new_w = h * aspect_ratio
        bbox_scale = np.array([new_w, h])
    return bbox_scale

class EdgeYOLODetector:
    """ì—£ì§€ ë””ë°”ì´ìŠ¤ìš© YOLO ê²€ì¶œê¸°"""
    
    def __init__(self, 
                 yolo_model: str = "yolo11l.pt",
                 conf_thresh: float = 0.4,
                 iou_thresh: float = 0.6,
                 max_det: int = 50,
                 img_size: int = 832):
        
        if not YOLO_AVAILABLE:
            raise ImportError("ultralyticsê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install ultralytics")
        
        self.conf_thresh = conf_thresh
        self.iou_thresh = iou_thresh
        self.max_det = max_det
        self.img_size = img_size
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”§ YOLO11L ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = YOLO(yolo_model)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
        if hasattr(self.model.model, 'to'):
            try:
                import torch
                if torch.cuda.is_available():
                    self.model.to('cuda')
                    print(f"âœ… YOLO11L CUDA ëª¨ë“œ í™œì„±í™”")
                else:
                    print(f"âœ… YOLO11L CPU ëª¨ë“œ")
            except:
                print(f"âœ… YOLO11L CPU ëª¨ë“œ")
        
        print(f"âœ… YOLO11L ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def detect_persons(self, image: np.ndarray) -> List[List[float]]:
        """ì‚¬ëŒ ê²€ì¶œ"""
        try:
            results = self.model(
                image,
                conf=self.conf_thresh,
                iou=self.iou_thresh,
                max_det=self.max_det,
                classes=[0],  # ì‚¬ëŒ í´ë˜ìŠ¤ë§Œ
                verbose=False,
                imgsz=self.img_size
            )
            
            person_boxes = []
            for result in results:
                boxes = result.boxes
                if boxes is not None and len(boxes) > 0:
                    person_coords = boxes.xyxy
                    person_confs = boxes.conf
                    
                    # ì‹ ë¢°ë„ ì¬í•„í„°ë§
                    conf_mask = person_confs >= self.conf_thresh
                    if conf_mask.any():
                        filtered_boxes = person_coords[conf_mask]
                        filtered_confs = person_confs[conf_mask]
                        
                        # numpy ë³€í™˜
                        if hasattr(filtered_boxes, 'cpu'):
                            filtered_boxes = filtered_boxes.cpu().numpy()
                            filtered_confs = filtered_confs.cpu().numpy()
                        
                        # ì‹ ë¢°ë„ìˆœ ì •ë ¬í•˜ì—¬ ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ë°•ìŠ¤ë§Œ ì„ íƒ
                        best_idx = np.argmax(filtered_confs)
                        best_box = filtered_boxes[best_idx]
                        
                        person_boxes.append(best_box.tolist())
            
            return person_boxes
            
        except Exception as e:
            print(f"âŒ YOLO ê²€ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def crop_person_image_rtmw(self, image: np.ndarray, bbox: List[float]) -> Optional[np.ndarray]:
        """RTMW ë°©ì‹ìœ¼ë¡œ ì‚¬ëŒ ì´ë¯¸ì§€ í¬ë¡­"""
        try:
            # RTMW ì„¤ì •: width=288, height=384
            input_width, input_height = 288, 384
            
            # 1. bboxë¥¼ center, scaleë¡œ ë³€í™˜
            bbox_array = np.array(bbox, dtype=np.float32)
            center, scale = bbox_xyxy2cs(bbox_array)
            
            # 2. aspect ratio ê³ ì •
            aspect_ratio = input_width / input_height  # 0.75
            scale = fix_aspect_ratio(scale, aspect_ratio)
            
            # 3. ì•„í•€ ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            warp_mat = get_warp_matrix(
                center=center,
                scale=scale,
                rot=0.0,
                output_size=(input_width, input_height)
            )
            
            # 4. ì•„í•€ ë³€í™˜ ì ìš©
            cropped_image = cv2.warpAffine(
                image, 
                warp_mat, 
                (input_width, input_height), 
                flags=cv2.INTER_LINEAR
            )
            
            return cropped_image
                
        except Exception as e:
            print(f"âš ï¸ í¬ë¡­ ì‹¤íŒ¨: {e}")
            return None

class EdgeServer:
    """ì—£ì§€ ì„œë²„ - ì›¹ìº  ìº¡ì²˜ + YOLO ê²€ì¶œ + í¬ë¡­ + ì „ì†¡"""
    
    def __init__(self, 
                 pose_server_url: str,
                 camera_id: int = 0,
                 window_size: Tuple[int, int] = (1280, 720),
                 jpeg_quality: int = 85,
                 max_fps: int = 30):
        
        self.pose_server_url = pose_server_url.rstrip('/')
        self.camera_id = camera_id
        self.window_size = window_size
        self.jpeg_quality = jpeg_quality
        self.max_fps = max_fps
        
        # YOLO ê²€ì¶œê¸° ì´ˆê¸°í™”
        self.detector = EdgeYOLODetector()
        
        # ì›¹ìº  ì´ˆê¸°í™”
        self.cap = None
        self.init_camera()
        
        # ì„±ëŠ¥ ì¸¡ì •
        self.fps_history = deque(maxlen=30)
        self.frame_count = 0
        
        print(f"âœ… ì—£ì§€ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - í¬ì¦ˆ ì„œë²„: {self.pose_server_url}")
        print(f"   - ì¹´ë©”ë¼: {camera_id}")
        print(f"   - í•´ìƒë„: {window_size}")
        print(f"   - JPEG í’ˆì§ˆ: {jpeg_quality}%")
    
    def init_camera(self):
        """ì›¹ìº  ì´ˆê¸°í™”"""
        print(f"ğŸ“¹ ì›¹ìº  ì—°ê²° ì¤‘... (ì¹´ë©”ë¼ ID: {self.camera_id})")
        
        self.cap = cv2.VideoCapture(self.camera_id)
        if not self.cap.isOpened():
            raise RuntimeError(f"âŒ ì›¹ìº  ì—´ê¸° ì‹¤íŒ¨ (ì¹´ë©”ë¼ ID: {self.camera_id})")
        
        # ì›¹ìº  ì„¤ì •
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.window_size[0])
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.window_size[1])
        self.cap.set(cv2.CAP_PROP_FPS, self.max_fps)
        
        # ì‹¤ì œ ì›¹ìº  í•´ìƒë„ í™•ì¸
        actual_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = self.cap.get(cv2.CAP_PROP_FPS)
        
        print(f"âœ… ì›¹ìº  ì—°ê²° ì„±ê³µ: {actual_width}x{actual_height}, {actual_fps:.1f}fps")
    
    def send_crop_to_pose_server(self, crop_image: np.ndarray, bbox: List[float], frame_id: int) -> Optional[dict]:
        """í¬ë¡­ëœ ì´ë¯¸ì§€ë¥¼ í¬ì¦ˆ ì„œë²„ë¡œ ì „ì†¡"""
        try:
            # JPEG ì¸ì½”ë”©
            ret, jpeg_data = cv2.imencode('.jpg', crop_image, 
                                        [cv2.IMWRITE_JPEG_QUALITY, self.jpeg_quality])
            if not ret:
                return None
            
            # ìš”ì²­ ë°ì´í„° ì¤€ë¹„
            files = {'image': ('crop.jpg', jpeg_data.tobytes(), 'image/jpeg')}
            data = {
                'frame_id': frame_id,
                'bbox': json.dumps(bbox),
                'timestamp': time.time()
            }
            
            # POST ìš”ì²­
            response = requests.post(
                f"{self.pose_server_url}/estimate_pose",
                files=files,
                data=data,
                timeout=5.0
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âš ï¸ í¬ì¦ˆ ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                return None
                
        except requests.RequestException as e:
            print(f"âš ï¸ í¬ì¦ˆ ì„œë²„ í†µì‹  ì‹¤íŒ¨: {e}")
            return None
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")
            return None
    
    def transform_keypoints_to_original(self, keypoints: np.ndarray, bbox: List[float]) -> np.ndarray:
        """í¬ë¡­ ì¢Œí‘œ(288x384)ì˜ í‚¤í¬ì¸íŠ¸ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜"""
        try:
            # RTMWì™€ ë™ì¼í•œ ë³€í™˜ ê³¼ì •ì„ ì—­ë³€í™˜
            input_width, input_height = 288, 384
            
            # 1. bboxë¥¼ center, scaleë¡œ ë³€í™˜ (crop ì‹œì™€ ë™ì¼)
            bbox_array = np.array(bbox, dtype=np.float32)
            center, scale = bbox_xyxy2cs(bbox_array)
            
            # 2. aspect ratio ê³ ì • (crop ì‹œì™€ ë™ì¼)
            aspect_ratio = input_width / input_height  # 0.75
            scale = fix_aspect_ratio(scale, aspect_ratio)
            
            # 3. ì•„í•€ ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° (crop ì‹œì™€ ë™ì¼)
            warp_mat = get_warp_matrix(
                center=center,
                scale=scale,
                rot=0.0,
                output_size=(input_width, input_height)
            )
            
            # 4. ì—­ë³€í™˜ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            inv_warp_mat = cv2.invertAffineTransform(warp_mat)
            
            # 5. í‚¤í¬ì¸íŠ¸ë¥¼ homogeneous coordinatesë¡œ ë³€í™˜
            num_keypoints = keypoints.shape[0]
            kpts_homo = np.ones((num_keypoints, 3))
            kpts_homo[:, :2] = keypoints[:, :2]
            
            # 6. ì—­ë³€í™˜ ì ìš©
            original_keypoints = np.zeros_like(keypoints)
            for i in range(num_keypoints):
                transformed_pt = inv_warp_mat @ kpts_homo[i]
                original_keypoints[i, 0] = transformed_pt[0]
                original_keypoints[i, 1] = transformed_pt[1]
            
            return original_keypoints
            
        except Exception as e:
            print(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return keypoints  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    
    def visualize_results(self, image: np.ndarray, person_boxes: List[List[float]], 
                         pose_results: List[dict]) -> np.ndarray:
        """ê²°ê³¼ ì‹œê°í™”"""
        vis_image = image.copy()
        
        # ê²€ì¶œëœ ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        for i, bbox in enumerate(person_boxes):
            x1, y1, x2, y2 = map(int, bbox)
            color = (0, 255, 0) if i == 0 else (255, 0, 255)
            cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, 2)
            cv2.putText(vis_image, f"Person {i+1}", (x1, y1-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # í¬ì¦ˆ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‚¤í¬ì¸íŠ¸ ê·¸ë¦¬ê¸° (ì •í™•í•œ ì¢Œí‘œ ë³€í™˜)
        for i, pose_result in enumerate(pose_results):
            if pose_result and 'keypoints' in pose_result and i < len(person_boxes):
                keypoints = np.array(pose_result['keypoints'])
                scores = np.array(pose_result['scores'])
                bbox = person_boxes[i]
                
                # í¬ë¡­ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ì •í™•íˆ ë³€í™˜
                original_keypoints = self.transform_keypoints_to_original(keypoints, bbox)
                
                for j, (orig_kpt, score) in enumerate(zip(original_keypoints, scores)):
                    if score > 0.3:
                        x, y = int(orig_kpt[0]), int(orig_kpt[1])
                        
                        # ì´ë¯¸ì§€ ë²”ìœ„ ë‚´ì— ìˆëŠ” í‚¤í¬ì¸íŠ¸ë§Œ ê·¸ë¦¬ê¸°
                        if 0 <= x < vis_image.shape[1] and 0 <= y < vis_image.shape[0]:
                            if score > 0.8:
                                kpt_color = (0, 255, 0)    # ë†’ì€ ì‹ ë¢°ë„: ì´ˆë¡
                            elif score > 0.6:
                                kpt_color = (0, 255, 255)  # ì¤‘ê°„ ì‹ ë¢°ë„: ë…¸ë‘
                            else:
                                kpt_color = (0, 0, 255)    # ë‚®ì€ ì‹ ë¢°ë„: ë¹¨ê°•
                            
                            cv2.circle(vis_image, (x, y), 3, kpt_color, -1)
                            
                            # í‚¤í¬ì¸íŠ¸ ì¸ë±ìŠ¤ í‘œì‹œ (ë””ë²„ê¹…ìš©, í•„ìš”ì‹œ)
                            if j < 17:  # body keypointsë§Œ
                                cv2.putText(vis_image, str(j), (x+5, y), 
                                          cv2.FONT_HERSHEY_SIMPLEX, 0.3, kpt_color, 1)
        
        # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
        if self.fps_history:
            avg_fps = np.mean(self.fps_history)
            cv2.putText(vis_image, f"Edge FPS: {avg_fps:.1f}", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        cv2.putText(vis_image, f"YOLO11L Edge Server", 
                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(vis_image, f"Pose Server: {self.pose_server_url.split('://')[-1]}", 
                   (10, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        cv2.putText(vis_image, f"Detections: {'1 person' if person_boxes else 'No person'}", 
                   (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        return vis_image
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰ ë£¨í”„"""
        print(f"\nğŸš€ ì—£ì§€ ì„œë²„ ì‹¤í–‰ ì‹œì‘")
        print(f"ğŸ® ì¡°ì‘ë²•:")
        print(f"   - ESC: ì¢…ë£Œ")
        print(f"   - S: ìŠ¤í¬ë¦°ìƒ·")
        print(f"   - SPACE: ì¼ì‹œì •ì§€/ì¬ìƒ")
        
        paused = False
        screenshot_count = 0
        
        try:
            while True:
                if not paused:
                    ret, frame = self.cap.read()
                    if not ret:
                        print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                        break
                    
                    start_time = time.time()
                    
                    # 1. YOLO ê²€ì¶œ (ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ ë°•ìŠ¤ 1ê°œë§Œ)
                    person_boxes = self.detector.detect_persons(frame)
                    
                    # 2. ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ì‚¬ëŒì— ëŒ€í•´ í¬ë¡­ + í¬ì¦ˆ ì„œë²„ë¡œ ì „ì†¡
                    pose_results = []
                    if person_boxes:  # ê²€ì¶œëœ ë°•ìŠ¤ê°€ ìˆìœ¼ë©´
                        bbox = person_boxes[0]  # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ ë°•ìŠ¤
                        crop_image = self.detector.crop_person_image_rtmw(frame, bbox)
                        if crop_image is not None:
                            # í¬ì¦ˆ ì„œë²„ë¡œ ì „ì†¡
                            pose_result = self.send_crop_to_pose_server(
                                crop_image, bbox, self.frame_count
                            )
                            pose_results.append(pose_result)
                        else:
                            pose_results.append(None)
                    
                    # 3. FPS ê³„ì‚°
                    process_time = time.time() - start_time
                    fps = 1.0 / process_time if process_time > 0 else 0
                    self.fps_history.append(fps)
                    
                    # 4. ì‹œê°í™”
                    vis_frame = self.visualize_results(frame, person_boxes, pose_results)
                    self.frame_count += 1
                
                # í™”ë©´ í‘œì‹œ
                cv2.imshow('YOLO Edge Server', vis_frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                
                if key == 27:  # ESC
                    break
                elif key == ord('s') or key == ord('S'):  # ìŠ¤í¬ë¦°ìƒ·
                    screenshot_name = f"edge_screenshot_{screenshot_count:04d}.jpg"
                    cv2.imwrite(screenshot_name, vis_frame)
                    print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_name}")
                    screenshot_count += 1
                elif key == ord(' '):  # ì¼ì‹œì •ì§€/ì¬ìƒ
                    paused = not paused
                    print(f"â¸ï¸ {'ì¼ì‹œì •ì§€' if paused else 'ì¬ìƒ'}")
                
                # ì£¼ê¸°ì  í†µê³„ ì¶œë ¥
                if self.frame_count % 60 == 0 and self.frame_count > 0:
                    avg_fps = np.mean(self.fps_history) if self.fps_history else 0
                    detection_status = "ê²€ì¶œë¨" if person_boxes else "ë¯¸ê²€ì¶œ"
                    print(f"ğŸ“Š í”„ë ˆì„ {self.frame_count}: {avg_fps:.1f}fps, "
                          f"ì‚¬ëŒ {detection_status}")
                    
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ìê°€ ì—£ì§€ ì„œë²„ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        
        finally:
            self.cleanup()
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()
        
        if self.fps_history:
            avg_fps = np.mean(self.fps_history)
            print(f"\nğŸ“Š ì—£ì§€ ì„œë²„ ì™„ë£Œ:")
            print(f"   - ì²˜ë¦¬ëœ í”„ë ˆì„: {self.frame_count}")
            print(f"   - í‰ê·  FPS: {avg_fps:.1f}")

def main():
    parser = argparse.ArgumentParser(description="YOLO Edge Server")
    parser.add_argument("--pose-server", type=str, default="http://192.168.1.100:5000",
                       help="í¬ì¦ˆ ì„œë²„ URL (ê¸°ë³¸ê°’: http://192.168.1.100:5000)")
    parser.add_argument("--camera", type=int, default=0,
                       help="ì¹´ë©”ë¼ ID (ê¸°ë³¸ê°’: 0)")
    parser.add_argument("--width", type=int, default=1280,
                       help="ì›¹ìº  ê°€ë¡œ í•´ìƒë„ (ê¸°ë³¸ê°’: 1280)")
    parser.add_argument("--height", type=int, default=720,
                       help="ì›¹ìº  ì„¸ë¡œ í•´ìƒë„ (ê¸°ë³¸ê°’: 720)")
    parser.add_argument("--jpeg-quality", type=int, default=85,
                       help="JPEG ì „ì†¡ í’ˆì§ˆ (ê¸°ë³¸ê°’: 85)")
    parser.add_argument("--fps", type=int, default=30,
                       help="ìµœëŒ€ FPS (ê¸°ë³¸ê°’: 30)")
    
    args = parser.parse_args()
    
    try:
        edge_server = EdgeServer(
            pose_server_url=args.pose_server,
            camera_id=args.camera,
            window_size=(args.width, args.height),
            jpeg_quality=args.jpeg_quality,
            max_fps=args.fps
        )
        
        edge_server.run()
        
    except Exception as e:
        print(f"âŒ ì—£ì§€ ì„œë²„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()