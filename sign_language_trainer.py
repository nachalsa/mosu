#!/usr/bin/env python3
"""
ìˆ˜í™” ì¸ì‹ ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
import numpy as np
import json
import time
from pathlib import Path
from tqdm import tqdm
import logging
from typing import Dict, List, Tuple, Optional

from unified_pose_dataloader import create_dataloader
from device_utils import DeviceManager
from sign_language_model import SequenceToSequenceSignModel, RealtimeDecoder

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SignLanguageTrainer:
    """ìˆ˜í™” ì¸ì‹ ëª¨ë¸ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self,
                 model: nn.Module,
                 train_dataloader,
                 val_dataloader,
                 device: str = "auto",
                 checkpoint_dir: str = "./checkpoints",
                 log_dir: str = "./logs",
                 vocab_words: List[str] = None):
        
        self.model = model
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.vocab_words = vocab_words
        
        # ì¢…ë£Œ ì‹ í˜¸ í”Œë˜ê·¸
        self.shutdown_requested = False
        self.current_epoch = 0
        self.current_step = 0
        
        # ì¥ì¹˜ ì„¤ì • (ê°œì„ ëœ ë°©ì‹)
        self.device = DeviceManager.detect_best_device(device)
        device_info = DeviceManager.get_device_info(self.device)
        logger.info(f"ğŸš€ ë””ë°”ì´ìŠ¤ ì„¤ì •: {device_info.get('name', self.device)}")
        
        # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™”
        DeviceManager.optimize_for_device(self.device)
        
        self.vocab_words = vocab_words or []
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.checkpoint_dir = Path(checkpoint_dir)
        self.log_dir = Path(log_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.log_dir.mkdir(exist_ok=True)
        
        # TensorBoard
        self.writer = SummaryWriter(self.log_dir / f"train_{int(time.time())}")
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™
        self.model.to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=1e-4,
            weight_decay=1e-2,
            betas=(0.9, 0.98)
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            min_lr=1e-6
        )
        
        # í•™ìŠµ í†µê³„
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        self.epochs_without_improvement = 0
        self.early_stopping_patience = 15
        
        # ë©”íŠ¸ë¦­
        self.train_metrics = {
            'word_accuracy': [],
            'boundary_accuracy': [],
            'confidence_mae': []
        }
        self.val_metrics = {
            'word_accuracy': [],
            'boundary_accuracy': [],
            'confidence_mae': []
        }
        
        logger.info(f"ğŸ”§ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"   - ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.model.parameters()):,}")
        logger.info(f"   - í›ˆë ¨ ë°°ì¹˜: {len(self.train_dataloader)}")
        logger.info(f"   - ê²€ì¦ ë°°ì¹˜: {len(self.val_dataloader)}")
    
    def request_shutdown(self):
        """ì¢…ë£Œ ìš”ì²­"""
        logger.info("âš ï¸ ì•ˆì „í•œ ì¢…ë£Œê°€ ìš”ì²­ë˜ì—ˆìŠµë‹ˆë‹¤.")
        self.shutdown_requested = True
    
    def setup_training(self, 
                      learning_rate: float = 1e-4, 
                      weight_decay: float = 1e-2,
                      warmup_steps: int = 500,
                      gradient_clip_val: float = 1.0,
                      word_loss_weight: float = 1.0,
                      boundary_loss_weight: float = 0.5,
                      confidence_loss_weight: float = 0.3):
        """í•™ìŠµ ì„¤ì • ì—…ë°ì´íŠ¸"""
        logger.info(f"âš™ï¸ í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •:")
        logger.info(f"   - Learning Rate: {learning_rate}")
        logger.info(f"   - Weight Decay: {weight_decay}")
        logger.info(f"   - Warmup Steps: {warmup_steps}")
        logger.info(f"   - Gradient Clip: {gradient_clip_val}")
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„° ì €ì¥
        self.gradient_clip_val = gradient_clip_val
        self.word_loss_weight = word_loss_weight
        self.boundary_loss_weight = boundary_loss_weight
        self.confidence_loss_weight = confidence_loss_weight
        
        # ì˜µí‹°ë§ˆì´ì € ì¬ì„¤ì •
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
            betas=(0.9, 0.98)
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ì„¤ì •
        self.scheduler = ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            min_lr=1e-6
        )
    
    def calculate_metrics(self, outputs: Dict, targets: Dict, vocab_masks: torch.Tensor) -> Dict:
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        metrics = {}
        
        # ë‹¨ì–´ ì •í™•ë„ ê³„ì‚°
        if 'word_logits' in outputs and 'vocab_ids' in targets:
            word_logits = outputs['word_logits']  # [batch, vocab_len, vocab_size]
            target_ids = targets['vocab_ids']  # [batch, vocab_len]
            
            # ì˜ˆì¸¡ê°’ ê³„ì‚°
            predicted_ids = torch.argmax(word_logits, dim=-1)
            
            # ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ì—ì„œë§Œ ì •í™•ë„ ê³„ì‚°
            correct_predictions = (predicted_ids == target_ids) & vocab_masks
            word_accuracy = correct_predictions.sum().float() / vocab_masks.sum().float()
            metrics['word_accuracy'] = word_accuracy.item()
        
        # ê²½ê³„ ì •í™•ë„ ê³„ì‚°
        if 'boundary_logits' in outputs and 'boundary_labels' in targets:
            boundary_logits = outputs['boundary_logits']  # [batch, vocab_len, 3]
            boundary_labels = targets['boundary_labels']  # [batch, vocab_len]
            
            predicted_boundaries = torch.argmax(boundary_logits, dim=-1)
            boundary_correct = (predicted_boundaries == boundary_labels) & vocab_masks
            boundary_accuracy = boundary_correct.sum().float() / vocab_masks.sum().float()
            metrics['boundary_accuracy'] = boundary_accuracy.item()
        
        # ì‹ ë¢°ë„ MAE ê³„ì‚°
        if 'confidence_scores' in outputs and 'confidence_targets' in targets:
            confidence_scores = outputs['confidence_scores']  # [batch, vocab_len, 1]
            confidence_targets = targets['confidence_targets']  # [batch, vocab_len, 1]
            
            # ìœ íš¨í•œ ìœ„ì¹˜ì—ì„œë§Œ MAE ê³„ì‚°
            masked_diff = torch.abs(confidence_scores - confidence_targets)
            confidence_mae = masked_diff[vocab_masks.unsqueeze(-1)].mean()
            metrics['confidence_mae'] = confidence_mae.item()
        
        return metrics
        
    def create_boundary_labels(self, vocab_ids, vocab_masks):
        """ê²½ê³„ ë¼ë²¨ ìƒì„± (ë‹¨ìˆœ ë²„ì „)"""
        batch_size, vocab_len = vocab_ids.shape
        boundary_labels = torch.zeros(batch_size, vocab_len, dtype=torch.long, device=vocab_ids.device)
        
        for i in range(batch_size):
            mask = vocab_masks[i]
            valid_len = mask.sum().item()
            
            if valid_len > 0:
                boundary_labels[i, 0] = 0  # START
                boundary_labels[i, 1:valid_len-1] = 1  # CONTINUE
                if valid_len > 1:
                    boundary_labels[i, valid_len-1] = 2  # END
        
        return boundary_labels
    
    def compute_metrics(self, outputs, targets, masks):
        """ë©”íŠ¸ë¦­ ê³„ì‚° (í†µí•© ë²„ì „)"""
        vocab_masks = masks.get('vocab_masks', masks)  # í˜¸í™˜ì„±ì„ ìœ„í•´
        return self.calculate_metrics(outputs, targets, vocab_masks)
        word_correct = (word_preds == vocab_ids) & vocab_masks
        word_accuracy = word_correct.sum().float() / vocab_masks.sum().float()
        metrics['word_accuracy'] = word_accuracy.item()
        
        # ê²½ê³„ ì •í™•ë„ (ìˆëŠ” ê²½ìš°)
        if 'boundary_logits' in outputs and 'boundary_labels' in targets:
            boundary_logits = outputs['boundary_logits']
            boundary_labels = targets['boundary_labels']
            
            boundary_preds = torch.argmax(boundary_logits, dim=-1)
            boundary_correct = (boundary_preds == boundary_labels) & vocab_masks
            boundary_accuracy = boundary_correct.sum().float() / vocab_masks.sum().float()
            metrics['boundary_accuracy'] = boundary_accuracy.item()
        
        # ì‹ ë¢°ë„ MAE (ìˆëŠ” ê²½ìš°)
        if 'confidence_scores' in outputs and 'confidence_targets' in targets:
            confidence_scores = outputs['confidence_scores']
            confidence_targets = targets['confidence_targets']
            
            confidence_mae = torch.abs(confidence_scores - confidence_targets)[vocab_masks].mean()
            metrics['confidence_mae'] = confidence_mae.item()
        
        return metrics
    
    def train_epoch(self, epoch: int, save_every_n_steps: int = 500, log_every_n_steps: int = 100):
        """í•œ ì—í¬í¬ í›ˆë ¨ (ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬ í¬í•¨)"""
        self.model.train()
        total_loss = 0
        total_metrics = {key: 0 for key in self.train_metrics.keys()}
        
        pbar = tqdm(self.train_dataloader, desc=f"í›ˆë ¨ Epoch {epoch}")
        
        for batch_idx, batch in enumerate(pbar):
            # ì¢…ë£Œ ìš”ì²­ í™•ì¸
            if self.shutdown_requested:
                logger.info(f"âš ï¸ ì¢…ë£Œ ìš”ì²­ ê°ì§€ - í˜„ì¬ ë°°ì¹˜ ì™„ë£Œ í›„ ì•ˆì „ ì¢…ë£Œ (ë°°ì¹˜ {batch_idx}/{len(self.train_dataloader)})")
                break
            
            self.current_step += 1
            
            try:
                # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
                pose_features = batch['pose_features'].to(self.device)
                vocab_ids = batch['vocab_ids'].to(self.device)
                frame_masks = batch['frame_masks'].to(self.device)
                vocab_masks = batch['vocab_masks'].to(self.device)
                
                # ê²½ê³„ ë¼ë²¨ ìƒì„±
                boundary_labels = self.create_boundary_labels(vocab_ids, vocab_masks)
                
                # í¬ì›Œë“œ íŒ¨ìŠ¤
                outputs = self.model(
                    pose_features=pose_features,
                    vocab_ids=vocab_ids,
                    frame_masks=frame_masks,
                    vocab_masks=vocab_masks
                )
                
                # íƒ€ê²Ÿ ì¤€ë¹„
                targets = {
                    'vocab_ids': vocab_ids,
                    'boundary_labels': boundary_labels,
                    # 'confidence_targets': None  # ì‹¤ì œ ì‹ ë¢°ë„ íƒ€ê²Ÿì´ ìˆë‹¤ë©´ ì¶”ê°€
                }
                
                # ì†ì‹¤ ê³„ì‚°
                losses = self.model.compute_loss(outputs, targets, vocab_masks)
                loss = losses['total_loss']
                
                # ë°±í”„ë¡­
                self.optimizer.zero_grad()
                loss.backward()
                
                # ê·¸ë¼ë””ì–¸íŠ¸ í´ë¦¬í•‘
                if hasattr(self, 'gradient_clip_val'):
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_val)
                
                self.optimizer.step()
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                total_loss += loss.item()
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                metrics = self.compute_metrics(outputs, targets, {'vocab_masks': vocab_masks})
                for key, value in metrics.items():
                    if key in total_metrics:
                        total_metrics[key] += value
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                avg_loss = total_loss / (batch_idx + 1)
                pbar.set_postfix({
                    'Loss': f'{avg_loss:.4f}',
                    'Word_Acc': f'{total_metrics["word_accuracy"]/(batch_idx+1):.3f}',
                    'LR': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
                })
                
                # ì£¼ê¸°ì  ë¡œê¹…
                if self.current_step % log_every_n_steps == 0:
                    self.writer.add_scalar('Train/Loss_Step', loss.item(), self.current_step)
                    self.writer.add_scalar('Train/LR', self.optimizer.param_groups[0]['lr'], self.current_step)
                
                # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì„ íƒì ìœ¼ë¡œ ìŠ¤í… ë‹¨ìœ„ ì €ì¥)
                # ì—í¬í¬ ë‹¨ìœ„ ì €ì¥ì„ ìš°ì„ í•˜ë¯€ë¡œ ìŠ¤í… ë‹¨ìœ„ ì €ì¥ì€ ë¹„í™œì„±í™”
                # if self.current_step % save_every_n_steps == 0:
                #     logger.info(f"ğŸ’¾ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ìŠ¤í… {self.current_step})")
                #     self.save_checkpoint(epoch, f"step_{self.current_step}")
                
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ë°°ì¹˜ë¡œ ì§„í–‰
        
        # ì—í¬í¬ í‰ê·  ê³„ì‚°
        if len(self.train_dataloader) > 0:
            avg_loss = total_loss / len(self.train_dataloader)
            avg_metrics = {key: value / len(self.train_dataloader) 
                          for key, value in total_metrics.items()}
        else:
            avg_loss = 0
            avg_metrics = {key: 0 for key in total_metrics.keys()}
        
        # í†µê³„ ì €ì¥
        self.train_losses.append(avg_loss)
        for key, value in avg_metrics.items():
            if key in self.train_metrics:
                self.train_metrics[key].append(value)
        
        return avg_loss, avg_metrics
    
    @torch.no_grad()
    def validate_epoch(self, epoch: int):
        """í•œ ì—í¬í¬ ê²€ì¦"""
        self.model.eval()
        total_loss = 0
        total_metrics = {key: 0 for key in self.val_metrics.keys()}
        
        pbar = tqdm(self.val_dataloader, desc=f"ê²€ì¦ Epoch {epoch}")
        
        for batch in pbar:
            # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
            pose_features = batch['pose_features'].to(self.device)
            vocab_ids = batch['vocab_ids'].to(self.device)
            frame_masks = batch['frame_masks'].to(self.device)
            vocab_masks = batch['vocab_masks'].to(self.device)
            
            # ê²½ê³„ ë¼ë²¨ ìƒì„±
            boundary_labels = self.create_boundary_labels(vocab_ids, vocab_masks)
            
            # ê²€ì¦ ì‹œì—ë„ í›ˆë ¨ ëª¨ë“œì™€ ë™ì¼í•œ ì¶œë ¥ì„ ìœ„í•´ training í”Œë˜ê·¸ ì„ì‹œ ì„¤ì •
            self.model.train()
            outputs = self.model(
                pose_features=pose_features,
                vocab_ids=vocab_ids,
                frame_masks=frame_masks,
                vocab_masks=vocab_masks
            )
            self.model.eval()
            
            # íƒ€ê²Ÿ ì¤€ë¹„
            targets = {
                'vocab_ids': vocab_ids,
                'boundary_labels': boundary_labels,
            }
            
            # ì†ì‹¤ ê³„ì‚°
            losses = self.model.compute_loss(outputs, targets, vocab_masks)
            loss = losses['total_loss']
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_loss += loss.item()
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            masks = {'vocab_masks': vocab_masks}
            batch_metrics = self.compute_metrics(outputs, targets, masks)
            for key, value in batch_metrics.items():
                if key in total_metrics:
                    total_metrics[key] += value
            
            # Progress bar ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'Val Loss': f"{loss.item():.4f}",
                'Word Acc': f"{batch_metrics.get('word_accuracy', 0):.3f}"
            })
        
        # ì—í¬í¬ í‰ê·  ê³„ì‚°
        avg_loss = total_loss / len(self.val_dataloader)
        avg_metrics = {key: value / len(self.val_dataloader) 
                      for key, value in total_metrics.items()}
        
        # í†µê³„ ì €ì¥
        self.val_losses.append(avg_loss)
        for key, value in avg_metrics.items():
            self.val_metrics[key].append(value)
        
        # TensorBoard ë¡œê¹…
        self.writer.add_scalar('Val/Epoch_Loss', avg_loss, epoch)
        for key, value in avg_metrics.items():
            self.writer.add_scalar(f'Val/Epoch_{key}', value, epoch)
        
        return avg_loss, avg_metrics
    
    def save_checkpoint(self, epoch: int, is_best: bool = False, suffix: str = None):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics,
            'best_val_loss': self.best_val_loss,
            'vocab_words': self.vocab_words
        }
        
        # ì¼ë°˜ ì²´í¬í¬ì¸íŠ¸ (suffixê°€ ìˆìœ¼ë©´ ì¶”ê°€)
        if suffix:
            checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}_{suffix}.pt"
        else:
            checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸
        if is_best:
            best_path = self.checkpoint_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
            logger.info(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_path}")
        
        # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ (suffixê°€ ì—†ì„ ë•Œë§Œ)
        if not suffix:
            latest_path = self.checkpoint_dir / "latest_model.pt"
            torch.save(checkpoint, latest_path)
        
        return checkpoint_path
    
    def load_checkpoint(self, checkpoint_path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        self.train_metrics = checkpoint['train_metrics']
        self.val_metrics = checkpoint['val_metrics']
        self.best_val_loss = checkpoint['best_val_loss']
        
        start_epoch = checkpoint['epoch'] + 1
        logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: Epoch {start_epoch-1}")
        
        return start_epoch
    
    def train(self, num_epochs: int, resume_from: str = None, 
              save_every_n_steps: int = 500, log_every_n_steps: int = 100):
        """ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ (ì¢…ë£Œ ì‹ í˜¸ ì§€ì›)"""
        start_epoch = 0
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
        if resume_from:
            start_epoch = self.load_checkpoint(resume_from)
        
        logger.info(f"ğŸš€ í•™ìŠµ ì‹œì‘: Epoch {start_epoch} â†’ {num_epochs}")
        logger.info("=" * 60)
        
        try:
            for epoch in range(start_epoch, num_epochs):
                self.current_epoch = epoch
                
                # ì¢…ë£Œ ìš”ì²­ í™•ì¸
                if self.shutdown_requested:
                    logger.info("âš ï¸ ì¢…ë£Œ ìš”ì²­ìœ¼ë¡œ ì¸í•œ í•™ìŠµ ì¤‘ë‹¨")
                    break
                
                epoch_start_time = time.time()
                
                # í›ˆë ¨
                train_loss, train_metrics = self.train_epoch(
                    epoch, save_every_n_steps, log_every_n_steps
                )
                
                # ì¡°ê¸° ì¢…ë£Œ í™•ì¸
                if self.shutdown_requested:
                    logger.info("âš ï¸ ì—í¬í¬ ì¤‘ ì¢…ë£Œ ìš”ì²­ - í˜„ì¬ ìƒíƒœ ì €ì¥ í›„ ì¢…ë£Œ")
                    self.save_checkpoint(epoch, is_best=False, suffix="emergency_save")
                    break
                
                # ê²€ì¦
                val_loss, val_metrics = self.validate_epoch(epoch)
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                self.scheduler.step(val_loss)
                
                # ë¡œê¹…
                epoch_time = time.time() - epoch_start_time
                logger.info(f"Epoch {epoch:3d}/{num_epochs-1} ({epoch_time:.1f}s)")
                logger.info(f"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
                logger.info(f"  Train Acc: {train_metrics.get('word_accuracy', 0):.3f} | "
                           f"Val Acc: {val_metrics.get('word_accuracy', 0):.3f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬
                is_best = val_loss < self.best_val_loss
                if is_best:
                    self.best_val_loss = val_loss
                    self.epochs_without_improvement = 0
                    logger.info(f"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! Val Loss: {val_loss:.4f}")
                else:
                    self.epochs_without_improvement += 1
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë§¤ ì—í¬í¬ë§ˆë‹¤ ì €ì¥)
                self.save_checkpoint(epoch, is_best)
                
                # ì–¼ë¦¬ ìŠ¤í† í•‘
                if self.epochs_without_improvement >= self.early_stopping_patience:
                    logger.info(f"â¹ï¸ ì–¼ë¦¬ ìŠ¤í† í•‘: {self.early_stopping_patience} ì—í¬í¬ ê°œì„  ì—†ìŒ")
                    break
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                logger.info("-" * 60)
                
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì‘ê¸‰ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            self.save_checkpoint(self.current_epoch, is_best=False, suffix="error_checkpoint")
            raise
        
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if hasattr(self, 'writer'):
                self.writer.close()
                logger.info("ğŸ“ TensorBoard writer ì¢…ë£Œ")
            
            if self.shutdown_requested:
                logger.info("âœ… ì•ˆì „í•œ ì¢…ë£Œ ì™„ë£Œ")
            else:
                logger.info("ğŸ‰ ì •ìƒ í•™ìŠµ ì™„ë£Œ")
        
        # í•™ìŠµ ì™„ë£Œ
        logger.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        self.writer.close()
        
        return self.best_val_loss

def main():
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    print("ğŸš€ ìˆ˜í™” ì¸ì‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 50)
    
    # ì„¤ì •
    config = {
        'batch_size': 8,
        'num_epochs': 100,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'annotation_path': "./data/sign_language_dataset_only_sen_lzf.h5",
        'pose_data_dir': "./data",
        'sequence_length': 200,
        'min_segment_length': 20,
        'max_segment_length': 300,
        'train_split': 0.8,
        'resume_from': None  # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (í•„ìš”ì‹œ)
    }
    
    print(f"âš™ï¸ ì„¤ì •:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    try:
        # ë°ì´í„° ë¡œë” ìƒì„±
        print(f"\nğŸ“Š ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
        full_dataloader, dataset = create_dataloader(
            annotation_path=config['annotation_path'],
            pose_data_dir=config['pose_data_dir'],
            batch_size=config['batch_size'],
            sequence_length=config['sequence_length'],
            min_segment_length=config['min_segment_length'],
            max_segment_length=config['max_segment_length'],
            shuffle=True
        )
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        dataset_size = len(dataset)
        train_size = int(config['train_split'] * dataset_size)
        val_size = dataset_size - train_size
        
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        train_dataloader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=config['batch_size'],
            shuffle=True,
            num_workers=2,
            collate_fn=full_dataloader.collate_fn,
            pin_memory=True
        )
        
        val_dataloader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=config['batch_size'],
            shuffle=False,
            num_workers=2,
            collate_fn=full_dataloader.collate_fn,
            pin_memory=True
        )
        
        print(f"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
        print(f"   ì´ ë°ì´í„°: {dataset_size}")
        print(f"   í›ˆë ¨ ë°ì´í„°: {train_size}")
        print(f"   ê²€ì¦ ë°ì´í„°: {val_size}")
        print(f"   Vocabulary í¬ê¸°: {dataset.vocab_size}")
        
        # ëª¨ë¸ ìƒì„±
        print(f"\nğŸ¤– ëª¨ë¸ ìƒì„± ì¤‘...")
        model = SequenceToSequenceSignModel(
            vocab_size=dataset.vocab_size,
            embed_dim=256,
            num_encoder_layers=6,
            num_decoder_layers=4,
            num_heads=8,
            dim_feedforward=1024,
            dropout=0.1
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = SignLanguageTrainer(
            model=model,
            train_dataloader=train_dataloader,
            val_dataloader=val_dataloader,
            device=config['device'],
            vocab_words=dataset.words
        )
        
        # í•™ìŠµ ì‹œì‘
        print(f"\nğŸ‹ï¸ í•™ìŠµ ì‹œì‘...")
        best_loss = trainer.train(
            num_epochs=config['num_epochs'],
            resume_from=config['resume_from']
        )
        
        print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print(f"   ìµœê³  ì„±ëŠ¥: {best_loss:.4f}")
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
