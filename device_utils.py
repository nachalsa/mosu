"""
ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ì„¤ì • ìœ í‹¸ë¦¬í‹°
"""
import torch
import logging

logger = logging.getLogger(__name__)

class DeviceManager:
    """í†µí•© ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì"""
    
    @staticmethod
    def detect_best_device(preferred_device: str = "auto", multi_gpu: bool = False) -> torch.device:
        """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        
        Args:
            preferred_device: ì„ í˜¸ ë””ë°”ì´ìŠ¤ ("auto", "cuda", "xpu", "cpu")
            multi_gpu: ë©€í‹° GPU ì‚¬ìš© ì—¬ë¶€
        """
        
        if preferred_device != "auto":
            # ì‚¬ìš©ì ì§€ì • ë””ë°”ì´ìŠ¤ ê²€ì¦
            device = DeviceManager._validate_device(preferred_device)
            if multi_gpu and device.type == "cuda" and torch.cuda.device_count() > 1:
                logger.info(f"ğŸš€ ë©€í‹° GPU ëª¨ë“œ: {torch.cuda.device_count()}ê°œ CUDA ë””ë°”ì´ìŠ¤")
            return device
        
        # ìë™ ê°ì§€ ìˆœì„œ: CUDA (ë©€í‹° GPU í¬í•¨) > XPU > CPU
        logger.info("ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        
        # 1. CUDA í™•ì¸ (ë©€í‹° GPU ìš°ì„ )
        cuda_device = DeviceManager._check_cuda(multi_gpu)
        if cuda_device:
            return cuda_device
            
        # 2. XPU í™•ì¸ (Intel GPU)
        xpu_device = DeviceManager._check_xpu()
        if xpu_device:
            return xpu_device
        
        # 3. CPU í´ë°±
        logger.info("ğŸ’» CPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©")
        return torch.device("cpu")
    
    @staticmethod
    def _check_xpu() -> torch.device:
        """XPU ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            # XPU ëª¨ë“ˆ ì¡´ì¬ í™•ì¸
            if not hasattr(torch, 'xpu'):
                logger.debug("XPU ëª¨ë“ˆ ë¯¸ì§€ì›")
                return None
            
            # device_count() ë°©ì‹ìœ¼ë¡œ í™•ì¸
            try:
                device_count = torch.xpu.device_count()
                if device_count > 0:
                    logger.info(f"âš¡ XPU ë””ë°”ì´ìŠ¤ ê°ì§€: {device_count}ê°œ ë””ë°”ì´ìŠ¤")
                    # ì¶”ê°€ ê²€ì¦: ì‹¤ì œ í…ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
                    test_tensor = torch.tensor([1.0], device='xpu:0')
                    logger.info(f"âœ… XPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {test_tensor.device}")
                    return torch.device("xpu:0")
            except Exception as e:
                logger.debug(f"XPU device_count() ì‹¤íŒ¨: {e}")
            
            # is_available() ë°©ì‹ìœ¼ë¡œ í™•ì¸ (ìˆëŠ” ê²½ìš°)
            try:
                if hasattr(torch.xpu, 'is_available') and torch.xpu.is_available():
                    test_tensor = torch.tensor([1.0], device='xpu')
                    logger.info(f"âš¡ XPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {test_tensor.device}")
                    return torch.device("xpu")
            except Exception as e:
                logger.debug(f"XPU is_available() ì‹¤íŒ¨: {e}")
            
            # ì§ì ‘ í…ŒìŠ¤íŠ¸ ë°©ì‹
            try:
                test_tensor = torch.tensor([1.0], device='xpu')
                logger.info(f"âš¡ XPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {test_tensor.device}")
                return torch.device("xpu")
            except Exception as e:
                logger.debug(f"XPU ì§ì ‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            logger.debug(f"XPU ì „ì²´ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        return None
    
    @staticmethod
    def _check_cuda(multi_gpu: bool = False) -> torch.device:
        """CUDA ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            if torch.cuda.is_available():
                device_count = torch.cuda.device_count()
                current_device = torch.cuda.current_device()
                device_name = torch.cuda.get_device_name(current_device)
                
                if multi_gpu and device_count > 1:
                    logger.info(f"ï¿½ ë©€í‹° GPU CUDA ì‚¬ìš©: {device_count}ê°œ ë””ë°”ì´ìŠ¤")
                    for i in range(device_count):
                        gpu_name = torch.cuda.get_device_name(i)
                        logger.info(f"  GPU {i}: {gpu_name}")
                    return torch.device("cuda")  # ë©”ì¸ ë””ë°”ì´ìŠ¤
                else:
                    logger.info(f"ğŸ”¥ ë‹¨ì¼ CUDA ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {device_name}")
                    return torch.device(f"cuda:{current_device}")
        except Exception as e:
            logger.debug(f"CUDA í™•ì¸ ì‹¤íŒ¨: {e}")
        
        return None
    
    @staticmethod
    def _validate_device(device_str: str) -> torch.device:
        """ì‚¬ìš©ì ì§€ì • ë””ë°”ì´ìŠ¤ ê²€ì¦"""
        try:
            device = torch.device(device_str)
            
            if device.type == "cuda":
                if not torch.cuda.is_available():
                    logger.warning("âš ï¸ CUDA ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í´ë°±")
                    return torch.device("cpu")
                logger.info(f"ğŸ”¥ ì§€ì •ëœ CUDA ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {device}")
                return device
            
            elif device.type == "xpu":
                xpu_device = DeviceManager._check_xpu()
                if xpu_device is None:
                    logger.warning("âš ï¸ XPU ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í´ë°±")
                    return torch.device("cpu")
                logger.info(f"âš¡ ì§€ì •ëœ XPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©: {device}")
                return device
            
            elif device.type == "cpu":
                logger.info("ğŸ’» ì§€ì •ëœ CPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©")
                return device
            
            else:
                logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë””ë°”ì´ìŠ¤ íƒ€ì…: {device} - CPUë¡œ í´ë°±")
                return torch.device("cpu")
                
        except Exception as e:
            logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {e} - CPUë¡œ í´ë°±")
            return torch.device("cpu")
    
    @staticmethod
    def get_device_info(device: torch.device) -> dict:
        """ë””ë°”ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        info = {
            'type': device.type,
            'index': device.index,
            'name': str(device)
        }
        
        try:
            if device.type == "cuda":
                info.update({
                    'name': torch.cuda.get_device_name(device),
                    'memory_total': torch.cuda.get_device_properties(device).total_memory,
                    'memory_cached': torch.cuda.memory_cached(device),
                    'memory_allocated': torch.cuda.memory_allocated(device)
                })
            elif device.type == "xpu":
                # XPU ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
                try:
                    if hasattr(torch.xpu, 'get_device_name'):
                        info['name'] = torch.xpu.get_device_name(device.index)
                except:
                    pass
            elif device.type == "cpu":
                import os
                info.update({
                    'cores': os.cpu_count(),
                    'threads': torch.get_num_threads()
                })
        except Exception as e:
            logger.debug(f"ë””ë°”ì´ìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return info
    
    @staticmethod
    def optimize_for_device(device: torch.device, multi_gpu: bool = False):
        """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •"""
        if device.type == "cuda":
            # CUDA ìµœì í™”
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.enabled = True
            if multi_gpu and torch.cuda.device_count() > 1:
                logger.info(f"ğŸ”¥ ë©€í‹° GPU CUDA ìµœì í™” ì„¤ì • ì™„ë£Œ ({torch.cuda.device_count()}ê°œ GPU)")
            else:
                logger.info("ğŸ”¥ ë‹¨ì¼ GPU CUDA ìµœì í™” ì„¤ì • ì™„ë£Œ")
            
        elif device.type == "xpu":
            # XPU ìµœì í™” (ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                # XPU ê´€ë ¨ ìµœì í™” ì„¤ì •
                logger.info("âš¡ XPU ìµœì í™” ì„¤ì • ì™„ë£Œ")
            except:
                pass
                
        elif device.type == "cpu":
            # CPU ìµœì í™”
            torch.set_num_threads(min(8, torch.get_num_threads()))
            logger.info("ğŸ’» CPU ìµœì í™” ì„¤ì • ì™„ë£Œ")
    
    @staticmethod
    def setup_multi_gpu(model: torch.nn.Module, device: torch.device, 
                       use_data_parallel: bool = True) -> torch.nn.Module:
        """ë©€í‹° GPU ì„¤ì •
        
        Args:
            model: PyTorch ëª¨ë¸
            device: ë©”ì¸ ë””ë°”ì´ìŠ¤
            use_data_parallel: DataParallel ì‚¬ìš© ì—¬ë¶€ (Falseë©´ ìˆ˜ë™ ê´€ë¦¬)
        
        Returns:
            ë©€í‹° GPUê°€ ì„¤ì •ëœ ëª¨ë¸
        """
        if device.type == "cuda" and torch.cuda.device_count() > 1:
            if use_data_parallel:
                model = torch.nn.DataParallel(model)
                logger.info(f"ğŸš€ DataParallel ì„¤ì • ì™„ë£Œ: {torch.cuda.device_count()}ê°œ GPU ì‚¬ìš©")
            else:
                logger.info(f"ğŸš€ ë©€í‹° GPU í™˜ê²½ ê°ì§€ë¨: {torch.cuda.device_count()}ê°œ GPU (ìˆ˜ë™ ê´€ë¦¬)")
        
        return model.to(device)
    
    @staticmethod
    def get_effective_batch_size(base_batch_size: int, device: torch.device) -> int:
        """ë©€í‹° GPU í™˜ê²½ì— ë§ëŠ” íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if device.type == "cuda" and torch.cuda.device_count() > 1:
            gpu_count = torch.cuda.device_count()
            # ê° GPUë‹¹ ë°°ì¹˜ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì•„ì§€ì§€ ì•Šë„ë¡ ì¡°ì •
            effective_batch_size = max(base_batch_size, gpu_count * 4)  # ìµœì†Œ GPUë‹¹ 4
            logger.info(f"ğŸ“Š ë©€í‹° GPU ë°°ì¹˜ í¬ê¸° ì¡°ì •: {base_batch_size} â†’ {effective_batch_size} "
                       f"(GPUë‹¹ ~{effective_batch_size // gpu_count})")
            return effective_batch_size
        return base_batch_size
    
    @staticmethod
    def is_multi_gpu_available() -> bool:
        """ë©€í‹° GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return torch.cuda.is_available() and torch.cuda.device_count() > 1

def get_device_string(device: torch.device) -> str:
    """ë””ë°”ì´ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if device.index is not None:
        return f"{device.type}:{device.index}"
    return device.type

# í¸ì˜ í•¨ìˆ˜ë“¤
def auto_device(preferred: str = "auto", multi_gpu: bool = False) -> torch.device:
    """ìë™ ë””ë°”ì´ìŠ¤ ì„ íƒ"""
    return DeviceManager.detect_best_device(preferred, multi_gpu)

def device_info(device: torch.device = None) -> dict:
    """ë””ë°”ì´ìŠ¤ ì •ë³´"""
    if device is None:
        device = auto_device()
    return DeviceManager.get_device_info(device)

def optimize_device(device: torch.device = None, multi_gpu: bool = False):
    """ë””ë°”ì´ìŠ¤ ìµœì í™”"""
    if device is None:
        device = auto_device(multi_gpu=multi_gpu)
    DeviceManager.optimize_for_device(device, multi_gpu)

def setup_multi_gpu(model: torch.nn.Module, device: torch.device = None, 
                   use_data_parallel: bool = True) -> torch.nn.Module:
    """ë©€í‹° GPU ì„¤ì • í¸ì˜ í•¨ìˆ˜"""
    if device is None:
        device = auto_device(multi_gpu=True)
    return DeviceManager.setup_multi_gpu(model, device, use_data_parallel)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.INFO)
    
    print("=== ë””ë°”ì´ìŠ¤ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ ===")
    
    # ìë™ ê°ì§€
    device = auto_device()
    print(f"ê°ì§€ëœ ë””ë°”ì´ìŠ¤: {device}")
    
    # ì •ë³´ ì¶œë ¥
    info = device_info(device)
    print(f"ë””ë°”ì´ìŠ¤ ì •ë³´: {info}")
    
    # ìµœì í™” ì ìš©
    optimize_device(device)
    
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
