import torch
import torch.nn as nn
import torch.nn.functional as F
import subprocess
import os
import json
from pathlib import Path
import time

class HailoOperationTester:
    """
    Hailo AI ê°€ì†ê¸°ì—ì„œ ë‹¤ì–‘í•œ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ì—°ì‚°ë“¤ì´ ì œëŒ€ë¡œ ì§€ì›ë˜ê³  ì»´íŒŒì¼ë˜ëŠ”ì§€
    ì²´ê³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ëŠ” í´ë˜ìŠ¤.

    ê° ì—°ì‚°ì— íŠ¹í™”ëœ ì‘ì€ PyTorch ëª¨ë¸ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ ONNXë¡œ ë³€í™˜í•œ ë’¤,
    Hailo ì»´íŒŒì¼ëŸ¬ë¥¼ í†µí•´ íŒŒì‹±, ìµœì í™”, ì»´íŒŒì¼ ë‹¨ê³„ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, output_dir="hailo_test_results"):
        """
        í…ŒìŠ¤í„° ì´ˆê¸°í™”. ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

        Args:
            output_dir (str): ëª¨ë“  í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì €ì¥ë  ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ì´ë¦„.
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True) # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        
        self.models_dir = self.output_dir / "models"
        self.models_dir.mkdir(exist_ok=True) # ONNX ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        
        self.results_dir = self.output_dir / "results"
        self.results_dir.mkdir(exist_ok=True) # Hailo ì»´íŒŒì¼ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        
        self.test_results = {} # ê° ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        
    def create_test_models(self):
        """
        ë‹¤ì–‘í•œ ì—°ì‚°ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ PyTorch ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ê° ëª¨ë¸ì€ íŠ¹ì • ì—°ì‚° ë˜ëŠ” ì—°ì‚° íŒ¨í„´ì— ì§‘ì¤‘í•˜ì—¬ Hailo ì§€ì› ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

        Returns:
            dict: {ëª¨ë¸ ì´ë¦„: nn.Module ì¸ìŠ¤í„´ìŠ¤} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬.
        """
        models = {}
        
        print("=== PyTorch í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ì‹œì‘ ===")

        # 1. ê¸°ë³¸ Convolution ì—°ì‚°ë“¤
        models['basic_conv'] = self._create_basic_conv_model()
        models['depthwise_conv'] = self._create_depthwise_conv_model()
        models['pointwise_conv'] = self._create_pointwise_conv_model()
        models['dilated_conv'] = self._create_dilated_conv_model()
        models['grouped_conv'] = self._create_grouped_conv_model()
        
        # 2. í™œì„±í™” í•¨ìˆ˜ë“¤
        models['relu_activation'] = self._create_activation_model('relu')
        models['leaky_relu_activation'] = self._create_activation_model('leaky_relu')
        models['sigmoid_activation'] = self._create_activation_model('sigmoid')
        models['tanh_activation'] = self._create_activation_model('tanh')
        models['swish_activation'] = self._create_activation_model('swish') # x * sigmoid(x) ì¡°í•©
        models['gelu_activation'] = self._create_activation_model('gelu')
        models['elu_activation'] = self._create_activation_model('elu')
        models['prelu_activation'] = self._create_activation_model('prelu')
        models['hardswish_activation'] = self._create_activation_model('hardswish')
        models['hardsigmoid_activation'] = self._create_activation_model('hardsigmoid')
        
        # 3. ì •ê·œí™” ì—°ì‚°ë“¤
        models['batch_norm'] = self._create_batch_norm_model()
        models['layer_norm'] = self._create_layer_norm_model()
        models['instance_norm'] = self._create_instance_norm_model()
        
        # 4. í’€ë§ ì—°ì‚°ë“¤
        models['max_pool'] = self._create_pooling_model('max')
        models['avg_pool'] = self._create_pooling_model('avg')
        models['adaptive_pool'] = self._create_pooling_model('adaptive') # AdaptiveMaxPool2d
        models['global_avg_pool'] = self._create_pooling_model('global_avg') # AdaptiveAvgPool2d((1,1))
        models['max_pool_varied'] = self._create_varied_pooling_model('max') # ë‹¤ì–‘í•œ kernel/stride/padding
        models['avg_pool_varied'] = self._create_varied_pooling_model('avg') # ë‹¤ì–‘í•œ kernel/stride/padding
        
        # 5. Element-wise ì—°ì‚°ë“¤
        models['elementwise_add'] = self._create_elementwise_model('add')
        models['elementwise_mul'] = self._create_elementwise_model('mul')
        models['elementwise_sub'] = self._create_elementwise_model('sub')
        models['elementwise_div'] = self._create_elementwise_model('div')
        
        # 6. Resize/Interpolation ì—°ì‚°ë“¤
        models['bilinear_upsample'] = self._create_upsample_model('bilinear')
        models['nearest_upsample'] = self._create_upsample_model('nearest')
        
        # 7. Skip Connection íŒ¨í„´ë“¤ (Residual, DenseNet êµ¬ì¡°)
        models['residual_block'] = self._create_residual_model()
        models['dense_connection'] = self._create_dense_connection_model()
        
        # 8. Attention ë©”ì»¤ë‹ˆì¦˜
        models['simple_attention'] = self._create_attention_model()
        
        # 9. 1D Conv ì—°ì‚°ë“¤ (ë¬¸ì œ ë°œìƒ ì´ë ¥: Timeout)
        models['conv1d'] = self._create_conv1d_model()
        
        # 10. Transpose Conv (Deconv)
        models['transpose_conv'] = self._create_transpose_conv_model()

        # 11. ì¶”ê°€ëœ ë‹¤ì–‘í•œ ì—°ì‚°ë“¤
        models['fully_connected'] = self._create_fully_connected_model() # nn.Linearì˜ ê¸°ë³¸ì ì¸ ì‚¬ìš©
        models['flatten_op'] = self._create_flatten_model() # nn.Flatten ëª¨ë“ˆ ëª…ì‹œì  í…ŒìŠ¤íŠ¸
        models['dropout_op'] = self._create_dropout_model() # nn.Dropout2d í…ŒìŠ¤íŠ¸ (ì¶”ë¡  ì‹œ No-Op)
        models['concatenation_op'] = self._create_concatenation_model() # torch.cat í…ŒìŠ¤íŠ¸
        models['zero_pad'] = self._create_zero_pad_model() # nn.ZeroPad2d í…ŒìŠ¤íŠ¸
        models['permute_op'] = self._create_permute_model() # torch.permute í…ŒìŠ¤íŠ¸
        models['matmul_op'] = self._create_matmul_model() # nn.Linearë¥¼ í†µí•œ ê°„ì ‘ì ì¸ MatMul í…ŒìŠ¤íŠ¸
        models['clamp_op'] = self._create_clamp_model() # torch.clamp í…ŒìŠ¤íŠ¸
        models['mean_reduction'] = self._create_mean_reduction_model() # torch.mean(dim=...) í…ŒìŠ¤íŠ¸ (ë¬¸ì œ ë°œìƒ ì´ë ¥: íŒŒì‹± ì˜¤ë¥˜)
        
        print("=== PyTorch í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ ===")
        return models
    
    # ----------------------------------------------------------------------
    # ê° ì—°ì‚°/íŒ¨í„´ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ê°œë³„ PyTorch ëª¨ë¸ ì •ì˜
    # (ì¼ê´€ì„±ì„ ìœ„í•´ ëŒ€ë¶€ë¶„ AdaptiveAvgPool2d -> squeeze -> Linear íŒ¨í„´ìœ¼ë¡œ ëë‚¨)
    # ----------------------------------------------------------------------

    def _create_basic_conv_model(self):
        """
        í‘œì¤€ 2D Convolution (nn.Conv2d)ì˜ ê¸°ë³¸ì ì¸ ì‘ë™ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        ì´ˆê¸° ë²„ì „ì—ì„œ `x.view()` ì‚¬ìš© ì‹œ ONNX export ë¬¸ì œê°€ ìˆì—ˆìœ¼ë‚˜,
        í˜„ì¬ `x.squeeze()`ë¡œ ë³€ê²½í•˜ì—¬ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.
        """
        class BasicConvModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 5, padding=2) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10) # 64 features -> 10 classes
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = F.relu(self.conv2(x)) # Output shape e.g., (B, 64, H, W)
                x = self.pool(x)          # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64) - 1x1 ì°¨ì› ì œê±°
                return self.classifier(x) # (B, 10)
        return BasicConvModel()
    
    def _create_depthwise_conv_model(self):
        """Depthwise Convolution (groups=in_channels) ë° Pointwise Conv ì¡°í•© í…ŒìŠ¤íŠ¸."""
        class DepthwiseModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.stem = nn.Conv2d(1, 32, 3, padding=1)
                self.depthwise = nn.Conv2d(32, 32, 3, padding=1, groups=32)
                self.pointwise = nn.Conv2d(32, 64, 1) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.stem(x))
                x = F.relu(self.depthwise(x))
                x = F.relu(self.pointwise(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return DepthwiseModel()
    
    def _create_pointwise_conv_model(self):
        """Pointwise Convolution (1x1 Conv) ìì²´ë¥¼ í…ŒìŠ¤íŠ¸."""
        class PointwiseModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.pointwise1 = nn.Conv2d(32, 64, 1) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = F.relu(self.pointwise1(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return PointwiseModel()
    
    def _create_dilated_conv_model(self):
        """Dilated Convolution (dilation ì¸ì ì‚¬ìš©)ì„ í…ŒìŠ¤íŠ¸."""
        class DilatedModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.dilated_conv = nn.Conv2d(32, 64, 3, padding=2, dilation=2) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = F.relu(self.dilated_conv(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return DilatedModel()
    
    def _create_grouped_conv_model(self):
        """Grouped Convolution (groups > 1)ì„ í…ŒìŠ¤íŠ¸."""
        class GroupedModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.grouped_conv = nn.Conv2d(32, 64, 3, padding=1, groups=4) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = F.relu(self.grouped_conv(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return GroupedModel()
    
    def _create_activation_model(self, activation_type):
        """ë‹¤ì–‘í•œ í™œì„±í™” í•¨ìˆ˜(ReLU, LeakyReLU, Sigmoid, Tanh ë“±)ë¥¼ í…ŒìŠ¤íŠ¸."""
        class ActivationModel(nn.Module):
            def __init__(self, act_type):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                self.act_type = act_type
                # PReLU, Hardswish, HardsigmoidëŠ” ëª¨ë“ˆë¡œ ì„ ì–¸í•´ì•¼ í•¨
                if act_type == 'prelu':
                    self.prelu = nn.PReLU()
                elif act_type == 'hardswish':
                    self.hardswish = nn.Hardswish()
                elif act_type == 'hardsigmoid':
                    self.hardsigmoid = nn.Hardsigmoid()
                
            def forward(self, x):
                x = self.apply_activation(self.conv1(x))
                x = self.apply_activation(self.conv2(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
            
            def apply_activation(self, x):
                # ë‹¤ì–‘í•œ í™œì„±í™” í•¨ìˆ˜ ì ìš©
                if self.act_type == 'relu':
                    return F.relu(x)
                elif self.act_type == 'leaky_relu':
                    return F.leaky_relu(x, 0.1)
                elif self.act_type == 'sigmoid':
                    return torch.sigmoid(x)
                elif self.act_type == 'tanh':
                    return torch.tanh(x)
                elif self.act_type == 'swish':
                    return x * torch.sigmoid(x) # Element-wise ops for Swish
                elif self.act_type == 'gelu':
                    return F.gelu(x)
                elif self.act_type == 'elu':
                    return F.elu(x)
                elif self.act_type == 'prelu':
                    return self.prelu(x)
                elif self.act_type == 'hardswish':
                    return self.hardswish(x)
                elif self.act_type == 'hardsigmoid':
                    return self.hardsigmoid(x)
                else: # Fallback to ReLU if unknown type
                    return F.relu(x)
        return ActivationModel(activation_type)
    
    def _create_batch_norm_model(self):
        """Batch Normalization (nn.BatchNorm2d)ì„ í…ŒìŠ¤íŠ¸."""
        class BatchNormModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.bn1 = nn.BatchNorm2d(32)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.bn2 = nn.BatchNorm2d(64)
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.bn1(self.conv1(x)))
                x = F.relu(self.bn2(self.conv2(x))) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return BatchNormModel()
    
    def _create_layer_norm_model(self):
        """Layer Normalization (nn.LayerNorm)ì„ í…ŒìŠ¤íŠ¸."""
        class LayerNormModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                # LayerNormì€ ì…ë ¥ í…ì„œì˜ ë§ˆì§€ë§‰ ì°¨ì›ë“¤ì— ì ìš©ë˜ë¯€ë¡œ,
                # Conv2d ì¶œë ¥ (B, C, H, W)ì— ë§ê²Œ [C, H, W]ë¥¼ ì§€ì •
                self.ln1 = nn.LayerNorm([32, 28, 28])  # MNIST (1, 28, 28) ê¸°ì¤€, Conv1 ì¶œë ¥ (32, 28, 28)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = self.conv1(x)
                x = F.relu(self.ln1(x)) # LayerNorm í›„ì— í™œì„±í™” í•¨ìˆ˜ ì ìš©
                x = F.relu(self.conv2(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return LayerNormModel()
    
    def _create_instance_norm_model(self):
        """Instance Normalization (nn.InstanceNorm2d)ì„ í…ŒìŠ¤íŠ¸."""
        class InstanceNormModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.in1 = nn.InstanceNorm2d(32)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.in1(self.conv1(x)))
                x = F.relu(self.conv2(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return InstanceNormModel()
    
    def _create_pooling_model(self, pool_type):
        """
        Max, Avg, Adaptive, Global Avg Poolingì˜ ê¸°ë³¸ ë™ì‘ì„ í…ŒìŠ¤íŠ¸.
        F.adaptive_avg_pool2d((1,1))ì€ ìµœì¢… ë¶„ë¥˜ ì§ì „ì— í•­ìƒ ì ìš©ë©ë‹ˆë‹¤.
        """
        class PoolingModel(nn.Module):
            def __init__(self, p_type):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.pool_type = p_type
                
                if p_type == 'max':
                    self.pool_op = nn.MaxPool2d(2, 2)
                elif p_type == 'avg':
                    self.pool_op = nn.AvgPool2d(2, 2)
                elif p_type == 'adaptive':
                    self.pool_op = nn.AdaptiveMaxPool2d((7, 7)) # íŠ¹ì • í¬ê¸°ë¡œ Adaptive MaxPool
                elif p_type == 'global_avg':
                    # Global Average Poolingì€ forwardì—ì„œ ì§ì ‘ ì ìš©
                    pass 
                
                self.global_final_pool = nn.AdaptiveAvgPool2d((1, 1)) # ìµœì¢…ì ìœ¼ë¡œ í•­ìƒ ê¸€ë¡œë²Œ í’€ë§
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                if self.pool_type != 'global_avg':
                    x = self.pool_op(x) # íŠ¹ì • í’€ë§ ì—°ì‚° ì ìš©
                
                x = F.relu(self.conv2(x)) # (B, 64, H', W')
                
                # ìµœì¢…ì ìœ¼ë¡œ ê¸€ë¡œë²Œ í’€ë§ í›„ Linear ë ˆì´ì–´ ì—°ê²°
                x = self.global_final_pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return PoolingModel(pool_type)
    
    def _create_varied_pooling_model(self, pool_type):
        """ë‹¤ì–‘í•œ kernel_size, stride, paddingì„ ê°–ëŠ” Max/Avg Poolingì„ í…ŒìŠ¤íŠ¸."""
        class VariedPoolingModel(nn.Module):
            def __init__(self, p_type):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.pool_type = p_type
                
                if p_type == 'max':
                    self.pool_op = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
                elif p_type == 'avg':
                    self.pool_op = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)
                
                self.global_final_pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = self.pool_op(x) # ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ì˜ í’€ë§ ì ìš©
                x = F.relu(self.conv2(x)) # (B, 64, H', W')
                
                x = self.global_final_pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return VariedPoolingModel(pool_type)
    
    def _create_elementwise_model(self, op_type):
        """Add, Mul, Sub, Div ë“± ìš”ì†Œë³„(Element-wise) ì—°ì‚°ì„ í…ŒìŠ¤íŠ¸."""
        class ElementwiseModel(nn.Module):
            def __init__(self, operation):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(1, 32, 3, padding=1) # ë™ì¼í•œ í¬ê¸°ì˜ ë‘ í…ì„œ ìƒì„±
                self.conv3 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                self.operation = operation
                
            def forward(self, x):
                x1 = F.relu(self.conv1(x))
                x2 = F.relu(self.conv2(x))
                
                if self.operation == 'add':
                    x_combined = x1 + x2
                elif self.operation == 'mul':
                    x_combined = x1 * x2
                elif self.operation == 'sub':
                    x_combined = x1 - x2
                elif self.operation == 'div':
                    x_combined = x1 / (x2 + 1e-8)  # ZeroDivisionError ë°©ì§€ë¥¼ ìœ„í•œ ì‘ì€ ê°’ ì¶”ê°€
                else:
                    raise ValueError(f"Unsupported element-wise operation: {self.operation}")
                
                x = F.relu(self.conv3(x_combined)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return ElementwiseModel(op_type)
    
    def _create_upsample_model(self, mode):
        """Bilinear ë° Nearest ëª¨ë“œë¥¼ ì‚¬ìš©í•œ ì—…ìƒ˜í”Œë§ (F.interpolate)ì„ í…ŒìŠ¤íŠ¸."""
        class UpsampleModel(nn.Module):
            def __init__(self, upsample_mode):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                self.mode = upsample_mode
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = F.max_pool2d(x, 2)  # ì—…ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë‹¤ìš´ìƒ˜í”Œë§ ë¨¼ì € ìˆ˜í–‰
                x = F.interpolate(x, scale_factor=2, mode=self.mode)  # ì—…ìƒ˜í”Œë§
                x = F.relu(self.conv2(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return UpsampleModel(mode)
    
    def _create_residual_model(self):
        """Residual Connection (ì”ì°¨ ì—°ê²°) íŒ¨í„´ì„ í…ŒìŠ¤íŠ¸."""
        class ResidualModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 32, 3, padding=1)
                self.conv3 = nn.Conv2d(32, 32, 3, padding=1) # 32 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(32, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                identity = x # Residual connectionì„ ìœ„í•œ ì €ì¥
                x = F.relu(self.conv2(x))
                x = self.conv3(x)
                x = x + identity  # Residual connection: ì…ë ¥ê³¼ ì¶œë ¥ ë”í•˜ê¸°
                x = F.relu(x) # ìµœì¢… í™œì„±í™” (ResNet íŒ¨í„´) # (B, 32, H, W)
                x = self.pool(x) # (B, 32, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 32)
                return self.classifier(x)
        return ResidualModel()
    
    def _create_dense_connection_model(self):
        """Dense Connection (ë°€ì§‘ ì—°ê²°) íŒ¨í„´ (torch.cat ì‚¬ìš©)ì„ í…ŒìŠ¤íŠ¸."""
        class DenseModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 16, 3, padding=1)
                self.conv2 = nn.Conv2d(16, 16, 3, padding=1)
                # ì´ì „ conv1 ì¶œë ¥(16) + conv2 ì¶œë ¥(16) = 32 ì±„ë„ì´ ì…ë ¥
                self.conv3 = nn.Conv2d(32, 16, 3, padding=1)
                # conv1(16) + conv2(16) + conv3(16) = 48 ì±„ë„ì´ ìµœì¢… íŠ¹ì„±
                self.final_features_conv = nn.Conv2d(48, 48, 1) # 48 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(48, 10)
                
            def forward(self, x):
                x1 = F.relu(self.conv1(x))
                x2 = F.relu(self.conv2(x1))
                x2_cat = torch.cat([x1, x2], dim=1)  # Dense connection: x1ê³¼ x2 ì±„ë„ë³„ ì—°ê²°
                x3 = F.relu(self.conv3(x2_cat))
                x_final = torch.cat([x1, x2, x3], dim=1)  # ëª¨ë“  íŠ¹ì„± ì—°ê²° # (B, 48, H, W)
                x = F.relu(self.final_features_conv(x_final)) # ìµœì¢… Conv (B, 48, H, W)
                x = self.pool(x) # (B, 48, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 48)
                return self.classifier(x)
        return DenseModel()
    
    def _create_attention_model(self):
        """ê°„ë‹¨í•œ Spatial Attention ë©”ì»¤ë‹ˆì¦˜ì„ í…ŒìŠ¤íŠ¸."""
        class SimpleAttentionModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                # Attention ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•  1x1 Conv (ì¶œë ¥ ì±„ë„ 1)
                self.attention_conv = nn.Conv2d(64, 1, 1)
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = F.relu(self.conv2(x)) # (B, 64, H, W)
                
                # Spatial attention ê°€ì¤‘ì¹˜ ê³„ì‚° (Sigmoidë¡œ 0~1 ë²”ìœ„)
                attention_weights = torch.sigmoid(self.attention_conv(x)) # (B, 1, H, W)
                x = x * attention_weights # ìš”ì†Œë³„ ê³±ì…ˆìœ¼ë¡œ íŠ¹ì„±ì— ê°€ì¤‘ì¹˜ ì ìš© # (B, 64, H, W)
                
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return SimpleAttentionModel()
    
    def _create_conv1d_model(self):
        """
        2D ì…ë ¥ì—ì„œ 1D Convolution (nn.Conv1d)ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        (ì°¸ê³ : ì´ ëª¨ë¸ì€ ì´ì „ í…ŒìŠ¤íŠ¸ì—ì„œ 'Timeout'ìœ¼ë¡œ ì‹¤íŒ¨í•œ ì´ë ¥ì´ ìˆìŠµë‹ˆë‹¤.
        ì´ëŠ” Hailoê°€ 1D Convë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê±°ë‚˜ ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
        ëŒ€ì•ˆìœ¼ë¡œ 2D Convë¥¼ ì‚¬ìš©í•œ ìš°íšŒ êµ¬í˜„ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
        """
        class Conv1DModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv2d = nn.Conv2d(1, 32, 3, padding=1) # (B, 32, 28, 28)
                self.conv1d = nn.Conv1d(32, 64, 3, padding=1) # 64 features
                self.pool = nn.AdaptiveAvgPool1d(1) # 1D Global Average Pooling
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv2d(x)) # (B, 32, 28, 28)
                # ë†’ì´ ì°¨ì›ì„ 1ë¡œ ì¤„ì—¬ 2D í…ì„œë¥¼ 1D ì‹œí€€ìŠ¤ì²˜ëŸ¼ ë§Œë“¦
                x = F.adaptive_avg_pool2d(x, (1, 28)) # (B, 32, 1, 28)
                x = x.squeeze(2) # (B, 32, 28) - 1D Conv ì…ë ¥ì„ ìœ„í•´ 1ì°¨ì› ì œê±°
                x = F.relu(self.conv1d(x)) # (B, 64, 28)
                x = self.pool(x) # (B, 64, 1) - 1D Global Pooling
                x = x.squeeze(-1) # (B, 64) - ë§ˆì§€ë§‰ 1ì°¨ì› ì œê±°
                return self.classifier(x)
        return Conv1DModel()
    
    def _create_transpose_conv_model(self):
        """Transpose Convolution (nn.ConvTranspose2d)ì„ í…ŒìŠ¤íŠ¸."""
        class TransposeConvModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1) # ë‹¤ìš´ìƒ˜í”Œë§ íš¨ê³¼ (ì¶œë ¥ í¬ê¸° ê°ì†Œ)
                self.transpose_conv = nn.ConvTranspose2d(64, 32, 2, stride=2) # ì—…ìƒ˜í”Œë§ íš¨ê³¼ (ì¶œë ¥ í¬ê¸° ë³µì›) # 32 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(32, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = F.relu(self.conv2(x))
                x = F.relu(self.transpose_conv(x)) # (B, 32, H_orig, W_orig)
                x = self.pool(x) # (B, 32, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 32)
                return self.classifier(x)
        return TransposeConvModel()
    
    def _create_fully_connected_model(self):
        """nn.Linear (Fully Connected Layer)ì™€ nn.Flattenì˜ ê¸°ë³¸ì ì¸ ì‚¬ìš©ì„ í…ŒìŠ¤íŠ¸."""
        class FullyConnectedModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # (B, 32, 28, 28)
                self.flatten = nn.Flatten() # Flatten 2D features to 1D vector
                self.fc1 = nn.Linear(32 * 28 * 28, 128)
                self.fc2 = nn.Linear(128, 10) # Final output is (B, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = self.flatten(x) # (B, 32*28*28)
                x = F.relu(self.fc1(x))
                x = self.fc2(x)
                return x
        return FullyConnectedModel()
    
    def _create_flatten_model(self):
        """nn.Flatten ëª¨ë“ˆ ìì²´ì˜ ì§€ì›ì„ ëª…ì‹œì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸."""
        class FlattenModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # (B, 32, 28, 28)
                self.flatten = nn.Flatten() # Test nn.Flatten
                self.classifier = nn.Linear(32 * 28 * 28, 10) # (B, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = self.flatten(x) # (B, 32*28*28)
                x = self.classifier(x)
                return x
        return FlattenModel()

    def _create_dropout_model(self):
        """
        nn.Dropout2d ì—°ì‚°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        (ì°¸ê³ : Dropoutì€ ì¶”ë¡  ì‹œì—ëŠ” ë³´í†µ ì•„ë¬´ëŸ° ì—°ì‚°ë„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” No-Opìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.)
        """
        class DropoutModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.dropout = nn.Dropout2d(p=0.5) # Apply dropout
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = self.dropout(x) # Dropout ì ìš© (eval ëª¨ë“œì—ì„œëŠ” íš¨ê³¼ ì—†ìŒ)
                x = F.relu(self.conv2(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return DropoutModel()

    def _create_concatenation_model(self):
        """torch.cat (Concatenation) ì—°ì‚°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        class ConcatenationModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv_branch1 = nn.Conv2d(1, 16, 3, padding=1)
                self.conv_branch2 = nn.Conv2d(1, 16, 3, padding=1) # ë‘ ë¸Œëœì¹˜ì—ì„œ ë™ì¼í•œ í¬ê¸°/ì±„ë„ ìƒì„±
                self.conv_after_cat = nn.Conv2d(32, 32, 1) # Concatenation í›„ ì…ë ¥ ì±„ë„ 16+16=32 # 32 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(32, 10)
                
            def forward(self, x):
                x1 = F.relu(self.conv_branch1(x))
                x2 = F.relu(self.conv_branch2(x))
                x_cat = torch.cat([x1, x2], dim=1) # ì±„ë„ ì°¨ì› (dim=1)ìœ¼ë¡œ ì—°ê²° # (B, 32, H, W)
                x = F.relu(self.conv_after_cat(x_cat)) # (B, 32, H, W)
                x = self.pool(x) # (B, 32, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 32)
                return self.classifier(x)
        return ConcatenationModel()
    
    def _create_zero_pad_model(self):
        """nn.ZeroPad2d (ëª…ì‹œì ì¸ Zero Padding) ì—°ì‚°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        class ZeroPadModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=0) # No padding in Conv to clearly see ZeroPad
                self.pad = nn.ZeroPad2d((1, 1, 1, 1)) # (left, right, top, bottom)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=0) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x)) # Output size reduced due to padding=0
                x = self.pad(x) # Pad to restore or expand size
                x = F.relu(self.conv2(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return ZeroPadModel()

    def _create_permute_model(self):
        """torch.permute (í…ì„œ ì°¨ì› ìˆœì„œ ë³€ê²½) ì—°ì‚°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        class PermuteModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # (B, C, H, W) = (B, 32, 28, 28)
                self.final_conv = nn.Conv2d(32, 32, 1) # 32 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(32, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x)) # (B, 32, 28, 28)
                # ì°¨ì› ìˆœì„œ ë³€ê²½ í…ŒìŠ¤íŠ¸ (ì˜ˆ: NHWC -> NCHW -> NHWC)
                x = x.permute(0, 2, 3, 1) # (B, H, W, C) = (B, 28, 28, 32)
                x = x.permute(0, 3, 1, 2) # ë‹¤ì‹œ (B, C, H, W) = (B, 32, 28, 28)ë¡œ ë³µì›
                x = F.relu(self.final_conv(x)) # (B, 32, H, W)
                x = self.pool(x) # (B, 32, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 32)
                return self.classifier(x)
        return PermuteModel()
    
    def _create_matmul_model(self):
        """
        ê°„ì ‘ì ì¸ í–‰ë ¬ ê³±ì…ˆ (Matrix Multiplication)ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        (nn.LinearëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í–‰ë ¬ ê³±ì…ˆì„ ì‚¬ìš©í•˜ë©°, NPUì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤.)
        torch.matmulì„ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  nn.Linearë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì»´íŒŒì¼ì— ë” ì•ˆì •ì ì…ë‹ˆë‹¤.
        """
        class MatmulModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 10, 3, padding=1) # Output: (B, 10, 28, 28)
                self.flatten = nn.Flatten()
                self.fc = nn.Linear(10 * 28 * 28, 10) # (B, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = self.flatten(x) # (B, 10 * 28 * 28)
                x = self.fc(x) # nn.Linearë¥¼ í†µí•œ í–‰ë ¬ ê³±ì…ˆ
                return x
        return MatmulModel()

    def _create_clamp_model(self):
        """torch.clamp (í…ì„œ ê°’ ë²”ìœ„ ì œí•œ) ì—°ì‚°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        class ClampModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
                self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 64 features
                self.pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(64, 10)
                
            def forward(self, x):
                x = F.relu(self.conv1(x))
                x = torch.clamp(x, min=-1.0, max=1.0) # ê°’ì„ -1.0ì—ì„œ 1.0 ì‚¬ì´ë¡œ ì œí•œ
                x = F.relu(self.conv2(x)) # (B, 64, H, W)
                x = self.pool(x) # (B, 64, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 64)
                return self.classifier(x)
        return ClampModel()

    def _create_mean_reduction_model(self):
        """
        íŠ¹ì • ì°¨ì›ì—ì„œì˜ í‰ê· (torch.mean(dim=...)) ì—°ì‚°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        (ì°¸ê³ : ì´ ëª¨ë¸ì€ ì´ì „ í…ŒìŠ¤íŠ¸ì—ì„œ 'Invalid kernel shape' íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí•œ ì´ë ¥ì´ ìˆìŠµë‹ˆë‹¤.
        í˜„ì¬ëŠ” Hailo ì»´íŒŒì¼ëŸ¬ì— ë” ì¹œí™”ì ì¸ Global Average Pooling (AdaptiveAvgPool2d)ì„
        ì‚¬ìš©í•œ í˜•íƒœë¡œ ìˆ˜ì •ë˜ì–´ ì„±ê³µì ìœ¼ë¡œ ì»´íŒŒì¼ë  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
        ì›ë˜ì˜ `torch.mean(dim=2)` ì§ì ‘ ì‚¬ìš© ì‹œì—ëŠ” ë¬¸ì œ ë°œìƒ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.)
        """
        class MeanReductionModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # (B, 32, 28, 28)
                # NPU ì¹œí™”ì ì¸ Global Average Poolingìœ¼ë¡œ ë³€ê²½ (ì´ì „ torch.mean(dim=2) ëŒ€ì²´)
                self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
                self.classifier = nn.Linear(32, 10) # Conv1ì˜ ì¶œë ¥ ì±„ë„ 32ë¥¼ Linear ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                
            def forward(self, x):
                x = F.relu(self.conv1(x)) # (B, 32, 28, 28)
                # ì›ë˜ì˜ í…ŒìŠ¤íŠ¸: x = torch.mean(x, dim=2) # (B, 32, 28)
                # NPU ì¹œí™”ì  ë³€ê²½:
                x = self.global_avg_pool(x) # (B, 32, 1, 1)
                x = x.squeeze(-1).squeeze(-1) # (B, 32)
                x = self.classifier(x)
                return x
        return MeanReductionModel()
    
    def export_models_to_onnx(self, models):
        """
        ìƒì„±ëœ PyTorch ëª¨ë¸ë“¤ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        Hailo ì»´íŒŒì¼ëŸ¬ëŠ” ONNXë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        # ë”ë¯¸ ì…ë ¥ í…ì„œ (ë°°ì¹˜ í¬ê¸° 1, ì±„ë„ 1, 28x28 ì´ë¯¸ì§€)
        dummy_input = torch.randn(1, 1, 28, 28)
        successful_exports = []
        
        print("\n=== ONNX ë³€í™˜ ì‹œì‘ ===")
        for name, model in models.items():
            onnx_path = self.models_dir / f"{name}.onnx"
            try:
                model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (Dropout ë“±ì´ ë¹„í™œì„±í™”ë¨)
                torch.onnx.export(
                    model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,        # ëª¨ë¸ì˜ í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë„ í•¨ê»˜ ì €ì¥
                    opset_version=11,          # ONNX Operator Set ë²„ì „ (Hailoì—ì„œ ì§€ì›í•˜ëŠ” ë²„ì „ í™•ì¸ í•„ìš”)
                    do_constant_folding=True,  # ìƒìˆ˜ í´ë”© ìµœì í™” í™œì„±í™”
                    input_names=['input'],     # ONNX ê·¸ë˜í”„ì˜ ì…ë ¥ ì´ë¦„ ì§€ì •
                    output_names=['output'],   # ONNX ê·¸ë˜í”„ì˜ ì¶œë ¥ ì´ë¦„ ì§€ì •
                    dynamic_axes={             # ë°°ì¹˜ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    }
                )
                successful_exports.append(name)
                print(f"âœ“ {name}.onnx ì €ì¥ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âœ— {name} ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
                self.test_results[name] = {'onnx_export': False, 'error': str(e), 'stage': 'onnx_export'}
        print("=== ONNX ë³€í™˜ ì™„ë£Œ ===")
        return successful_exports
    
    def test_hailo_compilation(self, model_names):
        """
        ONNXë¡œ ë³€í™˜ëœ ëª¨ë¸ë“¤ì— ëŒ€í•´ Hailo ì»´íŒŒì¼ëŸ¬ì˜ íŒŒì‹±, ìµœì í™”, ì»´íŒŒì¼ ë‹¨ê³„ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        """
        for name in model_names:
            print(f"\n=== {name} Hailo ì»´íŒŒì¼ í…ŒìŠ¤íŠ¸ ===")
            onnx_path = self.models_dir / f"{name}.onnx"
            
            if not onnx_path.exists():
                print(f"ê²½ê³ : {name} ONNX íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {onnx_path}. ë‹¤ìŒ ëª¨ë¸ë¡œ ê±´ë„ˆëœ€.")
                continue
            
            try:
                # 1ë‹¨ê³„: íŒŒì‹± (ONNX -> HAR ë³€í™˜)
                har_parsed = self.models_dir / f"{name}_parsed.har"
                parse_cmd = [
                    "hailo", "parser", "onnx", str(onnx_path),
                    "--har-path", str(har_parsed)
                ]
                print("1. íŒŒì‹± ì¤‘...")
                # capture_output=True: stdout, stderr ìº¡ì²˜
                # text=True: ì¶œë ¥ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
                # timeout: ëª…ë ¹ì–´ ì‹¤í–‰ ì‹œê°„ ì œí•œ (ì´ˆ)
                result = subprocess.run(parse_cmd, capture_output=True, text=True, timeout=60)
                
                if result.returncode != 0:
                    print(f"âœ— íŒŒì‹± ì‹¤íŒ¨: {result.stderr}")
                    self.test_results[name] = {
                        'parse': False,
                        'error': result.stderr.strip(),
                        'stage': 'parse'
                    }
                    continue
                
                print("âœ“ íŒŒì‹± ì„±ê³µ")
                
                # 2ë‹¨ê³„: ìµœì í™” (HAR íŒŒì¼ ìµœì í™”)
                har_optimized = self.models_dir / f"{name}_optimized.har"
                optimize_cmd = [
                    "hailo", "optimize", str(har_parsed),
                    "--output-har-path", str(har_optimized),
                    "--use-random-calib-set" # ì–‘ìí™”ë¥¼ ìœ„í•œ ë¬´ì‘ìœ„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì…‹ ì‚¬ìš©
                ]
                print("2. ìµœì í™” ì¤‘...")
                result = subprocess.run(optimize_cmd, capture_output=True, text=True, timeout=120)
                
                if result.returncode != 0:
                    print(f"âœ— ìµœì í™” ì‹¤íŒ¨: {result.stderr}")
                    self.test_results[name] = {
                        'parse': True,
                        'optimize': False,
                        'error': result.stderr.strip(),
                        'stage': 'optimize'
                    }
                    continue
                
                print("âœ“ ìµœì í™” ì„±ê³µ")
                
                # 3ë‹¨ê³„: ì»´íŒŒì¼ (ìµœì í™”ëœ HAR -> Hailo ë°”ì´ë„ˆë¦¬)
                # ê° ëª¨ë¸ë³„ë¡œ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ì—¬ ì»´íŒŒì¼ ê²°ê³¼ë¬¼ ì €ì¥
                output_model_dir = self.results_dir / name 
                output_model_dir.mkdir(parents=True, exist_ok=True)
                
                compile_cmd = [
                    "hailo", "compiler", str(har_optimized),
                    "--output-dir", str(output_model_dir),
                    "--hw-arch", "hailo8" # Hailo-8 ì•„í‚¤í…ì²˜ ì§€ì •
                ]
                print("3. ì»´íŒŒì¼ ì¤‘...")
                result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=180)
                
                if result.returncode != 0:
                    print(f"âœ— ì»´íŒŒì¼ ì‹¤íŒ¨: {result.stderr}")
                    self.test_results[name] = {
                        'parse': True,
                        'optimize': True,
                        'compile': False,
                        'error': result.stderr.strip(),
                        'stage': 'compile'
                    }
                    continue
                
                print("âœ“ ì»´íŒŒì¼ ì„±ê³µ!")
                self.test_results[name] = {
                    'parse': True,
                    'optimize': True,
                    'compile': True,
                    'stage': 'complete'
                }
                
            except subprocess.TimeoutExpired:
                # ëª…ë ¹ì–´ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ ì‹œ
                print(f"âœ— íƒ€ì„ì•„ì›ƒ ë°œìƒ: {name}")
                self.test_results[name] = {
                    'error': 'Timeout',
                    'stage': 'timeout'
                }
            except Exception as e:
                # ê¸°íƒ€ ì˜ˆì™¸ ë°œìƒ ì‹œ
                print(f"âœ— ì˜ˆì™¸ ë°œìƒ: {e}")
                self.test_results[name] = {
                    'error': str(e),
                    'stage': 'exception'
                }
    
    def analyze_results(self):
        """
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ì„±ê³µ/ì‹¤íŒ¨í•œ ëª¨ë¸, ì§€ì›ë˜ì§€ ì•ŠëŠ” ì—°ì‚° ë“±ì„ ì •ë¦¬í•˜ì—¬ ì¶œë ¥í•˜ê³  JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        successful_models = []
        failed_models = []
        unsupported_ops_messages = set() # ì¤‘ë³µ ë©”ì‹œì§€ ë°©ì§€ë¥¼ ìœ„í•´ set ì‚¬ìš©
        
        for name, result in self.test_results.items():
            if result.get('compile', False): # ì»´íŒŒì¼ ë‹¨ê³„ê°€ Trueì¸ ê²½ìš° ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                successful_models.append(name)
            else:
                failed_models.append(name)
                if 'error' in result:
                    # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ 'unsupported' í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                    error_msg_lower = result['error'].lower()
                    if 'unsupported' in error_msg_lower or 'invalid kernel shape' in error_msg_lower:
                        # ì˜¤ë¥˜ ë©”ì‹œì§€ì˜ ì²« ì¤„ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥í•˜ì—¬ ê°„ê²°í•˜ê²Œ í‘œì‹œ
                        unsupported_ops_messages.add(f"Error in {name}: {result['error'].splitlines()[0]}")
        
        print("\n" + "="*50)
        print("HAILO ì—°ì‚° ì§€ì› í…ŒìŠ¤íŠ¸ ìµœì¢… ê²°ê³¼")
        print("="*50)
        
        print(f"\nâœ“ ì„±ê³µí•œ ëª¨ë¸ë“¤ ({len(successful_models)}ê°œ):")
        for model in successful_models:
            print(f"  - {model}")
        
        print(f"\nâœ— ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤ ({len(failed_models)}ê°œ):")
        for model in failed_models:
            stage = self.test_results[model].get('stage', 'unknown')
            error_details = self.test_results[model].get('error', 'No error message available.').splitlines()[0]
            print(f"  - {model} (ì‹¤íŒ¨ ë‹¨ê³„: {stage}, ì˜¤ë¥˜: {error_details})")
        
        if unsupported_ops_messages:
            print(f"\nâš ï¸  ì§€ì›ë˜ì§€ ì•Šê±°ë‚˜ ë¬¸ì œ ë°œìƒí•œ ì—°ì‚°ë“¤ (ì¶”ì •):")
            for op_error in sorted(list(unsupported_ops_messages)):
                print(f"  - {op_error}")
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        results_file = self.output_dir / "test_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'successful_models': successful_models,
                'failed_models': failed_models,
                'detailed_results': self.test_results,
                'summary': {
                    'total_models': len(self.test_results),
                    'successful': len(successful_models),
                    'failed': len(failed_models),
                    'success_rate': (len(successful_models) / len(self.test_results) * 100) if self.test_results else 0
                }
            }, f, indent=2, ensure_ascii=False) # í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€
        
        print(f"\nğŸ“Š ìƒì„¸ ê²°ê³¼ê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_file}")
        
        # ì§€ì›ë˜ëŠ” ì—°ì‚° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ì¶œë ¥
        self._analyze_supported_operations(successful_models)
        
        return successful_models, failed_models
    
    def _analyze_supported_operations(self, successful_models):
        """
        ì„±ê³µì ìœ¼ë¡œ ì»´íŒŒì¼ëœ ëª¨ë¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì—°ì‚° ì¹´í…Œê³ ë¦¬ë³„ ì§€ì› ì—¬ë¶€ë¥¼ ë¶„ì„í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.
        """
        print(f"\nğŸ“‹ Hailo-8ì—ì„œ ì§€ì›ë˜ëŠ” ì—°ì‚° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„:")
        
        operation_categories = {
            'Convolution': ['basic_conv', 'depthwise_conv', 'pointwise_conv', 'dilated_conv', 'grouped_conv'],
            'Activation': [
                'relu_activation', 'leaky_relu_activation', 'sigmoid_activation', 'tanh_activation', 
                'swish_activation', 'gelu_activation', 'elu_activation', 'prelu_activation',
                'hardswish_activation', 'hardsigmoid_activation'
            ],
            'Normalization': ['batch_norm', 'layer_norm', 'instance_norm'],
            'Pooling': [
                'max_pool', 'avg_pool', 'adaptive_pool', 'global_avg_pool',
                'max_pool_varied', 'avg_pool_varied'
            ],
            'Element-wise': ['elementwise_add', 'elementwise_mul', 'elementwise_sub', 'elementwise_div'],
            'Resize/Interpolation': ['bilinear_upsample', 'nearest_upsample'],
            'Skip Connections': ['residual_block', 'dense_connection'],
            'Attention': ['simple_attention'],
            '1D Operations': ['conv1d'], # ì‹¤íŒ¨í–ˆë”ë¼ë„ ì—¬ê¸°ì— í¬í•¨ì‹œì¼œ ì–´ë–¤ ì—°ì‚° ê·¸ë£¹ì¸ì§€ ëª…ì‹œ
            'Transpose Convolution': ['transpose_conv'],
            'Fully Connected & Flatten': ['fully_connected', 'flatten_op', 'matmul_op'],
            'Dropout': ['dropout_op'],
            'Concatenation': ['concatenation_op'],
            'Padding': ['zero_pad'],
            'Tensor Manipulation': ['permute_op'],
            'Value Manipulation': ['clamp_op'],
            'Reduction': ['mean_reduction'] # ì‹¤íŒ¨í–ˆë”ë¼ë„ ì–´ë–¤ ì—°ì‚° ê·¸ë£¹ì¸ì§€ ëª…ì‹œ
        }
        
        for category, models_in_category in operation_categories.items():
            supported_in_category = [m for m in models_in_category if m in successful_models]
            total_in_category = len(models_in_category)
            print(f"\n{category}:")
            print(f"  ì§€ì›: {len(supported_in_category)}/{total_in_category}")
            for model_name in models_in_category:
                status = "âœ“" if model_name in successful_models else "âœ—"
                print(f"    {status} {model_name}")
    
    def generate_batch_test_script(self):
        """
        ONNX íŒŒì¼ë“¤ì„ ì‚¬ìš©í•˜ì—¬ Hailo ì»´íŒŒì¼ ê³¼ì •ì„ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ”
        Bash ì‰˜ ìŠ¤í¬ë¦½íŠ¸(batch_test.sh)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        script_content = """#!/bin/bash
# Hailo ì—°ì‚° ì§€ì› ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
# ONNX íŒŒì¼ë“¤ì„ Hailo ì»´íŒŒì¼ëŸ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì„ ìë™í™”í•©ë‹ˆë‹¤.

echo "=== Hailo ì—°ì‚° ì§€ì› ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ ==="
echo "í…ŒìŠ¤íŠ¸ ì‹œê°„: $(date)"

# ê²°ê³¼ ë° ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
MODELS_DIR="hailo_test_results/models"
RESULTS_DIR="hailo_test_results/results"
LOG_FILE="hailo_test_results/batch_test.log"

# ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ)
mkdir -p "$RESULTS_DIR"

# ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™” ë˜ëŠ” ìƒˆë¡œ ìƒì„±
echo "Hailo Batch Test Log - $(date)" > "$LOG_FILE"

success_count=0
total_count=0

# models_dir ì•ˆì˜ ëª¨ë“  .onnx íŒŒì¼ì„ ìˆœíšŒí•˜ë©° í…ŒìŠ¤íŠ¸
for onnx_file in "$MODELS_DIR"/*.onnx; do
    # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if [ -f "$onnx_file" ]; then
        model_name=$(basename "$onnx_file" .onnx) # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°
        echo "\\n=== í…ŒìŠ¤íŠ¸ ì¤‘: $model_name ===" | tee -a "$LOG_FILE" # ì½˜ì†”ê³¼ ë¡œê·¸ íŒŒì¼ì— ì¶œë ¥
        
        total_count=$((total_count + 1)) # ì „ì²´ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìˆ˜ ì¦ê°€
        
        # ê° ëª¨ë¸ì˜ ì¤‘ê°„ HAR íŒŒì¼ ë° ìµœì¢… ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        har_parsed="$MODELS_DIR/${model_name}_parsed.har"
        har_optimized="$MODELS_DIR/${model_name}_optimized.har"
        output_dir="$RESULTS_DIR/$model_name"
        
        mkdir -p "$output_dir" # ëª¨ë¸ë³„ ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        
        # 1ë‹¨ê³„: íŒŒì‹± (ONNX -> HAR)
        echo "1. íŒŒì‹± ì¤‘..." | tee -a "$LOG_FILE"
        if hailo parser onnx "$onnx_file" --har-path "$har_parsed" >> "$LOG_FILE" 2>&1; then
            echo "âœ“ íŒŒì‹± ì„±ê³µ" | tee -a "$LOG_FILE"
            
            # 2ë‹¨ê³„: ìµœì í™” (HAR íŒŒì¼ ìµœì í™”)
            echo "2. ìµœì í™” ì¤‘..." | tee -a "$LOG_FILE"
            if hailo optimize "$har_parsed" --output-har-path "$har_optimized" --use-random-calib-set >> "$LOG_FILE" 2>&1; then
                echo "âœ“ ìµœì í™” ì„±ê³µ" | tee -a "$LOG_FILE"
                
                # 3ë‹¨ê³„: ì»´íŒŒì¼ (ìµœì í™”ëœ HAR -> Hailo ë°”ì´ë„ˆë¦¬)
                echo "3. ì»´íŒŒì¼ ì¤‘..." | tee -a "$LOG_FILE"
                if hailo compiler "$har_optimized" --output-dir "$output_dir" --hw-arch hailo8 >> "$LOG_FILE" 2>&1; then
                    echo "âœ… $model_name: ì „ì²´ ì»´íŒŒì¼ ì„±ê³µ!" | tee -a "$LOG_FILE"
                    success_count=$((success_count + 1)) # ì„±ê³µ ì¹´ìš´íŠ¸ ì¦ê°€
                else
                    echo "âŒ $model_name: ì»´íŒŒì¼ ì‹¤íŒ¨" | tee -a "$LOG_FILE"
                fi
            else
                echo "âŒ $model_name: ìµœì í™” ì‹¤íŒ¨" | tee -a "$LOG_FILE"  
            fi
        else
            echo "âŒ $model_name: íŒŒì‹± ì‹¤íŒ¨" | tee -a "$LOG_FILE"
        fi
    fi
done

echo "\\n=== ìµœì¢… ê²°ê³¼ ìš”ì•½ ===" | tee -a "$LOG_FILE"
echo "ì´ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìˆ˜: $total_count" | tee -a "$LOG_FILE"
echo "ì„±ê³µ: $success_count" | tee -a "$LOG_FILE"
echo "ì‹¤íŒ¨: $((total_count - success_count))" | tee -a "$LOG_FILE"

if [ $total_count -gt 0 ]; then
    success_rate=$((success_count * 100 / total_count))
    echo "ì„±ê³µë¥ : ${success_rate}%" | tee -a "$LOG_FILE"
fi

echo "\\nìƒì„¸ ë¡œê·¸ íŒŒì¼: $LOG_FILE"
echo "ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì‹œê°„: $(date)" | tee -a "$LOG_FILE"
"""
        
        script_path = self.output_dir / "batch_test.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # ìƒì„±ëœ ìŠ¤í¬ë¦½íŠ¸ì— ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
        os.chmod(script_path, 0o755)
        
        print(f"\në°°ì¹˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {script_path}")
        print(f"í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: ./hailo_test_results/batch_test.sh")
    
    def run_full_test(self):
        """
        ì „ì²´ Hailo ì—°ì‚° ì§€ì› í…ŒìŠ¤íŠ¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        (ëª¨ë¸ ìƒì„± -> ONNX ë³€í™˜ -> Hailo ì»´íŒŒì¼ -> ê²°ê³¼ ë¶„ì„ -> ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±)
        """
        print("\n" + "="*60)
        print("=== Hailo ì—°ì‚° ì§€ì› í…ŒìŠ¤íŠ¸ ì‹œì‘ (PyTorch -> ONNX -> Hailo) ===")
        print("="*60)
        
        # 1. í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„±
        print("\n[ë‹¨ê³„ 1/5] PyTorch í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ì¤‘...")
        models = self.create_test_models()
        print(f"ì´ {len(models)}ê°œ PyTorch í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ.")
        
        # 2. ONNX ë³€í™˜
        print("\n[ë‹¨ê³„ 2/5] PyTorch ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        successful_exports = self.export_models_to_onnx(models)
        print(f"ì´ {len(successful_exports)}ê°œ ëª¨ë¸ ONNX ë³€í™˜ ì„±ê³µ.")
        
        # 3. Hailo ì»´íŒŒì¼ í…ŒìŠ¤íŠ¸
        print(f"\n[ë‹¨ê³„ 3/5] ë³€í™˜ëœ ONNX ëª¨ë¸ì— ëŒ€í•´ Hailo ì»´íŒŒì¼ í…ŒìŠ¤íŠ¸ ì¤‘... (ëŒ€ìƒ: {len(successful_exports)}ê°œ ëª¨ë¸)")
        self.test_hailo_compilation(successful_exports)
        
        # 4. ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±
        print("\n[ë‹¨ê³„ 4/5] í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        successful_models, failed_models = self.analyze_results()
        
        # 5. ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        print("\n[ë‹¨ê³„ 5/5] ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì¤‘...")
        self.generate_batch_test_script()
        
        print("\n" + "="*60)
        print("=== Hailo ì—°ì‚° ì§€ì› í…ŒìŠ¤íŠ¸ ì „ì²´ ì™„ë£Œ! ===")
        print("="*60)
        
        return successful_models, failed_models


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = HailoOperationTester()
    
    print("\n" + "#"*70)
    print("## Hailo AI ê°€ì†ê¸° ì—°ì‚° ì§€ì› í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ##")
    print("## ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Hailo-8ì—ì„œ ë‹¤ì–‘í•œ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ì—°ì‚°ì˜ í˜¸í™˜ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤. ##")
    print("#"*70)
    
    print("\ní…ŒìŠ¤íŠ¸ê°€ ìˆ˜í–‰í•  ì£¼ìš” ì—°ì‚° ì¹´í…Œê³ ë¦¬:")
    print("- Convolution (í‘œì¤€, Depthwise, Grouped, Dilated, Transpose)")
    print("- í™œì„±í™” í•¨ìˆ˜ (ReLU, LeakyReLU, Sigmoid, Tanh, Swish, GELU, ELU, PReLU, Hardswish, Hardsigmoid)")
    print("- ì •ê·œí™” (BatchNorm, LayerNorm, InstanceNorm)")
    print("- í’€ë§ (MaxPool, AvgPool, AdaptivePool, GlobalAvgPool, ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°)")
    print("- Element-wise ì—°ì‚° (Add, Mul, Sub, Div, Clamp)")
    print("- Resize/Interpolation (Bilinear, Nearest)")
    print("- Skip Connection íŒ¨í„´ (Residual, Dense)")
    print("- Attention ë©”ì»¤ë‹ˆì¦˜")
    print("- 1D Convolution (íŠ¹ì • ìƒí™©ì—ì„œ ë¬¸ì œ ë°œìƒ ì´ë ¥)")
    print("- Fully Connected Layer (Linear)")
    print("- Tensor ì¡°ì‘ (Flatten, Dropout, Concatenation, Padding, Permute, Squeeze)")
    print("- Reduction (Mean Reduction, íŠ¹ì • ìƒí™©ì—ì„œ ë¬¸ì œ ë°œìƒ ì´ë ¥)")
    
    print(f"\nëª¨ë“  ê²°ê³¼ë¬¼ì€ '{tester.output_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.")
    
    try:
        successful_models, failed_models = tester.run_full_test()
        
        print(f"\nğŸ‰ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½!")
        print(f"  ì´ ì„±ê³µ ëª¨ë¸ ìˆ˜: {len(successful_models)}ê°œ")
        print(f"  ì´ ì‹¤íŒ¨ ëª¨ë¸ ìˆ˜: {len(failed_models)}ê°œ")
        
        if successful_models:
            print(f"\nâœ… Hailo-8ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì»´íŒŒì¼ëœ ì—°ì‚° íŒ¨í„´:")
            for model in successful_models:
                print(f"    - {model}")
        
        if failed_models:
            print(f"\nâŒ ì§€ì›ë˜ì§€ ì•Šê±°ë‚˜ ì»´íŒŒì¼ì— ë¬¸ì œê°€ ìˆëŠ” ì—°ì‚° íŒ¨í„´:")
            for model in failed_models:
                print(f"    - {model}")
        
        print(f"\nğŸ“„ ìƒì„¸ ê²°ê³¼ í™•ì¸ì„ ìœ„í•œ íŒŒì¼ ë° ë””ë ‰í† ë¦¬:")
        print(f"  - ê²°ê³¼ ìš”ì•½ JSON: {tester.output_dir}/test_results.json")
        print(f"  - ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸: {tester.output_dir}/batch_test.sh")
        print(f"  - ì»´íŒŒì¼ëœ ëª¨ë¸ ë° ë¡œê·¸: {tester.output_dir}/results/")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()