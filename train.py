"""
í†µí•© ìˆ˜í™” ì¸ì‹ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
XPU, CUDA, CPU í™˜ê²½ ìžë™ ê°ì§€ ë° ìµœì í™”
"""

import os
import sys
import time
import signal
import traceback
import logging
import argparse
from datetime import datetime
from typing import Optional, Dict, Any

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from config import create_config, print_config, CONFIGS
from unified_pose_dataloader import UnifiedSignLanguageDataset, collate_fn
from sign_language_model import SequenceToSequenceSignModel
from sign_language_trainer import SignLanguageTrainer


class GracefulShutdownHandler:
    """ì•ˆì „í•œ ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.shutdown_requested = False
        self.trainer = None
        
    def register_trainer(self, trainer):
        """íŠ¸ë ˆì´ë„ˆ ë“±ë¡"""
        self.trainer = trainer
        
    def signal_handler(self, signum, frame):
        """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬"""
        signal_names = {
            signal.SIGINT: "SIGINT (Ctrl+C)",
            signal.SIGTERM: "SIGTERM"
        }
        signal_name = signal_names.get(signum, f"Signal {signum}")
        
        print(f"\nâš ï¸ {signal_name} ìˆ˜ì‹ ë¨. ì•ˆì „í•˜ê²Œ ì¢…ë£Œ ì¤‘...")
        self.shutdown_requested = True
        
        if self.trainer:
            self.trainer.request_shutdown()
            
    def setup_signals(self):
        """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì •"""
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        print("âœ… ì¢…ë£Œ ì‹ í˜¸ í•¸ë“¤ëŸ¬ ì„¤ì • ì™„ë£Œ")


def setup_logging(log_dir: str, device_type: str) -> logging.Logger:
    """ë¡œê¹… ì„¤ì •"""
    os.makedirs(log_dir, exist_ok=True)
    
    # ë¡œê·¸ íŒŒì¼ëª…
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"training_{device_type}_{timestamp}.log")
    
    # ë¡œê±° ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¡œê¹… ì‹œìž‘: {log_file}")
    return logger


def load_data_with_retry(config: Dict[str, Any], logger: logging.Logger, max_retries: int = 3):
    """ë°ì´í„° ë¡œë”© (ìž¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    data_config = config["data"]
    system_config = config["system"]
    
    for attempt in range(max_retries):
        try:
            logger.info(f"ë°ì´í„°ì…‹ ë¡œë”© ì‹œë„ {attempt + 1}/{max_retries}")
            
            # ê¸°ë³¸ ë°ì´í„°ì…‹ ìƒì„± (ì¦ê°• ë¹„í™œì„±í™”)
            base_dataset = UnifiedSignLanguageDataset(
                annotation_path=data_config.annotation_path,
                pose_data_dir=data_config.pose_data_dir,
                sequence_length=data_config.sequence_length,
                min_segment_length=data_config.min_segment_length,
                max_segment_length=data_config.max_segment_length,
                enable_augmentation=False  # ê¸°ë³¸ ë°ì´í„°ì…‹ (ì¦ê°• ì—†ìŒ)
            )
            
            if len(base_dataset) == 0:
                raise ValueError("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"âœ… ë°ì´í„°ì…‹ ë¡œë”© ì„±ê³µ: {len(base_dataset)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            
            # í›ˆë ¨/ê²€ì¦ ë¶„í•  (ê¸°ë³¸ ë°ì´í„°ì…‹ ê¸°ì¤€)
            train_size = int(data_config.train_ratio * len(base_dataset))
            val_size = len(base_dataset) - train_size
            
            train_indices, val_indices = torch.utils.data.random_split(
                range(len(base_dataset)), [train_size, val_size],
                generator=torch.Generator().manual_seed(system_config.random_seed)
            )
            
            # í›ˆë ¨ìš©: ì¦ê°• í™œì„±í™”ëœ ë°ì´í„°ì…‹ì—ì„œ train_indices ì‚¬ìš©
            if data_config.enable_augmentation:
                train_dataset_full = UnifiedSignLanguageDataset(
                    annotation_path=data_config.annotation_path,
                    pose_data_dir=data_config.pose_data_dir,
                    sequence_length=data_config.sequence_length,
                    min_segment_length=data_config.min_segment_length,
                    max_segment_length=data_config.max_segment_length,
                    enable_augmentation=True,
                    augmentation_config=data_config.augmentation_config
                )
                train_dataset = torch.utils.data.Subset(train_dataset_full, train_indices.indices)
            else:
                train_dataset = torch.utils.data.Subset(base_dataset, train_indices.indices)
            
            # ê²€ì¦ìš©: í•­ìƒ ì¦ê°• ì—†ìŒ
            val_dataset = torch.utils.data.Subset(base_dataset, val_indices.indices)
            
            # ë°ì´í„°ë¡œë” ìƒì„±
            train_dataloader = DataLoader(
                train_dataset,
                batch_size=config["training"].batch_size,
                shuffle=True,
                num_workers=system_config.num_workers,
                pin_memory=system_config.pin_memory,
                collate_fn=collate_fn,
                persistent_workers=system_config.num_workers > 0
            )
            
            val_dataloader = DataLoader(
                val_dataset,
                batch_size=config["training"].batch_size,
                shuffle=False,
                num_workers=system_config.num_workers,
                pin_memory=system_config.pin_memory,
                collate_fn=collate_fn,
                persistent_workers=system_config.num_workers > 0
            )
            
            logger.info(f"í›ˆë ¨ ì„¸íŠ¸: {len(train_dataset)} ({'ì¦ê°• í™œì„±í™”' if data_config.enable_augmentation else 'ì¦ê°• ë¹„í™œì„±í™”'}), ê²€ì¦ ì„¸íŠ¸: {len(val_dataset)} (ì¦ê°• ë¹„í™œì„±í™”)")
            return train_dataloader, val_dataloader, base_dataset.vocab_size
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {str(e)}")
            if attempt == max_retries - 1:
                logger.error("ìµœëŒ€ ìž¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
                raise
            
            logger.info("ìž ì‹œ í›„ ìž¬ì‹œë„...")

            time.sleep(2)


def create_model(config: Dict[str, Any], vocab_size: int, logger: logging.Logger) -> nn.Module:
    """ëª¨ë¸ ìƒì„±"""
    model_config = config["model"]
    
    # vocab_size ì—…ë°ì´íŠ¸
    model_config.vocab_size = vocab_size
    
    model = SequenceToSequenceSignModel(
        vocab_size=model_config.vocab_size,
        embed_dim=model_config.embed_dim,
        num_encoder_layers=model_config.num_encoder_layers,
        num_decoder_layers=model_config.num_decoder_layers,
        num_heads=model_config.num_heads,
        dim_feedforward=model_config.dim_feedforward,
        max_seq_len=model_config.max_seq_len,
        dropout=model_config.dropout
    )
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"ðŸ“Š ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    logger.info(f"   - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    logger.info(f"   - í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
    logger.info(f"   - ì–´íœ˜ í¬ê¸°: {vocab_size}")
    
    return model


def setup_device_specific_optimizations(device_type: str, logger: logging.Logger):
    """ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •"""
    if device_type == "xpu":
        if torch.xpu.is_available():
            logger.info(f"âœ… XPU ë””ë°”ì´ìŠ¤: {torch.xpu.get_device_name()}")
            logger.info(f"   - ë””ë°”ì´ìŠ¤ ìˆ˜: {torch.xpu.device_count()}")
            if torch.xpu.device_count() > 0:
                props = torch.xpu.get_device_properties(0)
                logger.info(f"   - ë©”ëª¨ë¦¬: {props.total_memory / 1e9:.1f}GB")
        else:
            logger.warning("âš ï¸ XPU ì‚¬ìš© ë¶ˆê°€, CPUë¡œ í´ë°±")

    elif device_type == "cuda":
        if torch.cuda.is_available():
            logger.info(f"âœ… CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name()}")
            logger.info(f"   - CUDA ë²„ì „: {torch.version.cuda}")
            logger.info(f"   - ë””ë°”ì´ìŠ¤ ìˆ˜: {torch.cuda.device_count()}")
            if torch.cuda.device_count() > 0:
                props = torch.cuda.get_device_properties(0)
                logger.info(f"   - ë©”ëª¨ë¦¬: {props.total_memory / 1e9:.1f}GB")
        else:
            logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€, CPUë¡œ í´ë°±")
    
    elif device_type == "cpu":
        logger.info(f"âœ… CPU ë””ë°”ì´ìŠ¤ ì‚¬ìš©")
        logger.info(f"   - CPU ìŠ¤ë ˆë“œ: {torch.get_num_threads()}")
        logger.info(f"   - CPU ì½”ì–´: {os.cpu_count()}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ìˆ˜í™” ì¸ì‹ ëª¨ë¸ í•™ìŠµ")
    parser.add_argument("--device", choices=["auto", "cuda", "xpu", "cpu"], 
                       default="auto", help="ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤")
    parser.add_argument("--config", choices=["development", "production", "debug"],
                       default=None, help="ì‚¬ì „ ì •ì˜ëœ ì„¤ì •")
    parser.add_argument("--resume", type=str, default=None,
                       help="ìž¬ê°œí•  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ")
    parser.add_argument("--epochs", type=int, default=None,
                       help="í•™ìŠµ ì—í­ ìˆ˜ (ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ)")
    parser.add_argument("--batch-size", type=int, default=None,
                       help="ë°°ì¹˜ í¬ê¸° (ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ)")
    parser.add_argument("--no-signal-handler", action="store_true",
                       help="ì¢…ë£Œ ì‹ í˜¸ í•¸ë“¤ëŸ¬ ë¹„í™œì„±í™”")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    custom_overrides = {}
    if args.config and args.config in CONFIGS:
        custom_overrides = CONFIGS[args.config]
    
    # ëª…ë ¹í–‰ ì˜¤ë²„ë¼ì´ë“œ
    if args.epochs:
        custom_overrides.setdefault("training", {})["num_epochs"] = args.epochs
    if args.batch_size:
        custom_overrides.setdefault("training", {})["batch_size"] = args.batch_size
    
    config = create_config(args.device, custom_overrides)
    
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
    if args.no_signal_handler:
        config["system"].enable_signal_handler = False
    
    # ì„¤ì • ì¶œë ¥
    print_config(config)
    
    # ì‹œë“œ ì„¤ì •
    torch.manual_seed(config["system"].random_seed)
    
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(config["system"].log_dir, config["system"].device)
    
    # ì¢…ë£Œ ì‹ í˜¸ í•¸ë“¤ëŸ¬ ì„¤ì •
    shutdown_handler = None
    if config["system"].enable_signal_handler:
        shutdown_handler = GracefulShutdownHandler()
        shutdown_handler.setup_signals()
    
    try:
        # ë””ë°”ì´ìŠ¤ ìµœì í™” ì„¤ì •
        setup_device_specific_optimizations(config["system"].device, logger)
        
        # ë°ì´í„° ë¡œë”©
        logger.info("ðŸ“Š ë°ì´í„° ë¡œë”© ì‹œìž‘...")
        train_dataloader, val_dataloader, vocab_size = load_data_with_retry(
            config, logger
        )
        
        # ëª¨ë¸ ìƒì„±
        logger.info("ðŸ§  ëª¨ë¸ ìƒì„± ì‹œìž‘...")
        model = create_model(config, vocab_size, logger)
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        logger.info("ðŸŽ¯ íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì‹œìž‘...")
        trainer = SignLanguageTrainer(
            model=model,
            train_dataloader=train_dataloader,
            val_dataloader=val_dataloader,
            device=config["system"].device,
            checkpoint_dir=config["system"].checkpoint_dir,
            log_dir=config["system"].log_dir
        )
        
        # ì¢…ë£Œ í•¸ë“¤ëŸ¬ì— íŠ¸ë ˆì´ë„ˆ ë“±ë¡
        if shutdown_handler:
            shutdown_handler.register_trainer(trainer)
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
        trainer.setup_training(
            learning_rate=config["training"].learning_rate,
            weight_decay=config["training"].weight_decay,
            warmup_steps=config["training"].warmup_steps,
            gradient_clip_val=config["training"].gradient_clip_val,
            word_loss_weight=config["training"].word_loss_weight,
            boundary_loss_weight=config["training"].boundary_loss_weight,
            confidence_loss_weight=config["training"].confidence_loss_weight
        )
        
        # í•™ìŠµ ì‹œìž‘
        logger.info("ðŸš€ í•™ìŠµ ì‹œìž‘!")
        trainer.train(
            num_epochs=config["training"].num_epochs,
            resume_from=args.resume,
            save_every_n_steps=config["system"].save_every_n_steps,
            log_every_n_steps=config["system"].log_every_n_steps
        )
        
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ì‚¬ìš©ìžì— ì˜í•œ ì¤‘ë‹¨")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(traceback.format_exc())
        sys.exit(1)
        
    finally:
        logger.info("ðŸ”„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        # ë””ë°”ì´ìŠ¤ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info("CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except:
            pass
        
        try:
            # XPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
            if hasattr(torch, 'xpu') and torch.xpu.device_count() > 0:
                torch.xpu.empty_cache()
                logger.info("XPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except:
            pass


if __name__ == "__main__":
    main()
