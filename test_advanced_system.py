#!/usr/bin/env python3
"""
ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ê°„ë‹¨ í…ŒìŠ¤íŠ¸
"""
import os
import logging
import sys
from pathlib import Path

# í˜„ì¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from advanced_config import AdvancedTrainingConfig, TrainingStageConfig
from advanced_data_utils import StratifiedDataSplitter
from unified_pose_dataloader import UnifiedSignLanguageDataset

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
        handlers=[
            logging.StreamHandler(),
        ]
    )

def test_config():
    """ì„¤ì • í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª 1. ì„¤ì • í…ŒìŠ¤íŠ¸")
    config = AdvancedTrainingConfig()
    config.random_seed.fix_all_seeds()
    
    print(f"âœ… ëœë¤ì‹œë“œ: {config.random_seed.seed}")
    print(f"âœ… í•™ìŠµ ë‹¨ê³„ ìˆ˜: {len(config.multi_stage.stages)}")
    
    for i, stage in enumerate(config.multi_stage.stages):
        print(f"  Stage {i+1}: {stage.name} ({stage.num_epochs} epochs)")
    
    return config

def test_dataset():
    """ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª 2. ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸")
    
    try:
        dataset = UnifiedSignLanguageDataset(
            annotation_path="./data/sign_language_dataset_only_sen_lzf.h5",
            pose_data_dir="./data",
            sequence_length=200,
            min_segment_length=20,
            max_segment_length=300,
            enable_augmentation=False
        )
        
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: {len(dataset)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        print(f"âœ… Vocabulary í¬ê¸°: {dataset.vocab_size}")
        print(f"âœ… vocab ì†ì„± í™•ì¸: {hasattr(dataset, 'vocab')}")
        
        if hasattr(dataset, 'vocab') and len(dataset.vocab) > 0:
            first_words = list(dataset.vocab.values())[:5]
            print(f"âœ… ì²« 5ê°œ ë‹¨ì–´: {first_words}")
        
        # ìƒ˜í”Œ ë°ì´í„° í…ŒìŠ¤íŠ¸
        sample = dataset[0]
        print(f"âœ… ìƒ˜í”Œ ë°ì´í„° í˜•íƒœ:")
        print(f"  pose_features: {sample['pose_features'].shape}")
        print(f"  vocab_ids: {sample['vocab_ids'].shape}")
        print(f"  duration: {sample['duration']}")
        
        return dataset
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def test_data_splitter(dataset):
    """ë°ì´í„° ë¶„í•  í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª 3. ë°ì´í„° ë¶„í•  í…ŒìŠ¤íŠ¸")
    
    try:
        config = AdvancedTrainingConfig()
        splitter = StratifiedDataSplitter(
            config=config.data_split,
            random_seed=config.random_seed.seed
        )
        
        train_indices, val_indices, test_indices = splitter.split_dataset_stratified(dataset)
        
        print(f"âœ… ë°ì´í„° ë¶„í•  ì„±ê³µ:")
        print(f"  í›ˆë ¨: {len(train_indices)}ê°œ")
        print(f"  ê²€ì¦: {len(val_indices)}ê°œ") 
        print(f"  í…ŒìŠ¤íŠ¸: {len(test_indices)}ê°œ")
        print(f"  ì´í•©: {len(train_indices) + len(val_indices) + len(test_indices)}ê°œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¶„í•  ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_dataloaders(dataset):
    """ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª 4. ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸")
    
    try:
        config = AdvancedTrainingConfig()
        splitter = StratifiedDataSplitter(
            config=config.data_split,
            random_seed=config.random_seed.seed
        )
        
        train_loader, val_loader, test_loader = splitter.create_dataloaders(
            dataset=dataset,
            batch_size=4,
            enable_train_augmentation=False,
            augmentation_config=None
        )
        
        print(f"âœ… ë°ì´í„°ë¡œë” ìƒì„± ì„±ê³µ")
        
        # ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        train_batch = next(iter(train_loader))
        val_batch = next(iter(val_loader))
        
        print(f"âœ… ë°°ì¹˜ í…ŒìŠ¤íŠ¸:")
        print(f"  í›ˆë ¨ ë°°ì¹˜: {train_batch['pose_features'].shape}")
        print(f"  ê²€ì¦ ë°°ì¹˜: {val_batch['pose_features'].shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œ ê°„ë‹¨ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    setup_logging()
    
    # 1. ì„¤ì • í…ŒìŠ¤íŠ¸
    config = test_config()
    
    # 2. ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
    dataset = test_dataset()
    if dataset is None:
        print("âŒ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ì¢…ë£Œ")
        return False
    
    # 3. ë°ì´í„° ë¶„í•  í…ŒìŠ¤íŠ¸
    split_success = test_data_splitter(dataset)
    if not split_success:
        print("âŒ ë°ì´í„° ë¶„í•  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ì¢…ë£Œ")
        return False
    
    # 4. ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸
    loader_success = test_dataloaders(dataset)
    if not loader_success:
        print("âŒ ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ì¢…ë£Œ")
        return False
    
    print("\nğŸ‰ ëª¨ë“  ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í†µê³¼!")
    print("ê³ ê¸‰ í•™ìŠµ ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
