#!/usr/bin/env python3
"""
ë©€í‹° GPU ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from device_utils import DeviceManager
from advanced_config import AdvancedTrainingConfig
from sign_language_model import SequenceToSequenceSignModel

def test_device_detection():
    """ë””ë°”ì´ìŠ¤ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ğŸ” ë””ë°”ì´ìŠ¤ ê°ì§€ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ì‹±ê¸€ GPU ê°ì§€
    device_single = DeviceManager.detect_best_device("auto", multi_gpu=False)
    device_info_single = DeviceManager.get_device_info(device_single)
    print(f"ì‹±ê¸€ GPU ëª¨ë“œ: {device_info_single}")
    
    # ë©€í‹° GPU ê°ì§€
    device_multi = DeviceManager.detect_best_device("auto", multi_gpu=True)
    device_info_multi = DeviceManager.get_device_info(device_multi)
    print(f"ë©€í‹° GPU ëª¨ë“œ: {device_info_multi}")
    
    # ë©€í‹° GPU ê°€ìš©ì„± í™•ì¸
    multi_gpu_available = DeviceManager.is_multi_gpu_available()
    print(f"ë©€í‹° GPU ì‚¬ìš© ê°€ëŠ¥: {multi_gpu_available}")
    
    if multi_gpu_available:
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ìˆ˜: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")

def test_multi_gpu_model():
    """ë©€í‹° GPU ëª¨ë¸ ì„¤ì • í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸš€ ë©€í‹° GPU ëª¨ë¸ ì„¤ì • í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
    model = SequenceToSequenceSignModel(
        vocab_size=100,
        embed_dim=128,
        num_encoder_layers=2,
        num_decoder_layers=2,
        num_heads=4,
        dim_feedforward=256,
        dropout=0.1,
        max_seq_len=50
    )
    
    device = DeviceManager.detect_best_device("auto", multi_gpu=True)
    print(f"ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {device}")
    
    model = model.to(device)
    print(f"ëª¨ë¸ì´ {device}ë¡œ ì´ë™ë¨")
    
    # ë©€í‹° GPU ì„¤ì • í…ŒìŠ¤íŠ¸
    if DeviceManager.is_multi_gpu_available() and torch.cuda.device_count() > 1:
        print("ë©€í‹° GPU ì„¤ì • ì‹œë„...")
        try:
            multi_gpu_model = DeviceManager.setup_multi_gpu(model)
            print(f"âœ… ë©€í‹° GPU ì„¤ì • ì„±ê³µ: {type(multi_gpu_model)}")
            print(f"DataParallel device_ids: {getattr(multi_gpu_model, 'device_ids', None)}")
        except Exception as e:
            print(f"âŒ ë©€í‹° GPU ì„¤ì • ì‹¤íŒ¨: {e}")
    else:
        print("ë©€í‹° GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ê±°ë‚˜ GPUê°€ 1ê°œë¿ì…ë‹ˆë‹¤.")

def test_batch_size_optimization():
    """ë°°ì¹˜ í¬ê¸° ìµœì í™” í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ë°°ì¹˜ í¬ê¸° ìµœì í™” í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    base_batch_size = 32
    device = DeviceManager.detect_best_device("auto", multi_gpu=True)
    
    effective_batch_size = DeviceManager.get_effective_batch_size(base_batch_size, device)
    print(f"ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°: {base_batch_size}")
    print(f"ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°: {effective_batch_size}")
    
    if torch.cuda.is_available() and torch.cuda.device_count() > 1:
        expected_size = base_batch_size * torch.cuda.device_count()
        print(f"ì˜ˆìƒ ë°°ì¹˜ í¬ê¸° (GPU ìˆ˜ Ã— ê¸°ë³¸): {expected_size}")

def test_configuration():
    """ë©€í‹° GPU ì„¤ì • í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("âš™ï¸ ë©€í‹° GPU ì„¤ì • í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ë©€í‹° GPU ì„¤ì •ìœ¼ë¡œ êµ¬ì„± ìƒì„±
    config = AdvancedTrainingConfig(
        experiment_name="multi_gpu_test",
        multi_gpu=True,
        use_data_parallel=True,
        auto_adjust_batch_size=True
    )
    
    print(f"ë©€í‹° GPU í™œì„±í™”: {config.multi_gpu}")
    print(f"DataParallel ì‚¬ìš©: {config.use_data_parallel}")
    print(f"ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {config.auto_adjust_batch_size}")

if __name__ == "__main__":
    print("ğŸ§ª ë©€í‹° GPU ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        test_device_detection()
        test_multi_gpu_model()
        test_batch_size_optimization()
        test_configuration()
        
        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
