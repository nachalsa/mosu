#!/usr/bin/env python3
"""
ì •ê·œí™” ìƒìˆ˜ (ë²”ìœ„ ë§¤í•‘ ë°©ì‹: í‘œì¤€ë²”ìœ„ â†’ 0.1~0.9)
KEYPOINT_NORM = {
    'x_min': 0.0,      # í‘œì¤€ ìµœì†Œê°’
    'x_max': 2304.0,   # í‘œì¤€ ìµœëŒ€ê°’ 
    'y_min': 0.0,      # í‘œì¤€ ìµœì†Œê°’
    'y_max': 3072.0,   # í‘œì¤€ ìµœëŒ€ê°’
    'target_min': 0.1, # ëª©í‘œ ìµœì†Œê°’
    'target_max': 0.9, # ëª©í‘œ ìµœëŒ€ê°’
}
SCORE_NORM = {
    'min': 0.0,
    'max': 10.0,       # í‘œì¤€ ìµœëŒ€ê°’
    'target_min': 0.0,
    'target_max': 1.0,
}í…Œì´ì…˜ ë°ì´í„° ë¡œë”
- í¬ì¦ˆ ë°ì´í„°: batch_SEN_XX_YY_F_poses.h5 (keypoints + scores)
- ì–´ë…¸í…Œì´ì…˜: sign_language_dataset_only_sen_lzf.h5 (ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´)

ì •ê·œí™” ì •ë³´:
- keypoints_scaled: x(0~2304), y(0~3072) â†’ ì •ê·œí™”: /2304, /3072
- scores: 0.0~10.0 â†’ ì •ê·œí™”: /10.0
"""

import os
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional
import logging
from pathlib import Path
import random
import math

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì •ê·œí™” ìƒìˆ˜ (ë²”ìœ„ ë§¤í•‘ ë°©ì‹: í‘œì¤€ë²”ìœ„ â†’ 0.1~0.9)
KEYPOINT_NORM = {
    'x_min': 0.0,      # í‘œì¤€ ìµœì†Œê°’
    'x_max': 2304.0,   # í‘œì¤€ ìµœëŒ€ê°’ 
    'y_min': 0.0,      # í‘œì¤€ ìµœì†Œê°’
    'y_max': 3072.0,   # í‘œì¤€ ìµœëŒ€ê°’
    'target_min': 0.1, # ëª©í‘œ ìµœì†Œê°’
    'target_max': 0.9, # ëª©í‘œ ìµœëŒ€ê°’
}
SCORE_NORM = {
    'min': 0.0,
    'max': 10.0,       # í‘œì¤€ ìµœëŒ€ê°’
    'target_min': 0.0,
    'target_max': 1.0,
}

class PoseDataAugmentation:
    """ìˆ˜í™” í¬ì¦ˆ ë°ì´í„° ì¦ê°• í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 enable_horizontal_flip: bool = True,
                 enable_rotation: bool = True,
                 enable_scaling: bool = True,
                 enable_noise: bool = True,
                 horizontal_flip_prob: float = 0.5,
                 rotation_range: float = 15.0,  # Â±15ë„
                 scaling_range: Tuple[float, float] = (0.9, 1.1),  # 90%~110%
                 noise_std: float = 0.01):  # ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨
        """
        Args:
            enable_horizontal_flip: ì¢Œìš° ë°˜ì „ í™œì„±í™”
            enable_rotation: íšŒì „ ì¦ê°• í™œì„±í™”
            enable_scaling: í¬ê¸° ì¦ê°• í™œì„±í™”
            enable_noise: ë…¸ì´ì¦ˆ ì¦ê°• í™œì„±í™”
            horizontal_flip_prob: ì¢Œìš° ë°˜ì „ í™•ë¥ 
            rotation_range: íšŒì „ ê°ë„ ë²”ìœ„ (ë„)
            scaling_range: í¬ê¸° ë³€í™˜ ë²”ìœ„
            noise_std: ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨
        """
        self.enable_horizontal_flip = enable_horizontal_flip
        self.enable_rotation = enable_rotation
        self.enable_scaling = enable_scaling
        self.enable_noise = enable_noise
        
        self.horizontal_flip_prob = horizontal_flip_prob
        self.rotation_range = rotation_range
        self.scaling_range = scaling_range
        self.noise_std = noise_std
        
        logger.info(f"ğŸ¨ ë°ì´í„° ì¦ê°• ì„¤ì •:")
        logger.info(f"   - ì¢Œìš° ë°˜ì „: {'âœ…' if enable_horizontal_flip else 'âŒ'} (í™•ë¥ : {horizontal_flip_prob})")
        logger.info(f"   - íšŒì „: {'âœ…' if enable_rotation else 'âŒ'} (Â±{rotation_range}Â°)")
        logger.info(f"   - í¬ê¸° ë³€í™˜: {'âœ…' if enable_scaling else 'âŒ'} ({scaling_range[0]:.1f}x~{scaling_range[1]:.1f}x)")
        logger.info(f"   - ë…¸ì´ì¦ˆ: {'âœ…' if enable_noise else 'âŒ'} (std: {noise_std})")
    
    def __call__(self, keypoints: np.ndarray, scores: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        ë°ì´í„° ì¦ê°• ì ìš©
        
        Args:
            keypoints: [frames, 133, 2] - ì •ê·œí™”ëœ í‚¤í¬ì¸íŠ¸ (0~1)
            scores: [frames, 133] - ì •ê·œí™”ëœ ìŠ¤ì½”ì–´ (0~1)
            
        Returns:
            augmented_keypoints, augmented_scores
        """
        augmented_keypoints = keypoints.copy()
        augmented_scores = scores.copy()
        
        # 1. ì¢Œìš° ë°˜ì „
        if self.enable_horizontal_flip and random.random() < self.horizontal_flip_prob:
            augmented_keypoints = self._horizontal_flip(augmented_keypoints)
        
        # 2. íšŒì „ (ì¤‘ì‹¬ì  ê¸°ì¤€)
        if self.enable_rotation:
            angle = random.uniform(-self.rotation_range, self.rotation_range)
            augmented_keypoints = self._rotate(augmented_keypoints, angle)
        
        # 3. í¬ê¸° ë³€í™˜ (ì¤‘ì‹¬ì  ê¸°ì¤€)
        if self.enable_scaling:
            scale = random.uniform(self.scaling_range[0], self.scaling_range[1])
            augmented_keypoints = self._scale(augmented_keypoints, scale)
        
        # 4. ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
        if self.enable_noise:
            augmented_keypoints = self._add_noise(augmented_keypoints)
            # ìŠ¤ì½”ì–´ì—ë„ ì•½ê°„ì˜ ë…¸ì´ì¦ˆ (ì‹ ë¢°ë„ ë³€í™” ì‹œë®¬ë ˆì´ì…˜)
            augmented_scores = self._add_score_noise(augmented_scores)
        
        # ì •ê·œí™” ë²”ìœ„ ìœ ì§€ (0~1)
        augmented_keypoints = np.clip(augmented_keypoints, 0.0, 1.0)
        augmented_scores = np.clip(augmented_scores, 0.0, 1.0)
        
        return augmented_keypoints, augmented_scores
    
    def _horizontal_flip(self, keypoints: np.ndarray) -> np.ndarray:
        """ì¢Œìš° ë°˜ì „ (X ì¢Œí‘œ ë’¤ì§‘ê¸°)"""
        flipped = keypoints.copy()
        flipped[:, :, 0] = 1.0 - flipped[:, :, 0]  # X ì¢Œí‘œ ë°˜ì „
        return flipped
    
    def _rotate(self, keypoints: np.ndarray, angle_degrees: float) -> np.ndarray:
        """ì¤‘ì‹¬ì  ê¸°ì¤€ íšŒì „"""
        angle_rad = math.radians(angle_degrees)
        cos_a = math.cos(angle_rad)
        sin_a = math.sin(angle_rad)
        
        rotated = keypoints.copy()
        
        # ì¤‘ì‹¬ì ì„ 0.5, 0.5ë¡œ ê°€ì • (ì •ê·œí™”ëœ ì¢Œí‘œê³„ì—ì„œ)
        center_x, center_y = 0.5, 0.5
        
        # ì¤‘ì‹¬ì  ê¸°ì¤€ìœ¼ë¡œ ì´ë™
        rotated[:, :, 0] -= center_x
        rotated[:, :, 1] -= center_y
        
        # íšŒì „ ë³€í™˜ ì ìš©
        x_rotated = rotated[:, :, 0] * cos_a - rotated[:, :, 1] * sin_a
        y_rotated = rotated[:, :, 0] * sin_a + rotated[:, :, 1] * cos_a
        
        # ë‹¤ì‹œ ì›ë˜ ìœ„ì¹˜ë¡œ ì´ë™
        rotated[:, :, 0] = x_rotated + center_x
        rotated[:, :, 1] = y_rotated + center_y
        
        return rotated
    
    def _scale(self, keypoints: np.ndarray, scale_factor: float) -> np.ndarray:
        """ì¤‘ì‹¬ì  ê¸°ì¤€ í¬ê¸° ë³€í™˜"""
        scaled = keypoints.copy()
        
        # ì¤‘ì‹¬ì ì„ 0.5, 0.5ë¡œ ê°€ì •
        center_x, center_y = 0.5, 0.5
        
        # ì¤‘ì‹¬ì  ê¸°ì¤€ìœ¼ë¡œ ì´ë™
        scaled[:, :, 0] -= center_x
        scaled[:, :, 1] -= center_y
        
        # í¬ê¸° ë³€í™˜ ì ìš©
        scaled[:, :, 0] *= scale_factor
        scaled[:, :, 1] *= scale_factor
        
        # ë‹¤ì‹œ ì›ë˜ ìœ„ì¹˜ë¡œ ì´ë™
        scaled[:, :, 0] += center_x
        scaled[:, :, 1] += center_y
        
        return scaled
    
    def _add_noise(self, keypoints: np.ndarray) -> np.ndarray:
        """ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€"""
        noise = np.random.normal(0, self.noise_std, keypoints.shape).astype(np.float32)
        return keypoints + noise
    
    def _add_score_noise(self, scores: np.ndarray) -> np.ndarray:
        """ìŠ¤ì½”ì–´ì— ë…¸ì´ì¦ˆ ì¶”ê°€ (ì‹ ë¢°ë„ ë³€í™” ì‹œë®¬ë ˆì´ì…˜)"""
        noise = np.random.normal(0, self.noise_std * 0.5, scores.shape).astype(np.float32)
        return scores + noise

class UnifiedSignLanguageDataset(Dataset):
    """í†µí•© ìˆ˜í™” ë°ì´í„°ì…‹ (í¬ì¦ˆ + ì–´ë…¸í…Œì´ì…˜)"""
    
    def __init__(self, 
                 annotation_path: str = "./data/sign_language_dataset_only_sen_lzf.h5",
                 pose_data_dir: str = "./data",
                 sequence_length: int = 200,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
                 min_segment_length: int = 10,  # ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´
                 max_segment_length: int = 300,  # ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´
                 enable_augmentation: bool = False,  # ë°ì´í„° ì¦ê°• í™œì„±í™”
                 augmentation_config: Dict = None):  # ì¦ê°• ì„¤ì •
        
        self.annotation_path = annotation_path
        self.pose_data_dir = Path(pose_data_dir)
        self.sequence_length = sequence_length
        self.min_segment_length = min_segment_length
        self.max_segment_length = max_segment_length
        self.enable_augmentation = enable_augmentation
        
        # ë°ì´í„° ì¦ê°• ì„¤ì •
        if enable_augmentation:
            aug_config = augmentation_config or {}
            self.augmentation = PoseDataAugmentation(**aug_config)
            logger.info("âœ… ë°ì´í„° ì¦ê°• í™œì„±í™”ë¨")
        else:
            self.augmentation = None
            logger.info("âŒ ë°ì´í„° ì¦ê°• ë¹„í™œì„±í™”ë¨")
        
        # ë°ì´í„° ë¡œë“œ
        self._load_annotations()
        self._load_pose_file_mapping()  # í¬ì¦ˆ íŒŒì¼ì„ ë¨¼ì € ìŠ¤ìº”
        self._filter_valid_segments()   # í¬ì¦ˆ ë°ì´í„°ê°€ ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ë§Œ í•„í„°ë§
        
        logger.info(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.valid_segments)}ê°œ ìœ íš¨ ì„¸ê·¸ë¨¼íŠ¸")
        logger.info(f"   - Vocabulary í¬ê¸°: {self.vocab_size}")
        logger.info(f"   - í‰ê·  ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´: {np.mean([s['duration'] for s in self.valid_segments]):.1f} í”„ë ˆì„")
        logger.info(f"   - ë°ì´í„° ì¦ê°•: {'âœ… í™œì„±í™”' if enable_augmentation else 'âŒ ë¹„í™œì„±í™”'}")
        
    def _load_annotations(self):
        """ì–´ë…¸í…Œì´ì…˜ ë°ì´í„° ë¡œë“œ"""
        logger.info(f"ğŸ“‹ ì–´ë…¸í…Œì´ì…˜ ë¡œë“œ ì¤‘: {self.annotation_path}")
        
        with h5py.File(self.annotation_path, 'r') as f:
            # Vocabulary ë¡œë“œ
            words_data = f['vocabulary']['words'][:]
            # ë‹¨ì–´ ë””ì½”ë”© (bytes â†’ string)
            self.words = []
            for w in words_data:
                if isinstance(w, bytes):
                    self.words.append(w.decode('utf-8'))
                else:
                    self.words.append(str(w))
            
            self.vocab_size = len(self.words)
            self.word_to_id = {word: idx for idx, word in enumerate(self.words)}
            self.vocab = {idx: word for idx, word in enumerate(self.words)}  # ID â†’ ë‹¨ì–´ ë§¤í•‘ ì¶”ê°€
            self.fps = 30  # ê¸°ë³¸ê°’
            
            logger.info(f"   Vocabulary í¬ê¸°: {self.vocab_size}")
            logger.info(f"   ì²« 5ê°œ ë‹¨ì–´: {self.words[:5]}")
            
            # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ë¡œë“œ
            n_segments = len(f['segments']['data_types'])
            self.segments = []
            
            for i in range(n_segments):
                vocab_len = int(f['segments']['vocab_lens'][i])
                segment = {
                    'index': i,
                    'data_type': int(f['segments']['data_types'][i]),  # 1=SEN (ëª¨ë“  ë°ì´í„°ê°€ SEN)
                    'data_id': int(f['segments']['data_ids'][i]),
                    'real_id': int(f['segments']['real_ids'][i]),
                    'view': int(f['segments']['views'][i]),  # 0=F (ì •ë©´ë§Œ ì‚¬ìš©)
                    'start_frame': int(f['segments']['start_frames'][i]),
                    'end_frame': int(f['segments']['end_frames'][i]),
                    'duration': int(f['segments']['duration_frames'][i]),
                    'vocab_len': vocab_len,
                    'vocab_ids': f['segments']['vocab_ids'][i, :vocab_len].tolist()
                }
                self.segments.append(segment)
    
    def _filter_valid_segments(self):
        """ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ í•„í„°ë§ - í¬ì¦ˆ ë°ì´í„°ê°€ ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ë§Œ"""
        logger.info("ğŸ” ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ í•„í„°ë§ ì¤‘...")
        
        # í¬ì¦ˆ ë°ì´í„°ê°€ ìˆëŠ” Real IDë“¤ í™•ì¸
        available_real_ids = set(real_id for real_id, _ in self.pose_files.keys())
        logger.info(f"   í¬ì¦ˆ ë°ì´í„°ê°€ ìˆëŠ” Real IDs: {sorted(available_real_ids)}")
        
        self.valid_segments = []
        filtered_counts = {
            'total': 0,
            'sen_type': 0,
            'front_view': 0,
            'proper_length': 0,
            'has_pose_data': 0,
            'final_valid': 0
        }
        
        for segment in self.segments:
            filtered_counts['total'] += 1
            
            # ì¡°ê±´ 1: SEN íƒ€ì…
            if segment['data_type'] != 1:
                continue
            filtered_counts['sen_type'] += 1
            
            # ì¡°ê±´ 2: ì •ë©´(F)
            if segment['view'] != 0:
                continue
            filtered_counts['front_view'] += 1
            
            # ì¡°ê±´ 3: ì ì ˆí•œ ê¸¸ì´
            if not (self.min_segment_length <= segment['duration'] <= self.max_segment_length):
                continue
            filtered_counts['proper_length'] += 1
            
            # ì¡°ê±´ 4: í¬ì¦ˆ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ (ìƒˆë¡œ ì¶”ê°€ëœ ì¡°ê±´)
            if segment['real_id'] not in available_real_ids:
                continue
            filtered_counts['has_pose_data'] += 1
            
            # ìµœì¢… ìœ íš¨ ì„¸ê·¸ë¨¼íŠ¸
            self.valid_segments.append(segment)
            filtered_counts['final_valid'] += 1
        
        logger.info(f"   í•„í„°ë§ ê²°ê³¼:")
        logger.info(f"     ì´ ì„¸ê·¸ë¨¼íŠ¸: {filtered_counts['total']}")
        logger.info(f"     SEN íƒ€ì…: {filtered_counts['sen_type']}")
        logger.info(f"     ì •ë©´(F): {filtered_counts['front_view']}")
        logger.info(f"     ì ì ˆí•œ ê¸¸ì´: {filtered_counts['proper_length']}")
        logger.info(f"     í¬ì¦ˆ ë°ì´í„° ì¡´ì¬: {filtered_counts['has_pose_data']}")
        logger.info(f"     ìµœì¢… ìœ íš¨: {filtered_counts['final_valid']}")
        
        # Real IDë³„ ë¶„í¬ í™•ì¸
        real_id_distribution = {}
        for segment in self.valid_segments:
            real_id = segment['real_id']
            real_id_distribution[real_id] = real_id_distribution.get(real_id, 0) + 1
        
        logger.info(f"   Real IDë³„ ìœ íš¨ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í¬:")
        for real_id in sorted(real_id_distribution.keys()):
            count = real_id_distribution[real_id]
            logger.info(f"     Real ID {real_id:2d}: {count:5d}ê°œ")
        
        # ì˜ˆìƒ ë¹„ìœ¨ vs ì‹¤ì œ ë¹„ìœ¨
        expected_ratio = len(available_real_ids) / 16  # 16ëª… ì¤‘ ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ë¹„ìœ¨
        actual_ratio = filtered_counts['final_valid'] / filtered_counts['proper_length']  # ê¸¸ì´ ì¡°ê±´ í†µê³¼ ëŒ€ë¹„ ìµœì¢… ìœ íš¨
        logger.info(f"   ì˜ˆìƒ ë¹„ìœ¨ (í¬ì¦ˆ ë°ì´í„° ê¸°ì¤€): {expected_ratio:.3f} ({len(available_real_ids)}/16)")
        logger.info(f"   ì‹¤ì œ ë¹„ìœ¨: {actual_ratio:.3f} ({filtered_counts['final_valid']}/{filtered_counts['proper_length']})")
    
    def _load_pose_file_mapping(self):
        """í¬ì¦ˆ íŒŒì¼ ë§¤í•‘ ìƒì„±"""
        logger.info("ğŸ—‚ï¸ í¬ì¦ˆ íŒŒì¼ ë§¤í•‘ ìƒì„± ì¤‘...")
        
        self.pose_files = {}
        pose_pattern = "batch_SEN_{:02d}_{:02d}_F_poses.h5"
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í¬ì¦ˆ íŒŒì¼ë“¤ ìŠ¤ìº”
        available_files = {}
        for real_id in range(1, 17):  # Real ID 1-16
            for batch_id in range(8):  # Batch ID 0-7 (ì¼ë°˜ì ìœ¼ë¡œ)
                pose_file = self.pose_data_dir / pose_pattern.format(real_id, batch_id)
                if pose_file.exists():
                    # íŒŒì¼ì—ì„œ ì‹¤ì œ data_id ë²”ìœ„ í™•ì¸
                    try:
                        with h5py.File(pose_file, 'r') as f:
                            video_keys = [k for k in f.keys() if k.startswith('video_sen')]
                            if video_keys:
                                # video_sen0001 â†’ 1
                                data_ids = [int(k.replace('video_sen', '')) for k in video_keys]
                                min_id, max_id = min(data_ids), max(data_ids)
                                available_files[(real_id, batch_id)] = {
                                    'file': str(pose_file),
                                    'data_id_range': (min_id, max_id),
                                    'video_keys': set(video_keys)
                                }
                                logger.debug(f"   ë°œê²¬: Real{real_id:02d}_Batch{batch_id:02d} â†’ data_id {min_id}-{max_id}")
                    except Exception as e:
                        logger.warning(f"   íŒŒì¼ ìŠ¤ìº” ì‹¤íŒ¨: {pose_file.name} - {e}")
        
        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ë§¤í•‘
        self.pose_files = available_files
        logger.info(f"   {len(self.pose_files)}ê°œ í¬ì¦ˆ íŒŒì¼ ë§¤í•‘ ì™„ë£Œ")
    
    def _get_pose_data(self, segment: Dict) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """ì„¸ê·¸ë¨¼íŠ¸ì— í•´ë‹¹í•˜ëŠ” í¬ì¦ˆ ë°ì´í„° ë¡œë“œ"""
        data_id = segment['data_id']
        
        # ëª¨ë“  íŒŒì¼ì—ì„œ í•´ë‹¹ data_idë¥¼ í¬í•¨í•˜ëŠ” íŒŒì¼ ì°¾ê¸°
        target_file = None
        for file_key, file_info in self.pose_files.items():
            video_key = f"video_sen{data_id:04d}"
            if video_key in file_info['video_keys']:
                target_file = file_info['file']
                break
        
        if target_file is None:
            logger.warning(f"âš ï¸ ì„¸ê·¸ë¨¼íŠ¸ {data_id}: í¬ì¦ˆ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return self._create_dummy_pose_data(segment)
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                with h5py.File(target_file, 'r') as f:
                    # ë¹„ë””ì˜¤ í‚¤ ìƒì„±
                    video_key = f"video_sen{data_id:04d}"
                    
                    if video_key not in f:
                        logger.warning(f"âš ï¸ ë¹„ë””ì˜¤ í‚¤ '{video_key}'ê°€ íŒŒì¼ {target_file}ì— ì—†ìŒ")
                        return self._create_dummy_pose_data(segment)
                    
                    # í‚¤í¬ì¸íŠ¸ì™€ ìŠ¤ì½”ì–´ ë¡œë“œ
                    keypoints = f[video_key]['keypoints_scaled'][:]  # [frames, 133, 2]
                    scores = f[video_key]['scores'][:]  # [frames, 133]
                    
                    # ë²”ìœ„ ë§¤í•‘ ì •ê·œí™” (í‘œì¤€ë²”ìœ„ â†’ 0.1~0.9)
                    
                    # X ì¢Œí‘œ: 0~2304 â†’ 0.1~0.9 ì„ í˜• ë§¤í•‘
                    x_norm = (keypoints[:, :, 0] - KEYPOINT_NORM['x_min']) / (KEYPOINT_NORM['x_max'] - KEYPOINT_NORM['x_min'])  # 0~1
                    x_mapped = KEYPOINT_NORM['target_min'] + x_norm * (KEYPOINT_NORM['target_max'] - KEYPOINT_NORM['target_min'])  # 0.1~0.9
                    keypoints[:, :, 0] = np.clip(x_mapped, 0.0, 1.0)  # ì´ìƒì¹˜ ìµœì¢… í´ë¦¬í•‘
                    
                    # Y ì¢Œí‘œ: 0~3072 â†’ 0.1~0.9 ì„ í˜• ë§¤í•‘
                    y_norm = (keypoints[:, :, 1] - KEYPOINT_NORM['y_min']) / (KEYPOINT_NORM['y_max'] - KEYPOINT_NORM['y_min'])  # 0~1
                    y_mapped = KEYPOINT_NORM['target_min'] + y_norm * (KEYPOINT_NORM['target_max'] - KEYPOINT_NORM['target_min'])  # 0.1~0.9
                    keypoints[:, :, 1] = np.clip(y_mapped, 0.0, 1.0)  # ì´ìƒì¹˜ ìµœì¢… í´ë¦¬í•‘
                    
                    # ìŠ¤ì½”ì–´: 0~10 â†’ 0.0~1.0 ì„ í˜• ë§¤í•‘
                    s_norm = (scores - SCORE_NORM['min']) / (SCORE_NORM['max'] - SCORE_NORM['min'])  # 0~1
                    s_mapped = SCORE_NORM['target_min'] + s_norm * (SCORE_NORM['target_max'] - SCORE_NORM['target_min'])  # 0.1~0.9
                    scores = np.clip(s_mapped, 0.0, 1.0)  # ì´ìƒì¹˜ ìµœì¢… í´ë¦¬í•‘
                    
                    logger.debug(f"ì •ê·œí™” í›„ ë²”ìœ„ - í‚¤í¬ì¸íŠ¸: [{keypoints.min():.3f}, {keypoints.max():.3f}], ìŠ¤ì½”ì–´: [{scores.min():.3f}, {scores.max():.3f}]")
                    
                    # ì„¸ê·¸ë¨¼íŠ¸ ë²”ìœ„ ì¶”ì¶œ - ì•ˆì „í•œ ë²”ìœ„ ì¡°ì •
                    start_frame = max(0, segment['start_frame'])
                    end_frame = segment['end_frame']
                    actual_frames = keypoints.shape[0]
                    
                    # ìœ íš¨í•œ í”„ë ˆì„ ë²”ìœ„ í™•ì¸ ë° ì¡°ì •
                    if end_frame > actual_frames:
                        logger.debug(f"í”„ë ˆì„ ë²”ìœ„ ì¡°ì •: {end_frame} â†’ {actual_frames} (Data ID: {data_id})")
                        end_frame = actual_frames
                    
                    # ì‹œì‘ í”„ë ˆì„ì´ ìœ íš¨ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš° ì¡°ì •
                    if start_frame >= actual_frames:
                        logger.debug(f"ì‹œì‘ í”„ë ˆì„ ë²”ìœ„ ì¡°ì •: {start_frame} â†’ {max(0, actual_frames - 1)} (Data ID: {data_id})")
                        start_frame = max(0, actual_frames - 1)
                    
                    # ìµœì†Œ ê¸¸ì´ ë³´ì¥ ë° ë²”ìœ„ ì¬ê²€ì¦
                    if start_frame >= end_frame:
                        # ìµœì†Œ 1í”„ë ˆì„ì€ í™•ë³´í•˜ë˜, ë” ì•ˆì •ì ìœ¼ë¡œ ì¡°ì •
                        if actual_frames > 0:
                            # ê°€ëŠ¥í•œ í•œ ì›ë³¸ ë²”ìœ„ì— ê°€ê¹ê²Œ ì¡°ì •
                            min_length = 1
                            if actual_frames >= min_length:
                                start_frame = max(0, min(start_frame, actual_frames - min_length))
                                end_frame = min(actual_frames, start_frame + min_length)
                                logger.debug(f"í”„ë ˆì„ ë²”ìœ„ ìˆ˜ì •: [{start_frame}-{end_frame}] (Data ID: {data_id})")
                            else:
                                start_frame = 0
                                end_frame = actual_frames
                                logger.debug(f"ì „ì²´ í”„ë ˆì„ ì‚¬ìš©: [0-{actual_frames}] (Data ID: {data_id})")
                        else:
                            logger.debug(f"ë¹ˆ ë°ì´í„° ìƒì„± (Data ID: {data_id})")
                            return self._create_dummy_pose_data(segment)
                    
                    # ë„ˆë¬´ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (ì„±ëŠ¥ ê°œì„ )
                    segment_length = end_frame - start_frame
                    if segment_length < 3:
                        # ê°€ëŠ¥í•œ ë²”ìœ„ì—ì„œ í™•ì¥
                        center = (start_frame + end_frame) // 2
                        half_length = max(1, segment_length // 2)
                        start_frame = max(0, center - half_length - 1)
                        end_frame = min(actual_frames, center + half_length + 2)
                        if segment_length == 1:  # 1í”„ë ˆì„ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ë¡œê·¸
                            logger.debug(f"1í”„ë ˆì„ ì„¸ê·¸ë¨¼íŠ¸ í™•ì¥: Data ID {data_id}")
                    
                    # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
                    segment_keypoints = keypoints[start_frame:end_frame]  # [seg_frames, 133, 2]
                    segment_scores = scores[start_frame:end_frame]  # [seg_frames, 133]
                    
                    # ìµœì¢… ë°ì´í„° ê²€ì¦ (ê°„ì†Œí™”)
                    if segment_keypoints.shape[0] == 0 or segment_keypoints.shape[1] != 133:
                        logger.warning(f"âš ï¸ ì˜ëª»ëœ í¬ì¦ˆ ë°ì´í„° í˜•íƒœ: {segment_keypoints.shape} (Data ID: {data_id})")
                        return None  # ë”ë¯¸ ë°ì´í„° ëŒ€ì‹  None ë°˜í™˜
                    
                    return segment_keypoints, segment_scores
                    
            except (OSError, IOError) as e:
                logger.warning(f"íŒŒì¼ I/O ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    import time
                    time.sleep(0.1)  # ì§§ì€ ëŒ€ê¸°
                    continue
            except Exception as e:
                logger.error(f"í¬ì¦ˆ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                break
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ (ë”ë¯¸ ë°ì´í„° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        logger.warning(f"âš ï¸ í¬ì¦ˆ ë°ì´í„° ë¡œë“œ ì™„ì „ ì‹¤íŒ¨ (Data ID: {data_id})")
        return None
    
    def _create_dummy_pose_data(self, segment):
        """ë”ë¯¸ í¬ì¦ˆ ë°ì´í„° ìƒì„± (ì •ê·œí™”ëœ ë²”ìœ„)"""
        duration = segment['end_frame'] - segment['start_frame']
        duration = max(1, min(duration, 300))  # 1~300 í”„ë ˆì„ ì œí•œ
        
        # ë”ë¯¸ í‚¤í¬ì¸íŠ¸ (ì¤‘ì•™ ìœ„ì¹˜ ê¸°ì¤€, ì´ë¯¸ ì •ê·œí™”ëœ 0~1 ë²”ìœ„)
        dummy_keypoints = np.full((duration, 133, 2), 0.5, dtype=np.float32)
        # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€
        dummy_keypoints += np.random.normal(0, 0.05, dummy_keypoints.shape).astype(np.float32)
        dummy_keypoints = np.clip(dummy_keypoints, 0.0, 1.0)
        
        # ë”ë¯¸ ìŠ¤ì½”ì–´ (ì •ê·œí™”ëœ 0~1 ë²”ìœ„, ë‚®ì€ ì‹ ë¢°ë„)
        dummy_scores = np.full((duration, 133), 0.1, dtype=np.float32)
        
        logger.debug(f"ë”ë¯¸ í¬ì¦ˆ ë°ì´í„° ìƒì„± (ì •ê·œí™”ë¨): {dummy_keypoints.shape}")
        return dummy_keypoints, dummy_scores
    
    def __len__(self):
        return len(self.valid_segments)
    
    def __getitem__(self, idx):
        """ë°ì´í„°ì…‹ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸° (ì•ˆì •ì ì¸ ì¸ë±ì‹±)"""
        # ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
        if not (0 <= idx < len(self.valid_segments)):
            idx = idx % len(self.valid_segments)
        
        max_retries = min(20, max(5, len(self.valid_segments) // 20))  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
        original_idx = idx
        tried_indices = set()
        
        for attempt in range(max_retries):
            current_idx = idx % len(self.valid_segments)
            
            # ì´ë¯¸ ì‹œë„í•œ ì¸ë±ìŠ¤ëŠ” ê±´ë„ˆë›°ê¸°
            if current_idx in tried_indices:
                idx = (idx + 1) % len(self.valid_segments)
                continue
                
            tried_indices.add(current_idx)
            
            try:
                segment = self.valid_segments[current_idx]
                
                # í¬ì¦ˆ ë°ì´í„° ë¡œë“œ ì‹œë„
                pose_data = self._get_pose_data(segment)
                
                if pose_data is None:
                    logger.debug(f"âŒ ì„¸ê·¸ë¨¼íŠ¸ {current_idx} (Data ID: {segment['data_id']}) í¬ì¦ˆ ë°ì´í„° ì—†ìŒ")
                    idx = (idx + 1) % len(self.valid_segments)
                    continue
                
                segment_keypoints, segment_scores = pose_data
                
                # ìœ íš¨í•œ ë°ì´í„°ì¸ì§€ í™•ì¸
                if segment_keypoints.shape[0] == 0 or segment_scores.shape[0] == 0:
                    logger.debug(f"âŒ ì„¸ê·¸ë¨¼íŠ¸ {current_idx} ë¹ˆ ë°ì´í„°")
                    idx = (idx + 1) % len(self.valid_segments)
                    continue
                
                # ë°ì´í„° ì¦ê°• ì ìš© (í›ˆë ¨ ì‹œì—ë§Œ)
                if self.enable_augmentation and self.augmentation is not None:
                    try:
                        segment_keypoints, segment_scores = self.augmentation(segment_keypoints, segment_scores)
                    except Exception as aug_e:
                        logger.debug(f"âš ï¸ ë°ì´í„° ì¦ê°• ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {current_idx}): {aug_e}")
                        # ì¦ê°• ì‹¤íŒ¨í•´ë„ ì›ë³¸ ë°ì´í„° ì‚¬ìš©
                
                # í¬ì¦ˆ íŠ¹ì§• ê²°í•©
                pose_features = np.concatenate([
                    segment_keypoints,  # [frames, 133, 2] - ì •ê·œí™”ë¨ (0~1)
                    np.expand_dims(segment_scores, axis=-1)  # [frames, 133, 1] - ì •ê·œí™”ë¨ (0~1)
                ], axis=-1)  # [frames, 133, 3]
                
                # PyTorch í…ì„œë¡œ ë³€í™˜
                pose_features = torch.from_numpy(pose_features).float()
                
                # vocab_ids ì•ˆì „í•œ ë³€í™˜
                try:
                    if isinstance(segment['vocab_ids'], (list, tuple)):
                        vocab_array = np.array(segment['vocab_ids'][:segment['vocab_len']], dtype=np.int64)
                    else:
                        vocab_array = segment['vocab_ids'][:segment['vocab_len']].astype(np.int64)
                    vocab_ids = torch.from_numpy(vocab_array)
                except Exception as vocab_e:
                    logger.debug(f"âš ï¸ vocab_ids ë³€í™˜ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {current_idx}): {vocab_e}")
                    vocab_ids = torch.zeros(1, dtype=torch.long)
                
                # ì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ ê²½ìš°
                return {
                    'pose_features': pose_features,
                    'vocab_ids': vocab_ids,
                    'vocab_len': min(segment['vocab_len'], len(vocab_ids)),
                    'duration': pose_features.shape[0],
                    'segment_info': {
                        'data_id': segment['data_id'],
                        'real_id': segment['real_id'],
                        'start_frame': segment['start_frame'],
                        'end_frame': segment['end_frame']
                    }
                }
                
            except IndexError as e:
                logger.debug(f"âŒ ì¸ë±ìŠ¤ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {current_idx}): {e}")
                idx = (idx + 1) % len(self.valid_segments)
                continue
            except Exception as e:
                logger.debug(f"âŒ ì„¸ê·¸ë¨¼íŠ¸ {current_idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                idx = (idx + 1) % len(self.valid_segments)
                continue
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ ë”ë¯¸ ìƒ˜í”Œ ë°˜í™˜ (ê²½ê³  ìˆ˜ì¤€ ë‚®ì¶¤)
        if len(tried_indices) >= max_retries:
            logger.debug(f"ì¸ë±ìŠ¤ {original_idx} ì£¼ë³€ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•¨. ë”ë¯¸ ìƒ˜í”Œ ì‚¬ìš©")
        
        return self._create_safe_dummy_sample(original_idx)
    
    def _create_safe_dummy_sample(self, idx):
        """ì•ˆì „í•œ ë”ë¯¸ ìƒ˜í”Œ ìƒì„± (ì •ê·œí™”ëœ ë²”ìœ„)"""
        logger.debug(f"ë”ë¯¸ ìƒ˜í”Œ ìƒì„± (ì¸ë±ìŠ¤: {idx})")
        
        # ê¸°ë³¸ ë”ë¯¸ ë°ì´í„° (ì´ë¯¸ ì •ê·œí™”ëœ 0~1 ë²”ìœ„)
        dummy_frames = 30
        pose_features = torch.full((dummy_frames, 133, 3), 0.5).float()
        
        # ì•½ê°„ì˜ ëœë¤ì„± ì¶”ê°€ (ì •ê·œí™”ëœ ë²”ìœ„ ìœ ì§€)
        pose_features += torch.randn_like(pose_features) * 0.1
        pose_features = torch.clamp(pose_features, 0.0, 1.0)
        
        vocab_ids = torch.zeros(1).long()  # ë”ë¯¸ ë‹¨ì–´ ID
        
        return {
            'pose_features': pose_features,
            'vocab_ids': vocab_ids,
            'vocab_len': 1,
            'duration': dummy_frames,
            'segment_info': {
                'data_id': -1,
                'real_id': -1,
                'start_frame': 0,
                'end_frame': dummy_frames
            }
        }


def collate_fn(batch):
    """ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ - ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ íŒ¨ë”©"""
    batch_size = len(batch)
    
    # ìµœëŒ€ ê¸¸ì´ ê³„ì‚°
    max_frames = max(item['duration'] for item in batch)
    max_vocab_len = max(item['vocab_len'] for item in batch)
    
    # íŒ¨ë”©ëœ ë°°ì¹˜ í…ì„œ ì´ˆê¸°í™”
    batch_pose_features = torch.zeros(batch_size, max_frames, 133, 3)
    batch_vocab_ids = torch.zeros(batch_size, max_vocab_len, dtype=torch.long)
    batch_frame_masks = torch.zeros(batch_size, max_frames, dtype=torch.bool)
    batch_vocab_masks = torch.zeros(batch_size, max_vocab_len, dtype=torch.bool)
    
    # ë©”íƒ€ë°ì´í„°
    batch_vocab_lens = []
    batch_durations = []
    batch_segment_infos = []
    
    for i, item in enumerate(batch):
        frames = item['duration']
        vocab_len = item['vocab_len']
        
        # í¬ì¦ˆ ë°ì´í„° ë³µì‚¬
        batch_pose_features[i, :frames] = item['pose_features']
        batch_frame_masks[i, :frames] = True
        
        # Vocabulary ë°ì´í„° ë³µì‚¬
        batch_vocab_ids[i, :vocab_len] = item['vocab_ids']
        batch_vocab_masks[i, :vocab_len] = True
        
        # ë©”íƒ€ë°ì´í„°
        batch_vocab_lens.append(vocab_len)
        batch_durations.append(frames)
        batch_segment_infos.append(item['segment_info'])
    
    return {
        'pose_features': batch_pose_features,  # [batch, max_frames, 133, 3]
        'vocab_ids': batch_vocab_ids,  # [batch, max_vocab_len]
        'frame_masks': batch_frame_masks,  # [batch, max_frames]
        'vocab_masks': batch_vocab_masks,  # [batch, max_vocab_len]
        'vocab_lens': torch.tensor(batch_vocab_lens),
        'durations': torch.tensor(batch_durations),
        'segment_infos': batch_segment_infos
    }

def create_dataloader(dataset_or_path,
                     batch_size: int = 8,
                     shuffle: bool = True,
                     num_workers: int = 2,
                     annotation_path: str = "./data/sign_language_dataset_only_sen_lzf.h5",
                     pose_data_dir: str = "./data",
                     enable_augmentation: bool = False,
                     augmentation_config: Dict = None,
                     **kwargs):
    """í†µí•© ë°ì´í„° ë¡œë” ìƒì„± - Dataset ê°ì²´ ë˜ëŠ” ê²½ë¡œ ì§€ì›"""
    
    # Dataset ë˜ëŠ” Subset ê°ì²´ì¸ì§€ í™•ì¸
    if hasattr(dataset_or_path, '__getitem__') and hasattr(dataset_or_path, '__len__'):
        # ì´ë¯¸ Dataset ë˜ëŠ” Subset ê°ì²´
        dataset = dataset_or_path
    else:
        # ê²½ë¡œê°€ ì£¼ì–´ì§„ ê²½ìš° ìƒˆ Dataset ìƒì„±
        dataset = UnifiedSignLanguageDataset(
            annotation_path=annotation_path,
            pose_data_dir=pose_data_dir,
            enable_augmentation=enable_augmentation,
            augmentation_config=augmentation_config,
            **kwargs
        )
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=False  # XPU í™˜ê²½ ê³ ë ¤
    )
    
    return dataloader, dataset

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸš€ í†µí•© í¬ì¦ˆ ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸ (ì¦ê°• í¬í•¨)")
    print("=" * 60)
    
    try:
        # ê¸°ë³¸ ë°ì´í„°ì…‹ ìƒì„± (ì¦ê°• ë¹„í™œì„±í™”)
        print("\n1ï¸âƒ£ ê¸°ë³¸ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ (ì¦ê°• ì—†ìŒ)")
        dataset_basic = UnifiedSignLanguageDataset(
            annotation_path="./data/sign_language_dataset_only_sen_lzf.h5",
            pose_data_dir="./data",
            sequence_length=200,
            min_segment_length=20,
            max_segment_length=300,
            enable_augmentation=False
        )
        
        print(f"âœ… ê¸°ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ:")
        print(f"   - ì´ ì„¸ê·¸ë¨¼íŠ¸: {len(dataset_basic)} ê°œ")
        print(f"   - Vocabulary í¬ê¸°: {dataset_basic.vocab_size}")
        
        # ì¦ê°• ë°ì´í„°ì…‹ ìƒì„±
        print("\n2ï¸âƒ£ ì¦ê°• ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ (ëª¨ë“  ì¦ê°• í™œì„±í™”)")
        augmentation_config = {
            'enable_horizontal_flip': True,
            'enable_rotation': True, 
            'enable_scaling': True,
            'enable_noise': True,
            'horizontal_flip_prob': 0.7,
            'rotation_range': 15.0,
            'scaling_range': (0.85, 1.15),
            'noise_std': 0.02
        }
        
        dataset_aug = UnifiedSignLanguageDataset(
            annotation_path="./data/sign_language_dataset_only_sen_lzf.h5",
            pose_data_dir="./data",
            sequence_length=200,
            min_segment_length=20,
            max_segment_length=300,
            enable_augmentation=True,
            augmentation_config=augmentation_config
        )
        
        # ë™ì¼í•œ ìƒ˜í”Œë¡œ ì¦ê°• íš¨ê³¼ ë¹„êµ
        print(f"\n3ï¸âƒ£ ì¦ê°• íš¨ê³¼ ë¹„êµ í…ŒìŠ¤íŠ¸")
        
        # ê¸°ë³¸ ìƒ˜í”Œ
        basic_sample = dataset_basic[0]
        basic_pose = basic_sample['pose_features'][:, :, :2]  # [frames, 133, 2]
        
        print(f"   ğŸ“Š ê¸°ë³¸ ìƒ˜í”Œ í†µê³„:")
        print(f"     - í”„ë ˆì„ ìˆ˜: {basic_sample['duration']}")
        print(f"     - í‚¤í¬ì¸íŠ¸ X ë²”ìœ„: [{basic_pose[:, :, 0].min():.3f}, {basic_pose[:, :, 0].max():.3f}]")
        print(f"     - í‚¤í¬ì¸íŠ¸ Y ë²”ìœ„: [{basic_pose[:, :, 1].min():.3f}, {basic_pose[:, :, 1].max():.3f}]")
        
        # ì¦ê°• ìƒ˜í”Œë“¤ (ê°™ì€ ì¸ë±ìŠ¤ì—ì„œ ì—¬ëŸ¬ë²ˆ ì¶”ì¶œ)
        aug_samples = []
        for i in range(3):
            aug_sample = dataset_aug[0]  # ë™ì¼ ì¸ë±ìŠ¤ì—ì„œ ì¦ê°•
            aug_samples.append(aug_sample)
        
        print(f"\n   ğŸ¨ ì¦ê°•ëœ ìƒ˜í”Œë“¤ ë¹„êµ:")
        for i, aug_sample in enumerate(aug_samples):
            aug_pose = aug_sample['pose_features'][:, :, :2]
            print(f"     ì¦ê°• {i+1}: X[{aug_pose[:, :, 0].min():.3f}, {aug_pose[:, :, 0].max():.3f}], "
                  f"Y[{aug_pose[:, :, 1].min():.3f}, {aug_pose[:, :, 1].max():.3f}]")
        
        # ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        print(f"\n4ï¸âƒ£ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        dataloader_basic = DataLoader(dataset_basic, batch_size=2, shuffle=False, collate_fn=collate_fn)
        dataloader_aug = DataLoader(dataset_aug, batch_size=2, shuffle=False, collate_fn=collate_fn)
        
        batch_basic = next(iter(dataloader_basic))
        batch_aug = next(iter(dataloader_aug))
        
        print(f"   ê¸°ë³¸ ë°°ì¹˜: {batch_basic['pose_features'].shape}")
        print(f"   ì¦ê°• ë°°ì¹˜: {batch_aug['pose_features'].shape}")
        print(f"   í‰ê·  í”„ë ˆì„ ìˆ˜: ê¸°ë³¸ {batch_basic['durations'].float().mean():.1f}, "
              f"ì¦ê°• {batch_aug['durations'].float().mean():.1f}")
        
        print(f"\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
